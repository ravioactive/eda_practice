{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dfac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project configuration:\n",
      "SLUG = customer-segmentation\n",
      "DATA_DIR = /Users/ravisharma/workdir/eda_practice/data/customer-segmentation\n",
      "DATASET_KEY = vjchoudhary7/customer-segmentation-tutorial-in-python\n",
      "FIG_DIR = /Users/ravisharma/workdir/eda_practice/figures/customer-segmentation\n",
      "REP_DIR = /Users/ravisharma/workdir/eda_practice/reports/customer-segmentation\n",
      "NOTEBOOK_DIR = /Users/ravisharma/workdir/eda_practice/notebooks/customer-segmentation\n",
      "Vars not found in globals: []\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "\n",
    "print(\"Project configuration:\")\n",
    "print(f\"SLUG = {SLUG}\")\n",
    "print(f\"DATA_DIR = {DATA_DIR}\")\n",
    "print(f\"DATASET_KEY = {DATASET_KEY}\")\n",
    "print(f\"FIG_DIR = {FIG_DIR}\")\n",
    "print(f\"REP_DIR = {REP_DIR}\")\n",
    "print(f\"NOTEBOOK_DIR = {NOTEBOOK_DIR}\")\n",
    "\n",
    "missing_vars = [var for var in ['SLUG', 'DATA_DIR', 'FIG_DIR', 'REP_DIR', 'NOTEBOOK_DIR', 'DATASET_KEY'] if var not in globals()]\n",
    "print(f\"Vars not found in globals: {missing_vars}\")\n",
    "\n",
    "# Set default values if variables are not found in store or are empty\n",
    "if not SLUG:  # Check if empty string\n",
    "    print(f\"{SLUG=} is empty, initializing everything explicitly\")\n",
    "    SLUG = 'customer-segmentation'\n",
    "    DATASET_KEY = 'vjchoudhary7/customer-segmentation-tutorial-in-python'\n",
    "    GIT_ROOT = Path.cwd().parent.parent\n",
    "    DATA_DIR = GIT_ROOT / 'data' / SLUG\n",
    "    FIG_DIR = GIT_ROOT / 'figures' / SLUG\n",
    "    REP_DIR = GIT_ROOT / 'reports' / SLUG\n",
    "    NOTEBOOK_DIR = GIT_ROOT / 'notebooks' / SLUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ae492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914685d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV /Users/ravisharma/workdir/eda_practice/data/customer-segmentation/Mall_Customers.csv loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
       "0           1    Male   19                  15                      39\n",
       "1           2    Male   21                  15                      81\n",
       "2           3  Female   20                  16                       6\n",
       "3           4  Female   23                  16                      77\n",
       "4           5  Female   31                  17                      40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downloading data\n",
    "\n",
    "base_df = pd.DataFrame()\n",
    "\n",
    "CSV_PATH = Path(DATA_DIR) / \"Mall_Customers.csv\"\n",
    "if not CSV_PATH.exists:\n",
    "    print(f\"CSV {CSV_PATH} does not exist. base_df will remain empty.\")\n",
    "else:\n",
    "    base_df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"CSV {CSV_PATH} loaded successfully.\")\n",
    "\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c10c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>38.850000</td>\n",
       "      <td>60.560000</td>\n",
       "      <td>50.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57.879185</td>\n",
       "      <td>13.969007</td>\n",
       "      <td>26.264721</td>\n",
       "      <td>25.823522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.750000</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>34.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>150.250000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CustomerID         Age  Annual Income (k$)  Spending Score (1-100)\n",
       "count  200.000000  200.000000          200.000000              200.000000\n",
       "mean   100.500000   38.850000           60.560000               50.200000\n",
       "std     57.879185   13.969007           26.264721               25.823522\n",
       "min      1.000000   18.000000           15.000000                1.000000\n",
       "25%     50.750000   28.750000           41.500000               34.750000\n",
       "50%    100.500000   36.000000           61.500000               50.000000\n",
       "75%    150.250000   49.000000           78.000000               73.000000\n",
       "max    200.000000   70.000000          137.000000               99.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e21f86",
   "metadata": {},
   "source": [
    "## üéØ One-Class SVM (Support Vector Machine) - Comprehensive Analysis\n",
    "\n",
    "### üìù **Code Breakdown**\n",
    "\n",
    "```python\n",
    "# One-Class SVM\n",
    "from sklearn.svm import OneClassSVM\n",
    "one_class_svm = OneClassSVM(nu=0.1, kernel='rbf', gamma='auto')\n",
    "outliers = one_class_svm.fit_predict(base_df[['Age']])\n",
    "print(outliers)\n",
    "```\n",
    "\n",
    "**Line-by-line explanation:**\n",
    "\n",
    "1. **Import**: `from sklearn.svm import OneClassSVM` - Imports the One-Class SVM implementation from scikit-learn\n",
    "2. **Model Creation**: `OneClassSVM(nu=0.1, kernel='rbf', gamma='auto')` - Creates model with specific parameters:\n",
    "   - `nu=0.1`: Expected fraction of outliers (10%)\n",
    "   - `kernel='rbf'`: Radial Basis Function kernel for non-linear decision boundaries\n",
    "   - `gamma='auto'`: Automatic scaling parameter for RBF kernel\n",
    "3. **Fit & Predict**: `fit_predict(base_df[['Age']])` - Trains model and predicts outliers in one step\n",
    "4. **Output**: Array of 1s (normal) and -1s (outliers)\n",
    "\n",
    "### üìö **Essential Documentation & Resources**\n",
    "\n",
    "#### **Official Documentation:**\n",
    "- **[Scikit-learn OneClassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html)** - Complete API reference\n",
    "- **[Scikit-learn Outlier Detection Guide](https://scikit-learn.org/stable/modules/outlier_detection.html)** - Overview of outlier detection methods\n",
    "- **[Scikit-learn SVM User Guide](https://scikit-learn.org/stable/modules/svm.html)** - Comprehensive SVM theory and usage\n",
    "\n",
    "#### **Research Papers:**\n",
    "- **[Original One-Class SVM Paper](http://www.jmlr.org/papers/volume2/scholkopf01a/scholkopf01a.pdf)** - Sch√∂lkopf et al. (2001) \"Estimating the Support of a High-Dimensional Distribution\"\n",
    "- **[SVDD Paper](https://www.jmlr.org/papers/volume5/tax04a/tax04a.pdf)** - Tax & Duin (2004) \"Support Vector Data Description\"\n",
    "\n",
    "#### **Helpful Blog Posts & Tutorials:**\n",
    "- **[Towards Data Science: One-Class SVM](https://towardsdatascience.com/outlier-detection-with-one-class-svms-5403a1a1878c)** - Practical tutorial with examples\n",
    "- **[Machine Learning Mastery: One-Class SVM](https://machinelearningmastery.com/one-class-classification-algorithms/)** - Comprehensive guide to one-class classification\n",
    "- **[Analytics Vidhya: Anomaly Detection](https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/)** - Comparison of outlier detection methods\n",
    "\n",
    "### üîç **Algorithm Theory & How It Works**\n",
    "\n",
    "**Core Concept:**\n",
    "One-Class SVM learns a decision boundary that encapsulates the \"normal\" data points in a high-dimensional feature space, treating anything outside this boundary as an outlier.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "1. **Kernel Transformation**: Maps data to higher-dimensional space using RBF kernel: `K(x,x') = exp(-Œ≥||x-x'||¬≤)`\n",
    "2. **Optimization Problem**: Finds hyperplane that separates data from origin with maximum margin\n",
    "3. **Decision Function**: `f(x) = Œ£·µ¢ Œ±·µ¢ K(x·µ¢,x) - œÅ` where Œ±·µ¢ are support vectors\n",
    "4. **Classification**: Points with `f(x) ‚â• 0` are normal, `f(x) < 0` are outliers\n",
    "\n",
    "**Visual Intuition:**\n",
    "- Imagine fitting a \"bubble\" around your normal data points\n",
    "- The SVM finds the optimal bubble boundary that contains most data\n",
    "- Points outside the bubble are considered outliers\n",
    "\n",
    "### üìä **Output Interpretation & Practical Usage**\n",
    "\n",
    "**Understanding the Output:**\n",
    "```python\n",
    "# Example output: [1 1 -1 1 1 1 -1 1 ...]\n",
    "# 1  = Normal point (inside the decision boundary)\n",
    "# -1 = Outlier point (outside the decision boundary)\n",
    "```\n",
    "\n",
    "**Practical Application Code:**\n",
    "```python\n",
    "# Get outlier indices and values\n",
    "outlier_indices = np.where(outliers == -1)[0]\n",
    "normal_indices = np.where(outliers == 1)[0]\n",
    "\n",
    "print(f\"Found {len(outlier_indices)} outliers out of {len(base_df)} customers\")\n",
    "print(f\"Outlier percentage: {len(outlier_indices)/len(base_df)*100:.1f}%\")\n",
    "\n",
    "# Examine outlier customers\n",
    "outlier_customers = base_df.iloc[outlier_indices]\n",
    "normal_customers = base_df.iloc[normal_indices]\n",
    "\n",
    "print(f\"\\nOutlier Age Statistics:\")\n",
    "print(f\"Age range: {outlier_customers['Age'].min()} - {outlier_customers['Age'].max()}\")\n",
    "print(f\"Mean age: {outlier_customers['Age'].mean():.1f}\")\n",
    "print(f\"\\nNormal Age Statistics:\")\n",
    "print(f\"Age range: {normal_customers['Age'].min()} - {normal_customers['Age'].max()}\")\n",
    "print(f\"Mean age: {normal_customers['Age'].mean():.1f}\")\n",
    "\n",
    "# Get decision scores for ranking\n",
    "decision_scores = one_class_svm.decision_function(base_df[['Age']])\n",
    "# More negative scores = more outlier-like\n",
    "print(f\"\\nMost outlier-like customers (lowest scores):\")\n",
    "most_outlier_idx = np.argsort(decision_scores.flatten())[:5]\n",
    "print(base_df.iloc[most_outlier_idx][['Age']])\n",
    "```\n",
    "\n",
    "**Business Interpretation for Customer Segmentation:**\n",
    "- **Outlier customers** may represent:\n",
    "  - Unique customer segments (very young/old customers)\n",
    "  - Data entry errors\n",
    "  - Special cases requiring different marketing strategies\n",
    "  - VIP customers with unusual behavior patterns\n",
    "\n",
    "### ‚öñÔ∏è **Strengths vs Weaknesses**\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"One-Class SVM Characteristics\"\n",
    "        A[\"üéØ One-Class SVM\"] --> B[\"Strengths\"]\n",
    "        A --> C[\"Weaknesses\"]\n",
    "        \n",
    "        B --> B1[\"Non-linear boundaries<br/>(RBF kernel)\"]\n",
    "        B --> B2[\"No assumption about<br/>data distribution\"]\n",
    "        B --> B3[\"Robust to<br/>high dimensions\"]\n",
    "        B --> B4[\"Theoretically<br/>well-founded\"]\n",
    "        B --> B5[\"Flexible kernel<br/>choices\"]\n",
    "        \n",
    "        C --> C1[\"Complex parameter<br/>tuning (nu, gamma)\"]\n",
    "        C --> C2[\"Computationally<br/>expensive O(n¬≥)\"]\n",
    "        C --> C3[\"Black box<br/>(hard to interpret)\"]\n",
    "        C --> C4[\"Sensitive to<br/>kernel parameters\"]\n",
    "        C --> C5[\"Memory intensive<br/>for large datasets\"]\n",
    "    end\n",
    "    \n",
    "    style A fill:#e1f5fe\n",
    "    style B fill:#e8f5e8\n",
    "    style C fill:#fce4ec\n",
    "```\n",
    "\n",
    "**Strengths:**\n",
    "- ‚úÖ **Non-linear boundaries** (RBF kernel)\n",
    "- ‚úÖ **No assumption about data distribution**\n",
    "- ‚úÖ **Robust to high dimensions**\n",
    "- ‚úÖ **Theoretically well-founded**\n",
    "- ‚úÖ **Flexible kernel choices**\n",
    "\n",
    "**Weaknesses:**\n",
    "- ‚ùå **Complex parameter tuning** (nu, gamma)\n",
    "- ‚ùå **Computationally expensive** O(n¬≥)\n",
    "- ‚ùå **Black box** (hard to interpret)\n",
    "- ‚ùå **Sensitive to kernel parameters**\n",
    "- ‚ùå **Memory intensive** for large datasets\n",
    "\n",
    "### üìà **Detailed Comparison with Other Outlier Detection Methods**\n",
    "\n",
    "| **Method** | **Type** | **Assumptions** | **Complexity** | **Interpretability** | **Best For** | **Limitations** |\n",
    "|------------|----------|-----------------|-----------------|---------------------|--------------|-----------------|\n",
    "| **One-Class SVM** | Non-parametric | None (kernel-based) | High O(n¬≥) | Low | Complex boundaries, high-dim | Parameter tuning, computation |\n",
    "| **Standard Z-Score** | Parametric | Normal distribution | Low O(n) | High | Quick screening | Assumes normality |\n",
    "| **Modified Z-Score** | Robust | Symmetric distribution | Low O(n) | High | Robust univariate | Univariate only |\n",
    "| **Isolation Forest** | Tree-based | None | Medium O(n log n) | Medium | Large datasets, fast | Less precise boundaries |\n",
    "| **LOF** | Density-based | Local density varies | High O(n¬≤) | Medium | Local outliers | Neighborhood sensitive |\n",
    "| **DBSCAN** | Clustering | Density clusters exist | Medium O(n log n) | High | Cluster-based outliers | Parameter sensitive |\n",
    "| **Elliptic Envelope** | Parametric | Gaussian/elliptical | Low O(n) | High | Multivariate Gaussian | Assumes elliptical shape |\n",
    "\n",
    "### üéØ **When to Use One-Class SVM**\n",
    "\n",
    "**‚úÖ Ideal Scenarios:**\n",
    "- **Complex decision boundaries** needed (non-linear patterns)\n",
    "- **No distributional assumptions** can be made\n",
    "- **High-dimensional data** with complex relationships\n",
    "- **Theoretical rigor** is important for your application\n",
    "- **Small to medium datasets** where computation isn't limiting\n",
    "\n",
    "**‚ùå Avoid When:**\n",
    "- **Large datasets** (>10,000 points) due to computational cost\n",
    "- **Simple linear patterns** (Z-score would be sufficient)\n",
    "- **Real-time applications** requiring fast predictions\n",
    "- **Interpretability is crucial** for business decisions\n",
    "- **Limited computational resources**\n",
    "\n",
    "### üîß **Parameter Tuning Guide**\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "1. **`nu` (0 < nu ‚â§ 1)**: Expected fraction of outliers\n",
    "   - **0.05**: Very conservative (5% outliers)\n",
    "   - **0.1**: Moderate (10% outliers) - **current setting**\n",
    "   - **0.2**: Liberal (20% outliers)\n",
    "   - **Rule**: Start with domain knowledge of expected outlier rate\n",
    "\n",
    "2. **`kernel`**: Type of kernel function\n",
    "   - **'rbf'**: Radial Basis Function - **current setting** (most common)\n",
    "   - **'linear'**: Linear kernel (simpler boundaries)\n",
    "   - **'poly'**: Polynomial kernel (specific polynomial patterns)\n",
    "   - **'sigmoid'**: Sigmoid kernel (neural network-like)\n",
    "\n",
    "3. **`gamma`**: RBF kernel coefficient\n",
    "   - **'auto'**: 1/n_features - **current setting**\n",
    "   - **'scale'**: 1/(n_features √ó X.var())\n",
    "   - **Float**: Custom value (higher = more complex boundaries)\n",
    "\n",
    "**Optimization Code:**\n",
    "```python\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def optimize_one_class_svm(X, param_grid=None):\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'nu': [0.05, 0.1, 0.15, 0.2],\n",
    "            'gamma': ['auto', 'scale', 0.001, 0.01, 0.1, 1],\n",
    "            'kernel': ['rbf']  # Focus on RBF for simplicity\n",
    "        }\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    results = []\n",
    "    \n",
    "    for params in ParameterGrid(param_grid):\n",
    "        try:\n",
    "            model = OneClassSVM(**params, random_state=42)\n",
    "            predictions = model.fit_predict(X)\n",
    "            \n",
    "            # Skip if all points classified as outliers or all as normal\n",
    "            if len(np.unique(predictions)) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Calculate silhouette score as quality metric\n",
    "            score = silhouette_score(X, predictions)\n",
    "            \n",
    "            # Count outliers\n",
    "            n_outliers = np.sum(predictions == -1)\n",
    "            outlier_ratio = n_outliers / len(X)\n",
    "            \n",
    "            results.append({\n",
    "                'params': params,\n",
    "                'score': score,\n",
    "                'n_outliers': n_outliers,\n",
    "                'outlier_ratio': outlier_ratio\n",
    "            })\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return best_params, best_score, results\n",
    "\n",
    "# Apply optimization\n",
    "best_params, best_score, all_results = optimize_one_class_svm(base_df[['Age']].values)\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best silhouette score: {best_score:.3f}\")\n",
    "```\n",
    "\n",
    "### üí° **Recommendations for Your Customer Segmentation**\n",
    "\n",
    "**Current Settings Analysis:**\n",
    "- `nu=0.1` (10% outliers) - **Reasonable** for customer data\n",
    "- `kernel='rbf'` - **Good choice** for non-linear patterns  \n",
    "- `gamma='auto'` - **Conservative** default setting\n",
    "\n",
    "**Suggested Improvements:**\n",
    "1. **Try `gamma='scale'`** - Often performs better than 'auto'\n",
    "2. **Test different `nu` values** - 0.05-0.15 range for business data\n",
    "3. **Consider multivariate analysis** - Use multiple features (Age + Income + Spending)\n",
    "4. **Validate results** - Check if outliers make business sense\n",
    "\n",
    "**Enhanced Implementation:**\n",
    "```python\n",
    "# Multi-feature One-Class SVM for better customer insights\n",
    "features = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
    "X_multi = base_df[features].values\n",
    "\n",
    "# Standardize features (important for SVM)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_multi)\n",
    "\n",
    "# Apply One-Class SVM\n",
    "enhanced_svm = OneClassSVM(nu=0.08, kernel='rbf', gamma='scale')\n",
    "multi_outliers = enhanced_svm.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"Multi-feature outlier detection:\")\n",
    "print(f\"Outliers found: {np.sum(multi_outliers == -1)} ({np.sum(multi_outliers == -1)/len(base_df)*100:.1f}%)\")\n",
    "```\n",
    "\n",
    "## üéØ **Summary: One-Class SVM for Customer Outlier Detection**\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. **Powerful non-linear** outlier detection without distributional assumptions\n",
    "2. **Current parameters** (nu=0.1, rbf, gamma='auto') are reasonable starting points\n",
    "3. **Best for complex patterns** but computationally expensive for large datasets\n",
    "4. **Consider multi-feature analysis** for richer customer insights\n",
    "5. **Validate business relevance** of detected outliers\n",
    "\n",
    "One-Class SVM excels at finding complex outlier patterns that simpler methods might miss, making it valuable for sophisticated customer segmentation where you suspect non-linear relationships in your data! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9916425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1 -1  1 -1  1  1  1 -1 -1  1  1  1 -1  1  1  1 -1  1  1  1  1  1 -1\n",
      "  1  1  1  1 -1  1  1  1 -1 -1  1  1 -1 -1  1 -1 -1 -1  1 -1  1 -1  1  1\n",
      "  1 -1  1  1 -1 -1  1 -1 -1  1  1 -1 -1 -1  1  1  1 -1 -1 -1 -1 -1 -1 -1\n",
      "  1  1 -1 -1  1 -1  1  1  1 -1  1  1  1  1  1  1 -1  1 -1 -1  1 -1 -1 -1\n",
      " -1  1  1 -1  1  1  1 -1  1  1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1  1\n",
      "  1 -1 -1  1  1 -1 -1 -1 -1 -1 -1  1  1 -1 -1  1  1 -1 -1  1  1 -1 -1 -1\n",
      "  1 -1  1 -1 -1 -1 -1  1  1 -1 -1  1  1 -1 -1 -1  1  1 -1 -1  1  1 -1  1\n",
      "  1 -1 -1 -1  1  1  1 -1  1  1 -1  1  1 -1  1  1 -1 -1  1 -1 -1  1 -1 -1\n",
      "  1 -1 -1  1  1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# One-Class SVM\n",
    "from sklearn.svm import OneClassSVM\n",
    "one_class_svm = OneClassSVM(nu=0.1, kernel='rbf', gamma='auto')\n",
    "outliers = one_class_svm.fit_predict(base_df[['Age']])\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c7392",
   "metadata": {},
   "source": [
    "## üéØ One-Class SVM Parameter Tuning: Comprehensive Guide\n",
    "\n",
    "### üìä Core Parameters and Data-Driven Selection\n",
    "\n",
    "#### **1. `nu` - Expected Outlier Fraction**\n",
    "\n",
    "**What it controls:** Upper bound on the fraction of training errors and lower bound on the fraction of support vectors\n",
    "\n",
    "**Data-driven selection methods:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def estimate_nu_for_one_class_svm(X, methods=['iqr', 'zscore', 'mahalanobis', 'domain']):\n",
    "    \"\"\"\n",
    "    Estimate nu parameter using multiple statistical methods\n",
    "    \"\"\"\n",
    "    \n",
    "    nu_estimates = {}\n",
    "    \n",
    "    # Method 1: IQR-based estimation (univariate)\n",
    "    if 'iqr' in methods:\n",
    "        for i in range(X.shape[1]):\n",
    "            data = X[:, i]\n",
    "            Q1 = np.percentile(data, 25)\n",
    "            Q3 = np.percentile(data, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            iqr_outliers = np.sum((data < lower_bound) | (data > upper_bound))\n",
    "            nu_estimates[f'iqr_feature_{i}'] = iqr_outliers / len(data)\n",
    "    \n",
    "    # Method 2: Z-score based (univariate)\n",
    "    if 'zscore' in methods:\n",
    "        for i in range(X.shape[1]):\n",
    "            data = X[:, i]\n",
    "            z_scores = np.abs(stats.zscore(data))\n",
    "            zscore_outliers = np.sum(z_scores > 2.5)\n",
    "            nu_estimates[f'zscore_feature_{i}'] = zscore_outliers / len(data)\n",
    "    \n",
    "    # Method 3: Mahalanobis distance (multivariate)\n",
    "    if 'mahalanobis' in methods and X.shape[1] > 1:\n",
    "        try:\n",
    "            mean = np.mean(X, axis=0)\n",
    "            cov = np.cov(X.T)\n",
    "            inv_cov = np.linalg.pinv(cov)\n",
    "            \n",
    "            mahal_distances = []\n",
    "            for i in range(len(X)):\n",
    "                diff = X[i] - mean\n",
    "                mahal_dist = np.sqrt(diff.T @ inv_cov @ diff)\n",
    "                mahal_distances.append(mahal_dist)\n",
    "            \n",
    "            mahal_distances = np.array(mahal_distances)\n",
    "            \n",
    "            # Use chi-squared distribution for p dimensions\n",
    "            p = X.shape[1]\n",
    "            chi2_95 = stats.chi2.ppf(0.95, p)\n",
    "            chi2_99 = stats.chi2.ppf(0.99, p)\n",
    "            \n",
    "            outliers_95 = np.sum(mahal_distances**2 > chi2_95)\n",
    "            outliers_99 = np.sum(mahal_distances**2 > chi2_99)\n",
    "            \n",
    "            nu_estimates['mahalanobis_95'] = outliers_95 / len(X)\n",
    "            nu_estimates['mahalanobis_99'] = outliers_99 / len(X)\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            pass\n",
    "    \n",
    "    # Method 4: Domain-specific heuristics\n",
    "    if 'domain' in methods:\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # Conservative estimates based on data type\n",
    "        if n_samples < 100:\n",
    "            nu_estimates['domain_small_dataset'] = 0.15\n",
    "        elif n_samples < 1000:\n",
    "            nu_estimates['domain_medium_dataset'] = 0.10\n",
    "        else:\n",
    "            nu_estimates['domain_large_dataset'] = 0.05\n",
    "        \n",
    "        # Feature-based heuristics\n",
    "        if X.shape[1] == 1:\n",
    "            nu_estimates['domain_univariate'] = 0.08\n",
    "        elif X.shape[1] <= 5:\n",
    "            nu_estimates['domain_low_dim'] = 0.10\n",
    "        else:\n",
    "            nu_estimates['domain_high_dim'] = 0.12\n",
    "    \n",
    "    # Calculate statistics\n",
    "    estimates = [v for v in nu_estimates.values() if 0 < v <= 0.5]\n",
    "    \n",
    "    if estimates:\n",
    "        conservative_nu = min(estimates)\n",
    "        liberal_nu = max(estimates)\n",
    "        median_nu = np.median(estimates)\n",
    "        robust_nu = np.clip(median_nu, 0.01, 0.3)\n",
    "    else:\n",
    "        conservative_nu = liberal_nu = median_nu = robust_nu = 0.1\n",
    "    \n",
    "    print(\"Nu Estimates:\")\n",
    "    for method, estimate in nu_estimates.items():\n",
    "        print(f\"  {method}: {estimate:.3f}\")\n",
    "    \n",
    "    print(f\"\\nConservative nu: {conservative_nu:.3f}\")\n",
    "    print(f\"Liberal nu: {liberal_nu:.3f}\")\n",
    "    print(f\"Median nu: {median_nu:.3f}\")\n",
    "    print(f\"Recommended nu: {robust_nu:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'conservative': conservative_nu,\n",
    "        'liberal': liberal_nu,\n",
    "        'median': median_nu,\n",
    "        'recommended': robust_nu,\n",
    "        'all_estimates': nu_estimates\n",
    "    }\n",
    "\n",
    "# Apply nu estimation\n",
    "nu_analysis = estimate_nu_for_one_class_svm(base_df[['Age']].values)\n",
    "```\n",
    "\n",
    "Rules for Nu:**\n",
    "- **Financial fraud**: 0.01-0.05 (very rare outliers)\n",
    "- **Customer behavior**: 0.05-0.15 (moderate outliers)\n",
    "- **Sensor data**: 0.02-0.08 (equipment failures)\n",
    "- **Medical diagnostics**: 0.01-0.10 (rare conditions)\n",
    "- **Quality control**: 0.03-0.10 (defects)\n",
    "\n",
    "#### **2. `gamma` - RBF Kernel Coefficient**\n",
    "\n",
    "**What it controls:** Influence of each training example (higher = more complex decision boundary)\n",
    "\n",
    "```python\n",
    "def estimate_gamma_for_rbf_kernel(X, method='comprehensive'):\n",
    "    \"\"\"\n",
    "    Estimate gamma parameter based on data characteristics\n",
    "    \"\"\"\n",
    "    \n",
    "    gamma_estimates = {}\n",
    "    \n",
    "    # Method 1: Default sklearn approaches\n",
    "    n_features = X.shape[1]\n",
    "    gamma_estimates['auto'] = 1.0 / n_features\n",
    "    gamma_estimates['scale'] = 1.0 / (n_features * X.var())\n",
    "    \n",
    "    # Method 2: Data variance analysis\n",
    "    overall_variance = np.var(X)\n",
    "    gamma_estimates['inverse_variance'] = 1.0 / overall_variance\n",
    "    \n",
    "    # Method 3: Distance-based estimation\n",
    "    n_samples = min(1000, len(X))\n",
    "    sample_indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "    \n",
    "    # Calculate pairwise distances\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    distances = euclidean_distances(X_sample)\n",
    "    \n",
    "    # Remove diagonal (zero distances)\n",
    "    mask = np.ones(distances.shape, dtype=bool)\n",
    "    np.fill_diagonal(mask, False)\n",
    "    distances_flat = distances[mask]\n",
    "    \n",
    "    # Use statistics of distances\n",
    "    mean_distance = np.mean(distances_flat)\n",
    "    median_distance = np.median(distances_flat)\n",
    "    \n",
    "    gamma_estimates['inverse_mean_distance'] = 1.0 / (mean_distance ** 2)\n",
    "    gamma_estimates['inverse_median_distance'] = 1.0 / (median_distance ** 2)\n",
    "    \n",
    "    # Method 4: Percentile-based\n",
    "    distance_90th = np.percentile(distances_flat, 90)\n",
    "    distance_75th = np.percentile(distances_flat, 75)\n",
    "    \n",
    "    gamma_estimates['inverse_90th_percentile'] = 1.0 / (distance_90th ** 2)\n",
    "    gamma_estimates['inverse_75th_percentile'] = 1.0 / (distance_75th ** 2)\n",
    "    \n",
    "    # Method 5: Rule of thumb based on data dimensionality\n",
    "    if n_features == 1:\n",
    "        gamma_estimates['rule_of_thumb'] = 1.0\n",
    "    elif n_features <= 5:\n",
    "        gamma_estimates['rule_of_thumb'] = 0.1\n",
    "    elif n_features <= 20:\n",
    "        gamma_estimates['rule_of_thumb'] = 0.01\n",
    "    else:\n",
    "        gamma_estimates['rule_of_thumb'] = 0.001\n",
    "    \n",
    "    # Filter extreme values\n",
    "    valid_gammas = {k: v for k, v in gamma_estimates.items() \n",
    "                   if 1e-6 <= v <= 1e3}\n",
    "    \n",
    "    if valid_gammas:\n",
    "        gamma_values = list(valid_gammas.values())\n",
    "        median_gamma = np.median(gamma_values)\n",
    "        conservative_gamma = min(gamma_values)\n",
    "        liberal_gamma = max(gamma_values)\n",
    "    else:\n",
    "        median_gamma = conservative_gamma = liberal_gamma = 'auto'\n",
    "    \n",
    "    print(\"Gamma Estimates:\")\n",
    "    for method, estimate in gamma_estimates.items():\n",
    "        if isinstance(estimate, str):\n",
    "            print(f\"  {method}: {estimate}\")\n",
    "        else:\n",
    "            print(f\"  {method}: {estimate:.6f}\")\n",
    "    \n",
    "    print(f\"\\nRecommended gamma: {median_gamma}\")\n",
    "    \n",
    "    return {\n",
    "        'recommended': median_gamma,\n",
    "        'conservative': conservative_gamma,\n",
    "        'liberal': liberal_gamma,\n",
    "        'all_estimates': valid_gammas\n",
    "    }\n",
    "\n",
    "# Apply gamma estimation\n",
    "gamma_analysis = estimate_gamma_for_rbf_kernel(base_df[['Age']].values)\n",
    "```\n",
    "\n",
    "**Gamma Selection Guidelines:**\n",
    "- **Small gamma (0.001-0.01)**: Smooth decision boundary, underfitting risk\n",
    "- **Medium gamma (0.01-1.0)**: Balanced complexity\n",
    "- **Large gamma (1.0-100)**: Complex boundary, overfitting risk\n",
    "- **'auto'**: Safe default (1/n_features)\n",
    "- **'scale'**: Variance-adjusted (1/(n_features √ó variance))\n",
    "\n",
    "### üß™ One-Class SVM Validation Methodologies\n",
    "\n",
    "#### **1. Cross-Validation for Outlier Detection**\n",
    "\n",
    "```python\n",
    "def cross_validate_one_class_svm(X, param_grid, cv_folds=5, scoring_methods=['silhouette', 'stability']):\n",
    "    \"\"\"\n",
    "    Cross-validation specifically designed for One-Class SVM\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Create parameter combinations\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "    \n",
    "    for params in ParameterGrid(param_grid):\n",
    "        fold_results = []\n",
    "        \n",
    "        kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            \n",
    "            try:\n",
    "                # Fit on training data\n",
    "                model = OneClassSVM(**params, random_state=42)\n",
    "                model.fit(X_train)\n",
    "                \n",
    "                # Predict on both training and validation\n",
    "                train_pred = model.predict(X_train)\n",
    "                val_pred = model.predict(X_val)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                fold_metrics = {}\n",
    "                \n",
    "                # 1. Silhouette score (if we have both classes)\n",
    "                if 'silhouette' in scoring_methods:\n",
    "                    if len(np.unique(val_pred)) > 1:\n",
    "                        sil_score = silhouette_score(X_val, val_pred)\n",
    "                        fold_metrics['silhouette'] = sil_score\n",
    "                    else:\n",
    "                        fold_metrics['silhouette'] = -1  # Penalty for no outliers\n",
    "                \n",
    "                # 2. Stability score (consistency between train/val outlier ratios)\n",
    "                if 'stability' in scoring_methods:\n",
    "                    train_outlier_ratio = np.sum(train_pred == -1) / len(train_pred)\n",
    "                    val_outlier_ratio = np.sum(val_pred == -1) / len(val_pred)\n",
    "                    stability = 1 - abs(train_outlier_ratio - val_outlier_ratio)\n",
    "                    fold_metrics['stability'] = stability\n",
    "                \n",
    "                # 3. Support vector ratio (model complexity indicator)\n",
    "                n_support_vectors = len(model.support_vectors_)\n",
    "                sv_ratio = n_support_vectors / len(X_train)\n",
    "                fold_metrics['sv_ratio'] = sv_ratio\n",
    "                \n",
    "                # 4. Decision function statistics\n",
    "                train_scores = model.decision_function(X_train)\n",
    "                val_scores = model.decision_function(X_val)\n",
    "                \n",
    "                fold_metrics['mean_train_score'] = np.mean(train_scores)\n",
    "                fold_metrics['mean_val_score'] = np.mean(val_scores)\n",
    "                fold_metrics['score_consistency'] = 1 - abs(np.mean(train_scores) - np.mean(val_scores)) / (abs(np.mean(train_scores)) + 1e-6)\n",
    "                \n",
    "                fold_results.append(fold_metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in fold {fold_idx} with params {params}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if fold_results:\n",
    "            # Aggregate fold results\n",
    "            avg_metrics = {}\n",
    "            for metric in fold_results[0].keys():\n",
    "                values = [fold[metric] for fold in fold_results if metric in fold]\n",
    "                if values:\n",
    "                    avg_metrics[f'mean_{metric}'] = np.mean(values)\n",
    "                    avg_metrics[f'std_{metric}'] = np.std(values)\n",
    "            \n",
    "            # Calculate composite score\n",
    "            composite_score = 0\n",
    "            score_components = 0\n",
    "            \n",
    "            if 'mean_silhouette' in avg_metrics and avg_metrics['mean_silhouette'] > -1:\n",
    "                composite_score += avg_metrics['mean_silhouette'] * 0.4\n",
    "                score_components += 0.4\n",
    "            \n",
    "            if 'mean_stability' in avg_metrics:\n",
    "                composite_score += avg_metrics['mean_stability'] * 0.3\n",
    "                score_components += 0.3\n",
    "            \n",
    "            if 'mean_score_consistency' in avg_metrics:\n",
    "                composite_score += avg_metrics['mean_score_consistency'] * 0.2\n",
    "                score_components += 0.2\n",
    "            \n",
    "            # Penalty for too many or too few support vectors\n",
    "            if 'mean_sv_ratio' in avg_metrics:\n",
    "                sv_penalty = 0\n",
    "                if avg_metrics['mean_sv_ratio'] > 0.8:  # Too many SVs\n",
    "                    sv_penalty = (avg_metrics['mean_sv_ratio'] - 0.8) * 0.5\n",
    "                elif avg_metrics['mean_sv_ratio'] < 0.1:  # Too few SVs\n",
    "                    sv_penalty = (0.1 - avg_metrics['mean_sv_ratio']) * 0.5\n",
    "                composite_score -= sv_penalty\n",
    "                score_components += 0.1\n",
    "            \n",
    "            if score_components > 0:\n",
    "                composite_score /= score_components\n",
    "            \n",
    "            result = {\n",
    "                'params': params,\n",
    "                'composite_score': composite_score,\n",
    "                **avg_metrics\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define parameter grid for optimization\n",
    "param_grid = {\n",
    "    'nu': [0.05, 0.08, 0.1, 0.12, 0.15],\n",
    "    'gamma': ['auto', 'scale', 0.001, 0.01, 0.1, 1.0],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "print(\"üîÑ Running One-Class SVM cross-validation...\")\n",
    "cv_results = cross_validate_one_class_svm(base_df[['Age']].values, param_grid)\n",
    "\n",
    "if len(cv_results) > 0:\n",
    "    # Find best parameters\n",
    "    best_result = cv_results.loc[cv_results['composite_score'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Parameters:\")\n",
    "    print(f\"Parameters: {best_result['params']}\")\n",
    "    print(f\"Composite Score: {best_result['composite_score']:.4f}\")\n",
    "    print(f\"Mean Silhouette: {best_result.get('mean_silhouette', 'N/A'):.4f}\")\n",
    "    print(f\"Mean Stability: {best_result.get('mean_stability', 'N/A'):.4f}\")\n",
    "    print(f\"Support Vector Ratio: {best_result.get('mean_sv_ratio', 'N/A'):.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå No valid parameter combinations found\")\n",
    "```\n",
    "\n",
    "#### **2. Outlier Stability Analysis**\n",
    "\n",
    "```python\n",
    "def analyze_one_class_svm_stability(X, nu, gamma, kernel='rbf', n_trials=10):\n",
    "    \"\"\"\n",
    "    Analyze stability of One-Class SVM outlier detection across multiple runs\n",
    "    \"\"\"\n",
    "    \n",
    "    stability_metrics = {\n",
    "        'outlier_ratios': [],\n",
    "        'outlier_sets': [],\n",
    "        'decision_scores': [],\n",
    "        'support_vector_counts': []\n",
    "    }\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Use different random states for bootstrap sampling\n",
    "        n_samples = len(X)\n",
    "        bootstrap_indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        X_bootstrap = X[bootstrap_indices]\n",
    "        \n",
    "        model = OneClassSVM(nu=nu, gamma=gamma, kernel=kernel, random_state=trial)\n",
    "        predictions = model.fit_predict(X_bootstrap)\n",
    "        scores = model.decision_function(X_bootstrap)\n",
    "        \n",
    "        # Map back to original indices\n",
    "        outlier_mask = predictions == -1\n",
    "        original_outlier_indices = set(bootstrap_indices[outlier_mask])\n",
    "        \n",
    "        # Record metrics\n",
    "        outlier_ratio = np.sum(predictions == -1) / len(predictions)\n",
    "        n_support_vectors = len(model.support_vectors_)\n",
    "        \n",
    "        stability_metrics['outlier_ratios'].append(outlier_ratio)\n",
    "        stability_metrics['outlier_sets'].append(original_outlier_indices)\n",
    "        stability_metrics['decision_scores'].append(scores)\n",
    "        stability_metrics['support_vector_counts'].append(n_support_vectors)\n",
    "    \n",
    "    # Calculate stability statistics\n",
    "    outlier_ratio_cv = np.std(stability_metrics['outlier_ratios']) / np.mean(stability_metrics['outlier_ratios'])\n",
    "    \n",
    "    # Outlier consensus analysis\n",
    "    all_outliers = set()\n",
    "    for outlier_set in stability_metrics['outlier_sets']:\n",
    "        all_outliers.update(outlier_set)\n",
    "    \n",
    "    # Count how many times each point was detected as outlier\n",
    "    outlier_counts = {}\n",
    "    for outlier_set in stability_metrics['outlier_sets']:\n",
    "        for outlier_idx in outlier_set:\n",
    "            outlier_counts[outlier_idx] = outlier_counts.get(outlier_idx, 0) + 1\n",
    "    \n",
    "    # Consensus outliers (detected in majority of runs)\n",
    "    consensus_threshold = n_trials * 0.6  # 60% consensus\n",
    "    consensus_outliers = [idx for idx, count in outlier_counts.items() \n",
    "                         if count >= consensus_threshold]\n",
    "    \n",
    "    # Support vector stability\n",
    "    sv_count_cv = np.std(stability_metrics['support_vector_counts']) / np.mean(stability_metrics['support_vector_counts'])\n",
    "    \n",
    "    stability_results = {\n",
    "        'outlier_ratio_cv': outlier_ratio_cv,\n",
    "        'sv_count_cv': sv_count_cv,\n",
    "        'consensus_outliers': consensus_outliers,\n",
    "        'outlier_detection_counts': outlier_counts,\n",
    "        'mean_outlier_ratio': np.mean(stability_metrics['outlier_ratios']),\n",
    "        'outlier_ratio_range': (min(stability_metrics['outlier_ratios']), \n",
    "                              max(stability_metrics['outlier_ratios'])),\n",
    "        'mean_sv_count': np.mean(stability_metrics['support_vector_counts'])\n",
    "    }\n",
    "    \n",
    "    print(f\"Stability Analysis (nu={nu}, gamma={gamma}):\")\n",
    "    print(f\"  Outlier ratio CV: {outlier_ratio_cv:.4f} (lower is more stable)\")\n",
    "    print(f\"  Support vector CV: {sv_count_cv:.4f} (lower is more stable)\")\n",
    "    print(f\"  Consensus outliers: {len(consensus_outliers)} points\")\n",
    "    print(f\"  Mean outlier ratio: {stability_results['mean_outlier_ratio']:.3f}\")\n",
    "    print(f\"  Outlier ratio range: {stability_results['outlier_ratio_range'][0]:.3f} - {stability_results['outlier_ratio_range'][1]:.3f}\")\n",
    "    \n",
    "    return stability_results\n",
    "\n",
    "# Test stability with best parameters\n",
    "if len(cv_results) > 0:\n",
    "    best_params = best_result['params']\n",
    "    stability_results = analyze_one_class_svm_stability(\n",
    "        base_df[['Age']].values,\n",
    "        best_params['nu'],\n",
    "        best_params['gamma']\n",
    "    )\n",
    "```\n",
    "\n",
    "#### **3. Business Validation and Interpretability**\n",
    "\n",
    "```python\n",
    "def validate_one_class_svm_business_logic(X, feature_names, model, outlier_predictions):\n",
    "    \"\"\"\n",
    "    Validate One-Class SVM results from business perspective\n",
    "    \"\"\"\n",
    "    \n",
    "    outlier_indices = np.where(outlier_predictions == -1)[0]\n",
    "    normal_indices = np.where(outlier_predictions == 1)[0]\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # 1. Feature distribution analysis\n",
    "    print(\"üìä Business Validation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        outlier_values = X[outlier_indices, i]\n",
    "        normal_values = X[normal_indices, i]\n",
    "        \n",
    "        if len(outlier_values) > 0 and len(normal_values) > 0:\n",
    "            # Statistical tests\n",
    "            from scipy.stats import mannwhitneyu\n",
    "            stat, p_value = mannwhitneyu(outlier_values, normal_values, alternative='two-sided')\n",
    "            \n",
    "            # Effect size (Cohen's d equivalent for non-parametric)\n",
    "            outlier_median = np.median(outlier_values)\n",
    "            normal_median = np.median(normal_values)\n",
    "            pooled_mad = np.median(np.abs(np.concatenate([outlier_values, normal_values]) - \n",
    "                                        np.median(np.concatenate([outlier_values, normal_values]))))\n",
    "            effect_size = abs(outlier_median - normal_median) / (pooled_mad + 1e-6)\n",
    "            \n",
    "            validation_results[feature_name] = {\n",
    "                'outlier_median': outlier_median,\n",
    "                'normal_median': normal_median,\n",
    "                'difference': outlier_median - normal_median,\n",
    "                'effect_size': effect_size,\n",
    "                'mannwhitney_p': p_value,\n",
    "                'significant_difference': p_value < 0.05\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{feature_name}:\")\n",
    "            print(f\"  Outlier median: {outlier_median:.2f}\")\n",
    "            print(f\"  Normal median: {normal_median:.2f}\")\n",
    "            print(f\"  Difference: {outlier_median - normal_median:.2f}\")\n",
    "            print(f\"  Effect size: {effect_size:.3f}\")\n",
    "            print(f\"  Statistical significance: {'Yes' if p_value < 0.05 else 'No'} (p={p_value:.4f})\")\n",
    "    \n",
    "    # 2. Decision boundary analysis\n",
    "    decision_scores = model.decision_function(X)\n",
    "    \n",
    "    print(f\"\\nüéØ Decision Boundary Analysis:\")\n",
    "    print(f\"  Decision score range: {decision_scores.min():.3f} to {decision_scores.max():.3f}\")\n",
    "    print(f\"  Mean score (outliers): {decision_scores[outlier_indices].mean():.3f}\")\n",
    "    print(f\"  Mean score (normal): {decision_scores[normal_indices].mean():.3f}\")\n",
    "    \n",
    "    # 3. Model complexity assessment\n",
    "    n_support_vectors = len(model.support_vectors_)\n",
    "    sv_ratio = n_support_vectors / len(X)\n",
    "    \n",
    "    print(f\"\\nüîß Model Complexity:\")\n",
    "    print(f\"  Support vectors: {n_support_vectors} ({sv_ratio:.1%} of data)\")\n",
    "    \n",
    "    if sv_ratio > 0.8:\n",
    "        print(\"  ‚ö†Ô∏è  Warning: Very high support vector ratio - model may be overfitting\")\n",
    "    elif sv_ratio < 0.1:\n",
    "        print(\"  ‚ö†Ô∏è  Warning: Very low support vector ratio - model may be underfitting\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ Support vector ratio looks reasonable\")\n",
    "    \n",
    "    # 4. Outlier characteristics summary\n",
    "    print(f\"\\nüìà Outlier Summary:\")\n",
    "    print(f\"  Total outliers: {len(outlier_indices)} ({len(outlier_indices)/len(X)*100:.1f}%)\")\n",
    "    print(f\"  Expected outliers (nu): {model.nu*100:.1f}%\")\n",
    "    \n",
    "    ratio_difference = abs(len(outlier_indices)/len(X) - model.nu)\n",
    "    if ratio_difference > 0.05:\n",
    "        print(f\"  ‚ö†Ô∏è  Warning: Large difference between expected and actual outlier ratio\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Outlier ratio close to expected value\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Apply business validation\n",
    "if len(cv_results) > 0:\n",
    "    best_model = OneClassSVM(**best_result['params'], random_state=42)\n",
    "    best_predictions = best_model.fit_predict(base_df[['Age']].values)\n",
    "    \n",
    "    business_validation = validate_one_class_svm_business_logic(\n",
    "        base_df[['Age']].values,\n",
    "        ['Age'],\n",
    "        best_model,\n",
    "        best_predictions\n",
    "    )\n",
    "```\n",
    "\n",
    "### üìù One-Class SVM Parameter Validation Checklist\n",
    "\n",
    "#### **‚úÖ Parameters are Well-Tuned When:**\n",
    "\n",
    "1. **Nu Parameter Validation:**\n",
    "   - Actual outlier ratio ‚âà nu parameter ¬±5%\n",
    "   - Outliers are statistically different from normal points\n",
    "   - Business logic confirms outliers make sense\n",
    "\n",
    "2. **Gamma Parameter Validation:**\n",
    "   - High cross-validation stability\n",
    "   - Reasonable support vector ratio (10%-80%)\n",
    "   - Good silhouette score (>0.3)\n",
    "\n",
    "3. **Overall Model Validation:**\n",
    "   - Consistent results across different random seeds\n",
    "   - Statistical significance in feature differences\n",
    "   - Support vector count stable across runs\n",
    "\n",
    "#### **üö® Red Flags (Poor Tuning):**\n",
    "\n",
    "- **Unstable outlier detection**: Different outliers across runs\n",
    "- **Extreme support vector ratios**: <5% or >90%\n",
    "- **No statistical difference**: Outliers statistically similar to normal points\n",
    "- **Business contradiction**: Outliers don't make domain sense\n",
    "\n",
    "### üéØ Complete One-Class SVM Optimization Pipeline\n",
    "\n",
    "```python\n",
    "def optimize_one_class_svm_complete_pipeline(X, feature_names=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline for One-Class SVM optimization and validation\n",
    "    \"\"\"\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    print(\"üîç Step 1: Data preprocessing...\")\n",
    "    # Standardize features for SVM\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(\"üìä Step 2: Parameter estimation...\")\n",
    "    nu_analysis = estimate_nu_for_one_class_svm(X_scaled)\n",
    "    gamma_analysis = estimate_gamma_for_rbf_kernel(X_scaled)\n",
    "    \n",
    "    print(\"üîÑ Step 3: Cross-validation optimization...\")\n",
    "    param_grid = {\n",
    "        'nu': [nu_analysis['conservative'], nu_analysis['median'], nu_analysis['liberal']],\n",
    "        'gamma': [gamma_analysis['conservative'], gamma_analysis['recommended'], gamma_analysis['liberal']],\n",
    "        'kernel': ['rbf']\n",
    "    }\n",
    "    \n",
    "    cv_results = cross_validate_one_class_svm(X_scaled, param_grid)\n",
    "    \n",
    "    if len(cv_results) > 0:\n",
    "        best_result = cv_results.loc[cv_results['composite_score'].idxmax()]\n",
    "        \n",
    "        print(\"üß™ Step 4: Stability analysis...\")\n",
    "        stability_results = analyze_one_class_svm_stability(\n",
    "            X_scaled, \n",
    "            best_result['params']['nu'], \n",
    "            best_result['params']['gamma']\n",
    "        )\n",
    "        \n",
    "        print(\"üíº Step 5: Business validation...\")\n",
    "        final_model = OneClassSVM(**best_result['params'], random_state=42)\n",
    "        final_predictions = final_model.fit_predict(X_scaled)\n",
    "        \n",
    "        business_validation = validate_one_class_svm_business_logic(\n",
    "            X, feature_names, final_model, final_predictions\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüèÜ Final Recommended Parameters:\")\n",
    "        print(f\"nu: {best_result['params']['nu']}\")\n",
    "        print(f\"gamma: {best_result['params']['gamma']}\")\n",
    "        print(f\"kernel: {best_result['params']['kernel']}\")\n",
    "        print(f\"Composite Score: {best_result['composite_score']:.4f}\")\n",
    "        print(f\"Stability (outlier ratio CV): {stability_results['outlier_ratio_cv']:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'recommended_params': best_result['params'],\n",
    "            'final_model': final_model,\n",
    "            'scaler': scaler,\n",
    "            'cv_results': cv_results,\n",
    "            'stability_results': stability_results,\n",
    "            'business_validation': business_validation\n",
    "        }\n",
    "    else:\n",
    "        print(\"‚ùå No suitable parameters found\")\n",
    "        return None\n",
    "\n",
    "# Apply complete pipeline\n",
    "optimal_svm_results = optimize_one_class_svm_complete_pipeline(\n",
    "    base_df[['Age']].values,\n",
    "    feature_names=['Age']\n",
    ")\n",
    "```\n",
    "\n",
    "### üéØ Summary: One-Class SVM Parameter Tuning Best Practices\n",
    "\n",
    "#### **üîß Parameter Selection Strategy:**\n",
    "\n",
    "**For `nu`:**\n",
    "1. **Start with statistical estimates** - IQR, Z-score, Mahalanobis distance\n",
    "2. **Consider domain knowledge** - typical outlier rates in your field\n",
    "3. **Validate with business logic** - check if outliers make sense\n",
    "4. **Test range around estimates** - ¬±0.05 from initial estimate\n",
    "\n",
    "**For `gamma`:**\n",
    "1. **Use data-driven estimates** - distance-based or variance-based\n",
    "2. **Start with 'scale' or 'auto'** - good defaults\n",
    "3. **Cross-validate thoroughly** - gamma is very sensitive\n",
    "4. **Check support vector ratio** - should be 10%-80%\n",
    "\n",
    "#### **üß™ Validation Methodology:**\n",
    "\n",
    "1. **Cross-validation with composite scoring** - stability + silhouette + consistency\n",
    "2. **Bootstrap stability analysis** - consistent outlier detection\n",
    "3. **Statistical validation** - outliers statistically different\n",
    "4. **Business logic validation** - outliers make domain sense\n",
    "5. **Model complexity checks** - reasonable support vector ratio\n",
    "\n",
    "The key to successful One-Class SVM tuning is **combining statistical rigor with business validation** - parameters should optimize both mathematical metrics AND produce interpretable, actionable results! üéØ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
