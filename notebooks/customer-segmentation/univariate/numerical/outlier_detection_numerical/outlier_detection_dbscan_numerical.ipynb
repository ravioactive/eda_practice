{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5fe8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project configuration:\n",
      "SLUG = customer-segmentation\n",
      "DATA_DIR = /Users/ravisharma/workdir/eda_practice/data/customer-segmentation\n",
      "DATASET_KEY = vjchoudhary7/customer-segmentation-tutorial-in-python\n",
      "FIG_DIR = /Users/ravisharma/workdir/eda_practice/figures/customer-segmentation\n",
      "REP_DIR = /Users/ravisharma/workdir/eda_practice/reports/customer-segmentation\n",
      "NOTEBOOK_DIR = /Users/ravisharma/workdir/eda_practice/notebooks/customer-segmentation\n",
      "Vars not found in globals: []\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "\n",
    "print(\"Project configuration:\")\n",
    "print(f\"SLUG = {SLUG}\")\n",
    "print(f\"DATA_DIR = {DATA_DIR}\")\n",
    "print(f\"DATASET_KEY = {DATASET_KEY}\")\n",
    "print(f\"FIG_DIR = {FIG_DIR}\")\n",
    "print(f\"REP_DIR = {REP_DIR}\")\n",
    "print(f\"NOTEBOOK_DIR = {NOTEBOOK_DIR}\")\n",
    "\n",
    "missing_vars = [var for var in ['SLUG', 'DATA_DIR', 'FIG_DIR', 'REP_DIR', 'NOTEBOOK_DIR', 'DATASET_KEY'] if var not in globals()]\n",
    "print(f\"Vars not found in globals: {missing_vars}\")\n",
    "\n",
    "# Set default values if variables are not found in store or are empty\n",
    "if not SLUG:  # Check if empty string\n",
    "    print(f\"{SLUG=} is empty, initializing everything explicitly\")\n",
    "    SLUG = 'customer-segmentation'\n",
    "    DATASET_KEY = 'vjchoudhary7/customer-segmentation-tutorial-in-python'\n",
    "    GIT_ROOT = Path.cwd().parent.parent\n",
    "    DATA_DIR = GIT_ROOT / 'data' / SLUG\n",
    "    FIG_DIR = GIT_ROOT / 'figures' / SLUG\n",
    "    REP_DIR = GIT_ROOT / 'reports' / SLUG\n",
    "    NOTEBOOK_DIR = GIT_ROOT / 'notebooks' / SLUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV /Users/ravisharma/workdir/eda_practice/data/customer-segmentation/Mall_Customers.csv loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
       "0           1    Male   19                  15                      39\n",
       "1           2    Male   21                  15                      81\n",
       "2           3  Female   20                  16                       6\n",
       "3           4  Female   23                  16                      77\n",
       "4           5  Female   31                  17                      40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downloading data\n",
    "\n",
    "base_df = pd.DataFrame()\n",
    "\n",
    "CSV_PATH = Path(DATA_DIR) / \"Mall_Customers.csv\"\n",
    "if not CSV_PATH.exists:\n",
    "    print(f\"CSV {CSV_PATH} does not exist. base_df will remain empty.\")\n",
    "else:\n",
    "    base_df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"CSV {CSV_PATH} loaded successfully.\")\n",
    "\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6096e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>38.850000</td>\n",
       "      <td>60.560000</td>\n",
       "      <td>50.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57.879185</td>\n",
       "      <td>13.969007</td>\n",
       "      <td>26.264721</td>\n",
       "      <td>25.823522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.750000</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>34.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>150.250000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CustomerID         Age  Annual Income (k$)  Spending Score (1-100)\n",
       "count  200.000000  200.000000          200.000000              200.000000\n",
       "mean   100.500000   38.850000           60.560000               50.200000\n",
       "std     57.879185   13.969007           26.264721               25.823522\n",
       "min      1.000000   18.000000           15.000000                1.000000\n",
       "25%     50.750000   28.750000           41.500000               34.750000\n",
       "50%    100.500000   36.000000           61.500000               50.000000\n",
       "75%    150.250000   49.000000           78.000000               73.000000\n",
       "max    200.000000   70.000000          137.000000               99.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3718f7f",
   "metadata": {},
   "source": [
    "## 📊 DBSCAN: Density-Based Clustering for Outlier Detection\n",
    "\n",
    "### 📋 Code Breakdown\n",
    "```python\n",
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2, metric='euclidean')\n",
    "outliers = dbscan.fit_predict(base_df[['Age']])\n",
    "print(outliers)\n",
    "```\n",
    "\n",
    "**Line-by-line explanation:**\n",
    "1. **Import DBSCAN** from sklearn clustering module (originally clustering algorithm, adapted for outlier detection)\n",
    "2. **Create DBSCAN instance** with epsilon=0.5, minimum samples=2, and Euclidean distance\n",
    "3. **Fit and predict** on Age column (returns cluster labels: 0, 1, 2... for clusters, -1 for outliers)\n",
    "4. **Print cluster labels** where -1 indicates noise/outliers\n",
    "\n",
    "### 📚 Essential Documentation & Resources\n",
    "\n",
    "#### **Official Documentation:**\n",
    "- **[Scikit-learn DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)** - Official API reference\n",
    "- **[Scikit-learn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html#dbscan)** - Comprehensive clustering overview\n",
    "- **[Original Paper: \"A Density-Based Algorithm for Discovering Clusters\" by Ester et al. (1996)](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf)** - Foundational research paper\n",
    "\n",
    "#### **Helpful Blogs & Tutorials:**\n",
    "- **[Towards Data Science: DBSCAN Explained](https://towardsdatascience.com/dbscan-algorithm-complete-guide-and-application-with-python-scikit-learn-d690cbae4c5d)**\n",
    "- **[Machine Learning Mastery: DBSCAN Clustering](https://machinelearningmastery.com/dbscan-clustering-algorithm/)**\n",
    "- **[Analytics Vidhya: DBSCAN Clustering Guide](https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/)**\n",
    "\n",
    "#### **Advanced Resources:**\n",
    "- **[DBSCAN Parameter Selection Study](https://iopscience.iop.org/article/10.1088/1742-6596/1168/2/022022)**\n",
    "- **[Density-Based Clustering Comparison](https://link.springer.com/article/10.1007/s10115-016-0964-9)**\n",
    "- **[DBSCAN for Anomaly Detection Applications](https://ieeexplore.ieee.org/document/8844706)**\n",
    "\n",
    "### 🔍 How DBSCAN Works for Outlier Detection\n",
    "\n",
    "#### **Core Algorithm Concept:**\n",
    "1. **Density-Based Clustering**: Groups points in high-density areas\n",
    "2. **Epsilon Neighborhood**: Points within `eps` distance are neighbors\n",
    "3. **Core Points**: Points with ≥ `min_samples` neighbors in their epsilon neighborhood\n",
    "4. **Border Points**: Non-core points within epsilon of a core point\n",
    "5. **Noise Points**: Points that are neither core nor border → **OUTLIERS**\n",
    "\n",
    "#### **Mathematical Foundation:**\n",
    "\n",
    "```python\n",
    "# DBSCAN Algorithm Steps:\n",
    "\n",
    "# 1. For each point p:\n",
    "#    - Find all points within eps distance (epsilon neighborhood)\n",
    "#    - If neighborhood has ≥ min_samples points: p is CORE point\n",
    "\n",
    "# 2. Form clusters:\n",
    "#    - Core points in same eps-neighborhood belong to same cluster\n",
    "#    - Border points belong to cluster of nearest core point\n",
    "\n",
    "# 3. Label remaining points as NOISE (outliers):\n",
    "#    - Points not core and not within eps of any core point = -1 (outliers)\n",
    "\n",
    "# Key Parameters:\n",
    "# eps (epsilon): Maximum distance between points in same neighborhood\n",
    "# min_samples: Minimum points required to form dense region (cluster)\n",
    "```\n",
    "\n",
    "#### **Visual Intuition:**\n",
    "- **Dense regions**: Become clusters (labeled 0, 1, 2, ...)\n",
    "- **Sparse isolated points**: Become noise/outliers (labeled -1)\n",
    "- **Border points**: Assigned to nearest cluster\n",
    "- **Core points**: Centers of dense regions\n",
    "\n",
    "### 📊 Output Interpretation\n",
    "\n",
    "Your output will be an array like: `[0, 0, -1, 1, 1, -1, 0, ...]`\n",
    "\n",
    "**Interpretation:**\n",
    "- **0, 1, 2, ...**: Cluster labels (normal points in dense regions)\n",
    "- **-1**: Noise/outliers (isolated points in sparse regions)\n",
    "\n",
    "**Practical Usage:**\n",
    "```python\n",
    "# Get cluster labels and outlier analysis\n",
    "cluster_labels = dbscan.fit_predict(base_df[['Age']])\n",
    "outlier_mask = cluster_labels == -1\n",
    "outlier_indices = np.where(outlier_mask)[0]\n",
    "outlier_customers = base_df.iloc[outlier_indices]\n",
    "\n",
    "# Cluster analysis\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_outliers = np.sum(outlier_mask)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of outliers: {n_outliers}\")\n",
    "print(f\"Outlier percentage: {n_outliers/len(base_df)*100:.1f}%\")\n",
    "\n",
    "# Analyze each cluster\n",
    "for cluster_id in set(cluster_labels):\n",
    "    if cluster_id != -1:  # Skip outliers\n",
    "        cluster_points = base_df[cluster_labels == cluster_id]\n",
    "        print(f\"\\nCluster {cluster_id}:\")\n",
    "        print(f\"  Size: {len(cluster_points)}\")\n",
    "        print(f\"  Age range: {cluster_points['Age'].min():.1f} - {cluster_points['Age'].max():.1f}\")\n",
    "        print(f\"  Age mean: {cluster_points['Age'].mean():.1f}\")\n",
    "\n",
    "# Show outlier details\n",
    "if n_outliers > 0:\n",
    "    print(f\"\\nOutlier Details:\")\n",
    "    outlier_details = pd.DataFrame({\n",
    "        'Customer_Index': outlier_indices,\n",
    "        'Age': base_df.iloc[outlier_indices]['Age'].values\n",
    "    })\n",
    "    print(outlier_details.sort_values('Age'))\n",
    "```\n",
    "\n",
    "**DBSCAN-Specific Insights:**\n",
    "- **Natural clustering**: Reveals natural age groups in your customer data\n",
    "- **Automatic outlier detection**: No need to specify contamination rate\n",
    "- **Cluster characteristics**: Each cluster represents a customer age segment\n",
    "- **Outliers**: Customers with unusual ages relative to main age groups\n",
    "\n",
    "### ⚖️ DBSCAN vs Other Outlier Detection Methods\n",
    "\n",
    "| **Method** | **Strengths** | **Weaknesses** | **Best Use Case** |\n",
    "|------------|---------------|----------------|-------------------|\n",
    "| **Standard Z-Score** | ✅ Simple, fast<br/>✅ Interpretable<br/>✅ Global outliers | ❌ Assumes normality<br/>❌ Misses local outliers<br/>❌ Univariate only | Normally distributed, global outliers |\n",
    "| **Modified Z-Score** | ✅ Robust to outliers<br/>✅ No normality assumption<br/>✅ Interpretable | ❌ Still global approach<br/>❌ Univariate only<br/>❌ Misses local patterns | Robust univariate outlier detection |\n",
    "| **Isolation Forest** | ✅ Multivariate<br/>✅ No assumptions<br/>✅ Scalable<br/>✅ Global patterns | ❌ Parameter sensitive<br/>❌ Poor with local outliers<br/>❌ Less interpretable | Large datasets, global anomalies |\n",
    "| **Local Outlier Factor** | ✅ Local outliers<br/>✅ Density-aware<br/>✅ Interpretable scores<br/>✅ Handles clusters | ❌ Sensitive to k parameter<br/>❌ O(n²) complexity<br/>❌ High-dimensional issues | Clustered data, local anomalies |\n",
    "| **DBSCAN** | ✅ **Natural clustering + outliers**<br/>✅ **No contamination needed**<br/>✅ **Arbitrary cluster shapes**<br/>✅ **Robust to noise**<br/>✅ **Discovers data structure** | ❌ **Very parameter sensitive**<br/>❌ **Struggles with varying densities**<br/>❌ **High-dimensional curse**<br/>❌ **Difficult parameter tuning** | **Unknown cluster structure, natural groupings** |\n",
    "\n",
    "### 🎯 Detailed Comparison\n",
    "\n",
    "#### **DBSCAN Unique Strengths:**\n",
    "1. **Simultaneous Clustering & Outlier Detection**: One algorithm, dual purpose\n",
    "2. **No Pre-specified Cluster Count**: Automatically determines number of clusters\n",
    "3. **Arbitrary Cluster Shapes**: Not limited to spherical clusters like k-means\n",
    "4. **Natural Outlier Definition**: Points that don't belong to any dense region\n",
    "5. **Robust to Noise**: Designed specifically to handle noisy data\n",
    "6. **No Contamination Parameter**: Doesn't require knowing expected outlier percentage\n",
    "\n",
    "#### **DBSCAN Weaknesses:**\n",
    "1. **Parameter Sensitivity**: Results highly dependent on `eps` and `min_samples`\n",
    "2. **Varying Densities**: Struggles when clusters have different densities\n",
    "3. **High Dimensionality**: Performance degrades with many features (curse of dimensionality)\n",
    "4. **Parameter Selection**: No clear guidelines for choosing optimal parameters\n",
    "5. **Border Point Ambiguity**: Border points can be assigned to different clusters\n",
    "6. **Scalability**: O(n log n) with spatial indexing, O(n²) without\n",
    "\n",
    "### 🚀 Advanced Usage and Parameter Tuning\n",
    "\n",
    "#### **Parameter Selection Strategies:**\n",
    "\n",
    "```python\n",
    "def find_optimal_eps(X, min_samples=2, plot=True):\n",
    "    \"\"\"\n",
    "    Find optimal eps using k-distance plot (elbow method)\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    \n",
    "    # Calculate k-distances (distance to k-th nearest neighbor)\n",
    "    k = min_samples\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "    \n",
    "    # Sort distances to k-th neighbor\n",
    "    k_distances = distances[:, k-1]\n",
    "    k_distances_sorted = np.sort(k_distances)[::-1]\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(len(k_distances_sorted)), k_distances_sorted, 'b-')\n",
    "        plt.xlabel('Points (sorted by distance)')\n",
    "        plt.ylabel(f'{k}-NN Distance')\n",
    "        plt.title('K-Distance Plot for Eps Selection')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Try to find elbow automatically\n",
    "        # Simple elbow detection using second derivative\n",
    "        if len(k_distances_sorted) > 10:\n",
    "            second_deriv = np.diff(k_distances_sorted, 2)\n",
    "            elbow_idx = np.argmax(second_deriv) + 2\n",
    "            suggested_eps = k_distances_sorted[elbow_idx]\n",
    "            plt.axhline(y=suggested_eps, color='r', linestyle='--', \n",
    "                       label=f'Suggested eps: {suggested_eps:.2f}')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return k_distances_sorted\n",
    "\n",
    "def evaluate_dbscan_parameters(X, eps_range, min_samples_range):\n",
    "    \"\"\"\n",
    "    Evaluate different DBSCAN parameters\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for eps in eps_range:\n",
    "        for min_samples in min_samples_range:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(X)\n",
    "            \n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            n_outliers = np.sum(labels == -1)\n",
    "            outlier_ratio = n_outliers / len(X)\n",
    "            \n",
    "            # Calculate silhouette score if we have clusters\n",
    "            if n_clusters > 1 and n_outliers < len(X):\n",
    "                from sklearn.metrics import silhouette_score\n",
    "                # Remove outliers for silhouette calculation\n",
    "                mask = labels != -1\n",
    "                if np.sum(mask) > 1:\n",
    "                    silhouette = silhouette_score(X[mask], labels[mask])\n",
    "                else:\n",
    "                    silhouette = -1\n",
    "            else:\n",
    "                silhouette = -1\n",
    "            \n",
    "            results.append({\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'n_clusters': n_clusters,\n",
    "                'n_outliers': n_outliers,\n",
    "                'outlier_ratio': outlier_ratio,\n",
    "                'silhouette_score': silhouette\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Apply parameter optimization\n",
    "k_distances = find_optimal_eps(base_df[['Age']], min_samples=3)\n",
    "\n",
    "# Test parameter ranges\n",
    "eps_range = np.arange(0.5, 5.0, 0.5)\n",
    "min_samples_range = [2, 3, 4, 5]\n",
    "\n",
    "param_results = evaluate_dbscan_parameters(base_df[['Age']], eps_range, min_samples_range)\n",
    "\n",
    "# Find best parameters (balance between clusters and silhouette score)\n",
    "best_params = param_results.loc[\n",
    "    (param_results['n_clusters'] > 0) & \n",
    "    (param_results['n_clusters'] < 10) &\n",
    "    (param_results['silhouette_score'] > 0)\n",
    "].sort_values('silhouette_score', ascending=False).iloc[0]\n",
    "\n",
    "print(\"Best DBSCAN Parameters:\")\n",
    "print(f\"eps: {best_params['eps']}\")\n",
    "print(f\"min_samples: {best_params['min_samples']}\")\n",
    "print(f\"Results: {best_params['n_clusters']} clusters, {best_params['n_outliers']} outliers\")\n",
    "```\n",
    "\n",
    "#### **Enhanced DBSCAN Implementation:**\n",
    "\n",
    "```python\n",
    "def enhanced_dbscan_analysis(X, eps=None, min_samples=None):\n",
    "    \"\"\"\n",
    "    Enhanced DBSCAN with automatic parameter selection and detailed analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Auto-select parameters if not provided\n",
    "    if eps is None or min_samples is None:\n",
    "        # Rule of thumb: min_samples = 2 * dimensions\n",
    "        if min_samples is None:\n",
    "            min_samples = max(2, 2 * X.shape[1])\n",
    "        \n",
    "        if eps is None:\n",
    "            # Use k-distance method\n",
    "            from sklearn.neighbors import NearestNeighbors\n",
    "            k = min_samples\n",
    "            nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
    "            distances, _ = nbrs.kneighbors(X)\n",
    "            k_distances = np.sort(distances[:, k-1])[::-1]\n",
    "            \n",
    "            # Simple elbow detection\n",
    "            if len(k_distances) > 10:\n",
    "                second_deriv = np.diff(k_distances, 2)\n",
    "                elbow_idx = np.argmax(second_deriv) + 2\n",
    "                eps = k_distances[elbow_idx]\n",
    "            else:\n",
    "                eps = np.mean(k_distances)\n",
    "    \n",
    "    # Apply DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "    labels = dbscan.fit_predict(X)\n",
    "    \n",
    "    # Detailed analysis\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_outliers = np.sum(labels == -1)\n",
    "    \n",
    "    analysis = {\n",
    "        'parameters': {'eps': eps, 'min_samples': min_samples},\n",
    "        'results': {\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_outliers': n_outliers,\n",
    "            'outlier_ratio': n_outliers / len(X),\n",
    "            'cluster_labels': labels\n",
    "        },\n",
    "        'cluster_details': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze each cluster\n",
    "    for cluster_id in set(labels):\n",
    "        if cluster_id != -1:\n",
    "            cluster_mask = labels == cluster_id\n",
    "            cluster_data = X[cluster_mask]\n",
    "            \n",
    "            analysis['cluster_details'][cluster_id] = {\n",
    "                'size': np.sum(cluster_mask),\n",
    "                'mean': np.mean(cluster_data, axis=0),\n",
    "                'std': np.std(cluster_data, axis=0),\n",
    "                'min': np.min(cluster_data, axis=0),\n",
    "                'max': np.max(cluster_data, axis=0)\n",
    "            }\n",
    "    \n",
    "    return analysis, dbscan\n",
    "\n",
    "# Apply enhanced DBSCAN\n",
    "analysis, dbscan_model = enhanced_dbscan_analysis(base_df[['Age']].values)\n",
    "\n",
    "print(\"Enhanced DBSCAN Analysis:\")\n",
    "print(f\"Parameters used: eps={analysis['parameters']['eps']:.2f}, min_samples={analysis['parameters']['min_samples']}\")\n",
    "print(f\"Found {analysis['results']['n_clusters']} clusters and {analysis['results']['n_outliers']} outliers\")\n",
    "print(f\"Outlier percentage: {analysis['results']['outlier_ratio']*100:.1f}%\")\n",
    "\n",
    "for cluster_id, details in analysis['cluster_details'].items():\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    print(f\"  Size: {details['size']}\")\n",
    "    print(f\"  Age range: {details['min'][0]:.1f} - {details['max'][0]:.1f}\")\n",
    "    print(f\"  Age mean ± std: {details['mean'][0]:.1f} ± {details['std'][0]:.1f}\")\n",
    "```\n",
    "\n",
    "### 🎯 When to Use DBSCAN for Outlier Detection\n",
    "\n",
    "**✅ Use DBSCAN when:**\n",
    "- **Unknown cluster structure** - want to discover natural groupings\n",
    "- **Arbitrary cluster shapes** - not limited to spherical clusters\n",
    "- **Simultaneous clustering + outlier detection** needed\n",
    "- **No prior knowledge** of outlier percentage\n",
    "- **Robust noise handling** required\n",
    "- **Moderate dataset size** (<10,000 points)\n",
    "\n",
    "**❌ Don't use DBSCAN when:**\n",
    "- **High-dimensional data** (>10 features) - curse of dimensionality\n",
    "- **Uniform density required** - struggles with varying densities\n",
    "- **Parameter tuning difficult** - unclear how to set eps/min_samples\n",
    "- **Only outlier detection needed** - simpler methods may be better\n",
    "- **Very large datasets** - scalability issues\n",
    "- **Real-time applications** - parameter sensitivity makes it unreliable\n",
    "\n",
    "### 🏆 Recommendation for Your Customer Segmentation\n",
    "\n",
    "For customer segmentation analysis, **DBSCAN is valuable** because:\n",
    "\n",
    "1. **Natural customer segments**: Discovers age-based customer groups automatically\n",
    "2. **Outlier identification**: Finds customers who don't fit standard age segments  \n",
    "3. **No assumptions**: Doesn't assume specific number of customer segments\n",
    "4. **Business insights**: Clusters represent actionable customer segments\n",
    "\n",
    "**Optimal implementation for your case:**\n",
    "```python\n",
    "# Recommended DBSCAN setup for customer age analysis\n",
    "def customer_dbscan_analysis(customer_data):\n",
    "    \"\"\"\n",
    "    DBSCAN optimized for customer segmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conservative parameters for business data\n",
    "    # Start with rule-of-thumb and refine\n",
    "    min_samples = 3  # At least 3 customers to form a segment\n",
    "    \n",
    "    # Use k-distance plot to find eps\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=min_samples).fit(customer_data[['Age']])\n",
    "    distances, _ = nbrs.kneighbors(customer_data[['Age']])\n",
    "    k_distances = np.sort(distances[:, min_samples-1])[::-1]\n",
    "    \n",
    "    # Take 90th percentile as eps (conservative approach)\n",
    "    eps = np.percentile(k_distances, 90)\n",
    "    \n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "    \n",
    "    return dbscan, eps\n",
    "\n",
    "# Apply to your customer data\n",
    "optimal_dbscan, suggested_eps = customer_dbscan_analysis(base_df)\n",
    "cluster_labels = optimal_dbscan.fit_predict(base_df[['Age']])\n",
    "\n",
    "print(f\"🎯 DBSCAN Customer Segmentation Results:\")\n",
    "print(f\"Using eps={suggested_eps:.2f}, min_samples=3\")\n",
    "\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_outliers = np.sum(cluster_labels == -1)\n",
    "\n",
    "print(f\"Discovered {n_clusters} customer age segments\")\n",
    "print(f\"Identified {n_outliers} outlier customers ({n_outliers/len(base_df)*100:.1f}%)\")\n",
    "\n",
    "# Business interpretation\n",
    "if n_outliers > 0:\n",
    "    outlier_customers = base_df[cluster_labels == -1]\n",
    "    print(f\"\\nOutlier customers have ages: {sorted(outlier_customers['Age'].values)}\")\n",
    "    print(\"These customers may need special attention or represent niche segments!\")\n",
    "```\n",
    "\n",
    "### 🎯 Summary: DBSCAN in Your Outlier Detection Arsenal\n",
    "\n",
    "**Perfect Complementary Approach:**\n",
    "1. **Z-Score**: Global statistical outliers\n",
    "2. **Modified Z-Score**: Robust global outliers  \n",
    "3. **Isolation Forest**: Multivariate global anomalies\n",
    "4. **LOF**: Local density-based outliers\n",
    "5. **DBSCAN**: **Clustering-based outliers + customer segmentation**\n",
    "\n",
    "**Use DBSCAN specifically when you want to:**\n",
    "- **Discover natural customer segments** while finding outliers\n",
    "- **No prior assumptions** about number of customer groups\n",
    "- **Find customers who don't belong to any major segment**\n",
    "- **Combine clustering and outlier detection** in one step\n",
    "\n",
    "DBSCAN is unique because it's primarily a **clustering algorithm that identifies outliers as a byproduct** - making it perfect for customer segmentation where you want both insights into customer groups AND identification of unusual customers! 🎯\n",
    "\n",
    "**Key Takeaway**: DBSCAN gives you the most **business-actionable results** because outliers are defined as \"customers who don't belong to any natural customer segment\" - which is exactly what businesses want to know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdf584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project configuration:\n",
      "SLUG = customer-segmentation\n",
      "DATA_DIR = /Users/ravisharma/workdir/eda_practice/data/customer-segmentation\n",
      "DATASET_KEY = vjchoudhary7/customer-segmentation-tutorial-in-python\n",
      "FIG_DIR = /Users/ravisharma/workdir/eda_practice/figures/customer-segmentation\n",
      "REP_DIR = /Users/ravisharma/workdir/eda_practice/reports/customer-segmentation\n",
      "NOTEBOOK_DIR = /Users/ravisharma/workdir/eda_practice/notebooks/customer-segmentation\n",
      "Vars not found in globals: []\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "\n",
    "print(\"Project configuration:\")\n",
    "print(f\"SLUG = {SLUG}\")\n",
    "print(f\"DATA_DIR = {DATA_DIR}\")\n",
    "print(f\"DATASET_KEY = {DATASET_KEY}\")\n",
    "print(f\"FIG_DIR = {FIG_DIR}\")\n",
    "print(f\"REP_DIR = {REP_DIR}\")\n",
    "print(f\"NOTEBOOK_DIR = {NOTEBOOK_DIR}\")\n",
    "\n",
    "missing_vars = [var for var in ['SLUG', 'DATA_DIR', 'FIG_DIR', 'REP_DIR', 'NOTEBOOK_DIR', 'DATASET_KEY'] if var not in globals()]\n",
    "print(f\"Vars not found in globals: {missing_vars}\")\n",
    "\n",
    "# Set default values if variables are not found in store or are empty\n",
    "if not SLUG:  # Check if empty string\n",
    "    print(f\"{SLUG=} is empty, initializing everything explicitly\")\n",
    "    SLUG = 'customer-segmentation'\n",
    "    DATASET_KEY = 'vjchoudhary7/customer-segmentation-tutorial-in-python'\n",
    "    GIT_ROOT = Path.cwd().parent.parent\n",
    "    DATA_DIR = GIT_ROOT / 'data' / SLUG\n",
    "    FIG_DIR = GIT_ROOT / 'figures' / SLUG\n",
    "    REP_DIR = GIT_ROOT / 'reports' / SLUG\n",
    "    NOTEBOOK_DIR = GIT_ROOT / 'notebooks' / SLUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca12080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV /Users/ravisharma/workdir/eda_practice/data/customer-segmentation/Mall_Customers.csv loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
       "0           1    Male   19                  15                      39\n",
       "1           2    Male   21                  15                      81\n",
       "2           3  Female   20                  16                       6\n",
       "3           4  Female   23                  16                      77\n",
       "4           5  Female   31                  17                      40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downloading data\n",
    "\n",
    "base_df = pd.DataFrame()\n",
    "\n",
    "CSV_PATH = Path(DATA_DIR) / \"Mall_Customers.csv\"\n",
    "if not CSV_PATH.exists:\n",
    "    print(f\"CSV {CSV_PATH} does not exist. base_df will remain empty.\")\n",
    "else:\n",
    "    base_df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"CSV {CSV_PATH} loaded successfully.\")\n",
    "\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c222bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>38.850000</td>\n",
       "      <td>60.560000</td>\n",
       "      <td>50.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57.879185</td>\n",
       "      <td>13.969007</td>\n",
       "      <td>26.264721</td>\n",
       "      <td>25.823522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.750000</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>34.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>150.250000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CustomerID         Age  Annual Income (k$)  Spending Score (1-100)\n",
       "count  200.000000  200.000000          200.000000              200.000000\n",
       "mean   100.500000   38.850000           60.560000               50.200000\n",
       "std     57.879185   13.969007           26.264721               25.823522\n",
       "min      1.000000   18.000000           15.000000                1.000000\n",
       "25%     50.750000   28.750000           41.500000               34.750000\n",
       "50%    100.500000   36.000000           61.500000               50.000000\n",
       "75%    150.250000   49.000000           78.000000               73.000000\n",
       "max    200.000000   70.000000          137.000000               99.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad4f039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  3 -1  7  8  6  9 10 11  5  6  2 12  6  6 13 14  4\n",
      " 15 16 17  6 18  3 19  1 20 21 22  1 23  7 24  2 25 10 26  4 22 10 27 28\n",
      " 16  4 22 29  4 30 27 31 32 -1 28 20 33  0  8 15 34 21 35 36  0 37 33 31\n",
      " 19 19 30 38 17 18  3 22 39 40  8 14  1 26 -1  5 41 27 36 21 26 18 37 10\n",
      " 31 28 26  2  3 22  8 38 22  1 42 15 36 42 25  0 40  0 21  0 34 22 32 27\n",
      " 28 40 18 43  3  4 35 18 30 40 31 43 13  4  2 16 44 37  0  6 39 37 45 37\n",
      " 13 45 26 37 41 41 35 43 44 40 31 28 11  7 41  7 -1 16  0  4 27 24 23 29\n",
      " 24 37 18 45 24 24 12  7  9 28 30  6 11 37 14 16 46  7 15 45 46 24 41 37\n",
      " 29 40 31  6 17 37 37  7]\n"
     ]
    }
   ],
   "source": [
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2, metric='euclidean')\n",
    "outliers = dbscan.fit_predict(base_df[['Age']])\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e904f",
   "metadata": {},
   "source": [
    "## 🎯 DBSCAN Parameter Tuning: Comprehensive Guide\n",
    "\n",
    "### 📊 Core Parameters and Data-Driven Selection\n",
    "\n",
    "#### **1. `eps` (Epsilon) - The Most Critical Parameter**\n",
    "\n",
    "**What it controls:** Maximum distance between points to be considered neighbors\n",
    "\n",
    "**Data-driven selection methods:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def k_distance_plot_analysis(X, k_range=None, plot=True):\n",
    "    \"\"\"\n",
    "    Comprehensive k-distance analysis for eps selection\n",
    "    \"\"\"\n",
    "    \n",
    "    if k_range is None:\n",
    "        # Test multiple k values to understand data structure\n",
    "        k_range = [2, 3, 4, 5, 6]\n",
    "    \n",
    "    eps_suggestions = {}\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Calculate k-distances\n",
    "        nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
    "        distances, indices = nbrs.kneighbors(X)\n",
    "        \n",
    "        # Get k-th nearest neighbor distances\n",
    "        k_distances = distances[:, k-1]\n",
    "        k_distances_sorted = np.sort(k_distances)[::-1]\n",
    "        \n",
    "        # Multiple methods for elbow detection\n",
    "        \n",
    "        # Method 1: Maximum curvature (second derivative)\n",
    "        if len(k_distances_sorted) > 10:\n",
    "            # Smooth the curve first\n",
    "            from scipy.ndimage import gaussian_filter1d\n",
    "            smoothed = gaussian_filter1d(k_distances_sorted, sigma=2)\n",
    "            \n",
    "            # Calculate second derivative\n",
    "            second_deriv = np.diff(smoothed, 2)\n",
    "            elbow_idx = np.argmax(second_deriv) + 2\n",
    "            eps_curvature = k_distances_sorted[elbow_idx]\n",
    "        else:\n",
    "            eps_curvature = np.median(k_distances_sorted)\n",
    "        \n",
    "        # Method 2: Percentile-based (conservative)\n",
    "        eps_90th = np.percentile(k_distances_sorted, 90)\n",
    "        eps_95th = np.percentile(k_distances_sorted, 95)\n",
    "        \n",
    "        # Method 3: Mean + std (statistical)\n",
    "        eps_mean_std = np.mean(k_distances_sorted) + np.std(k_distances_sorted)\n",
    "        \n",
    "        # Method 4: Largest gap method\n",
    "        gaps = np.diff(k_distances_sorted)\n",
    "        largest_gap_idx = np.argmax(gaps)\n",
    "        eps_gap = k_distances_sorted[largest_gap_idx]\n",
    "        \n",
    "        eps_suggestions[k] = {\n",
    "            'curvature': eps_curvature,\n",
    "            '90th_percentile': eps_90th,\n",
    "            '95th_percentile': eps_95th,\n",
    "            'mean_plus_std': eps_mean_std,\n",
    "            'largest_gap': eps_gap,\n",
    "            'distances': k_distances_sorted\n",
    "        }\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.subplot(2, 2, k-1 if k <= 4 else 1)\n",
    "            plt.plot(range(len(k_distances_sorted)), k_distances_sorted, 'b-', linewidth=2)\n",
    "            plt.axhline(y=eps_curvature, color='r', linestyle='--', \n",
    "                       label=f'Curvature: {eps_curvature:.3f}')\n",
    "            plt.axhline(y=eps_90th, color='g', linestyle='--', \n",
    "                       label=f'90th percentile: {eps_90th:.3f}')\n",
    "            plt.axhline(y=eps_gap, color='orange', linestyle='--', \n",
    "                       label=f'Largest gap: {eps_gap:.3f}')\n",
    "            \n",
    "            plt.xlabel('Points (sorted by distance)')\n",
    "            plt.ylabel(f'{k}-NN Distance')\n",
    "            plt.title(f'K-Distance Plot (k={k})')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if plot:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return eps_suggestions\n",
    "\n",
    "def analyze_data_characteristics(X):\n",
    "    \"\"\"\n",
    "    Analyze data characteristics to inform parameter selection\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    stats = {\n",
    "        'n_samples': n_samples,\n",
    "        'n_features': n_features,\n",
    "        'data_range': np.ptp(X, axis=0),  # Peak-to-peak range\n",
    "        'data_std': np.std(X, axis=0),\n",
    "        'data_mean': np.mean(X, axis=0)\n",
    "    }\n",
    "    \n",
    "    # Estimate data density\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(10, n_samples-1)).fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "    avg_density = np.mean(distances[:, -1])  # Average distance to 10th neighbor\n",
    "    \n",
    "    stats['estimated_density'] = avg_density\n",
    "    \n",
    "    # Dimensionality considerations\n",
    "    if n_features == 1:\n",
    "        stats['dimensionality_type'] = 'univariate'\n",
    "        stats['suggested_min_samples'] = max(3, int(np.sqrt(n_samples) * 0.1))\n",
    "    elif n_features <= 3:\n",
    "        stats['dimensionality_type'] = 'low_dimensional'\n",
    "        stats['suggested_min_samples'] = max(4, int(np.sqrt(n_samples) * 0.15))\n",
    "    elif n_features <= 10:\n",
    "        stats['dimensionality_type'] = 'medium_dimensional'\n",
    "        stats['suggested_min_samples'] = max(2 * n_features, int(np.sqrt(n_samples) * 0.2))\n",
    "    else:\n",
    "        stats['dimensionality_type'] = 'high_dimensional'\n",
    "        stats['suggested_min_samples'] = max(2 * n_features, int(np.sqrt(n_samples) * 0.3))\n",
    "        print(\"⚠️ Warning: DBSCAN may not perform well with high-dimensional data\")\n",
    "    \n",
    "    # Sample size considerations\n",
    "    if n_samples < 50:\n",
    "        stats['sample_size_category'] = 'very_small'\n",
    "        stats['eps_adjustment'] = 'increase'  # Need larger eps for connectivity\n",
    "    elif n_samples < 200:\n",
    "        stats['sample_size_category'] = 'small'\n",
    "        stats['eps_adjustment'] = 'moderate'\n",
    "    elif n_samples < 1000:\n",
    "        stats['sample_size_category'] = 'medium'\n",
    "        stats['eps_adjustment'] = 'standard'\n",
    "    else:\n",
    "        stats['sample_size_category'] = 'large'\n",
    "        stats['eps_adjustment'] = 'decrease'  # Can use smaller eps\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Apply comprehensive analysis to your data\n",
    "print(\"🔍 Analyzing data characteristics...\")\n",
    "data_stats = analyze_data_characteristics(base_df[['Age']].values)\n",
    "\n",
    "print(\"Data Characteristics:\")\n",
    "for key, value in data_stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n📊 K-distance analysis...\")\n",
    "eps_analysis = k_distance_plot_analysis(base_df[['Age']].values)\n",
    "\n",
    "# Synthesize recommendations\n",
    "print(f\"\\n💡 Eps Recommendations:\")\n",
    "for k, suggestions in eps_analysis.items():\n",
    "    print(f\"  k={k}:\")\n",
    "    for method, value in suggestions.items():\n",
    "        if method != 'distances':\n",
    "            print(f\"    {method}: {value:.3f}\")\n",
    "```\n",
    "\n",
    "#### **2. `min_samples` - Minimum Cluster Size**\n",
    "\n",
    "**What it controls:** Minimum number of points required to form a dense region\n",
    "\n",
    "**Heuristic Rules for min_samples:**\n",
    "\n",
    "```python\n",
    "def suggest_min_samples(X, domain_knowledge=None):\n",
    "    \"\"\"\n",
    "    Suggest min_samples based on data characteristics and domain knowledge\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    suggestions = {}\n",
    "    \n",
    "    # Rule 1: Classic heuristic (2 * dimensions)\n",
    "    suggestions['classic_heuristic'] = 2 * n_features\n",
    "    \n",
    "    # Rule 2: Statistical rule based on sample size\n",
    "    if n_samples < 100:\n",
    "        suggestions['sample_size_based'] = max(2, int(n_samples * 0.02))  # 2% of data\n",
    "    elif n_samples < 500:\n",
    "        suggestions['sample_size_based'] = max(3, int(n_samples * 0.015))  # 1.5% of data\n",
    "    else:\n",
    "        suggestions['sample_size_based'] = max(4, int(n_samples * 0.01))   # 1% of data\n",
    "    \n",
    "    # Rule 3: Square root rule\n",
    "    suggestions['sqrt_rule'] = max(2, int(np.sqrt(n_samples) * 0.2))\n",
    "    \n",
    "    # Rule 4: Domain-specific adjustments\n",
    "    if domain_knowledge:\n",
    "        if 'noise_level' in domain_knowledge:\n",
    "            noise_level = domain_knowledge['noise_level']  # 'low', 'medium', 'high'\n",
    "            if noise_level == 'low':\n",
    "                suggestions['noise_adjusted'] = min(suggestions.values())  # More sensitive\n",
    "            elif noise_level == 'medium':\n",
    "                suggestions['noise_adjusted'] = int(np.median(list(suggestions.values())))\n",
    "            else:  # high noise\n",
    "                suggestions['noise_adjusted'] = max(suggestions.values())  # Less sensitive\n",
    "        \n",
    "        if 'min_cluster_size' in domain_knowledge:\n",
    "            # Business requirement for minimum meaningful cluster size\n",
    "            suggestions['business_requirement'] = domain_knowledge['min_cluster_size']\n",
    "    \n",
    "    # Rule 5: Data density consideration\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(10, n_samples-1)).fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "    \n",
    "    # If data is very sparse, need higher min_samples\n",
    "    avg_10th_distance = np.mean(distances[:, -1])\n",
    "    data_range = np.ptp(X)\n",
    "    relative_sparsity = avg_10th_distance / data_range\n",
    "    \n",
    "    if relative_sparsity > 0.1:  # Sparse data\n",
    "        suggestions['density_adjusted'] = max(suggestions.values()) + 1\n",
    "    else:  # Dense data\n",
    "        suggestions['density_adjusted'] = min(max(2, min(suggestions.values()) - 1), \n",
    "                                            max(suggestions.values()))\n",
    "    \n",
    "    # Final recommendation (conservative approach)\n",
    "    final_suggestion = int(np.median(list(suggestions.values())))\n",
    "    \n",
    "    print(\"Min_samples suggestions:\")\n",
    "    for method, value in suggestions.items():\n",
    "        print(f\"  {method}: {value}\")\n",
    "    print(f\"Final recommendation: {final_suggestion}\")\n",
    "    \n",
    "    return final_suggestion, suggestions\n",
    "\n",
    "# Apply min_samples analysis\n",
    "domain_knowledge = {\n",
    "    'noise_level': 'medium',  # Customer data typically has moderate noise\n",
    "    'min_cluster_size': 3     # Want at least 3 customers per segment\n",
    "}\n",
    "\n",
    "optimal_min_samples, min_samples_analysis = suggest_min_samples(\n",
    "    base_df[['Age']].values, \n",
    "    domain_knowledge\n",
    ")\n",
    "```\n",
    "\n",
    "### 🧪 DBSCAN-Specific Validation Methodologies\n",
    "\n",
    "#### **1. Parameter Grid Search with Multiple Metrics**\n",
    "\n",
    "```python\n",
    "def comprehensive_dbscan_validation(X, eps_range=None, min_samples_range=None):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of DBSCAN parameters using multiple metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    if eps_range is None:\n",
    "        # Auto-generate eps range based on k-distance analysis\n",
    "        eps_analysis = k_distance_plot_analysis(X, plot=False)\n",
    "        eps_suggestions = []\n",
    "        for k_suggestions in eps_analysis.values():\n",
    "            eps_suggestions.extend([\n",
    "                k_suggestions['curvature'],\n",
    "                k_suggestions['90th_percentile'],\n",
    "                k_suggestions['largest_gap']\n",
    "            ])\n",
    "        \n",
    "        eps_min = min(eps_suggestions) * 0.5\n",
    "        eps_max = max(eps_suggestions) * 2.0\n",
    "        eps_range = np.linspace(eps_min, eps_max, 15)\n",
    "    \n",
    "    if min_samples_range is None:\n",
    "        min_samples_range = [2, 3, 4, 5, 6, 8, 10]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for eps in eps_range:\n",
    "        for min_samples in min_samples_range:\n",
    "            try:\n",
    "                # Fit DBSCAN\n",
    "                dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "                labels = dbscan.fit_predict(X)\n",
    "                \n",
    "                # Basic metrics\n",
    "                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                n_outliers = np.sum(labels == -1)\n",
    "                outlier_ratio = n_outliers / len(X)\n",
    "                \n",
    "                # Advanced metrics\n",
    "                metrics = {\n",
    "                    'eps': eps,\n",
    "                    'min_samples': min_samples,\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'n_outliers': n_outliers,\n",
    "                    'outlier_ratio': outlier_ratio\n",
    "                }\n",
    "                \n",
    "                # Silhouette Score (only if we have clusters and not all points are outliers)\n",
    "                if n_clusters > 1 and n_outliers < len(X) * 0.9:\n",
    "                    mask = labels != -1\n",
    "                    if np.sum(mask) > 1 and len(set(labels[mask])) > 1:\n",
    "                        silhouette = silhouette_score(X[mask], labels[mask])\n",
    "                        metrics['silhouette_score'] = silhouette\n",
    "                    else:\n",
    "                        metrics['silhouette_score'] = -1\n",
    "                else:\n",
    "                    metrics['silhouette_score'] = -1\n",
    "                \n",
    "                # Calinski-Harabasz Score (variance ratio)\n",
    "                if n_clusters > 1 and n_outliers < len(X) * 0.9:\n",
    "                    mask = labels != -1\n",
    "                    if np.sum(mask) > n_clusters and len(set(labels[mask])) > 1:\n",
    "                        from sklearn.metrics import calinski_harabasz_score\n",
    "                        ch_score = calinski_harabasz_score(X[mask], labels[mask])\n",
    "                        metrics['calinski_harabasz_score'] = ch_score\n",
    "                    else:\n",
    "                        metrics['calinski_harabasz_score'] = 0\n",
    "                else:\n",
    "                    metrics['calinski_harabasz_score'] = 0\n",
    "                \n",
    "                # Davies-Bouldin Score (lower is better)\n",
    "                if n_clusters > 1 and n_outliers < len(X) * 0.9:\n",
    "                    mask = labels != -1\n",
    "                    if np.sum(mask) > n_clusters and len(set(labels[mask])) > 1:\n",
    "                        from sklearn.metrics import davies_bouldin_score\n",
    "                        db_score = davies_bouldin_score(X[mask], labels[mask])\n",
    "                        metrics['davies_bouldin_score'] = db_score\n",
    "                    else:\n",
    "                        metrics['davies_bouldin_score'] = float('inf')\n",
    "                else:\n",
    "                    metrics['davies_bouldin_score'] = float('inf')\n",
    "                \n",
    "                # Cluster size statistics\n",
    "                if n_clusters > 0:\n",
    "                    cluster_sizes = []\n",
    "                    for cluster_id in set(labels):\n",
    "                        if cluster_id != -1:\n",
    "                            cluster_sizes.append(np.sum(labels == cluster_id))\n",
    "                    \n",
    "                    metrics['min_cluster_size'] = min(cluster_sizes) if cluster_sizes else 0\n",
    "                    metrics['max_cluster_size'] = max(cluster_sizes) if cluster_sizes else 0\n",
    "                    metrics['avg_cluster_size'] = np.mean(cluster_sizes) if cluster_sizes else 0\n",
    "                    metrics['cluster_size_std'] = np.std(cluster_sizes) if cluster_sizes else 0\n",
    "                else:\n",
    "                    metrics.update({\n",
    "                        'min_cluster_size': 0,\n",
    "                        'max_cluster_size': 0,\n",
    "                        'avg_cluster_size': 0,\n",
    "                        'cluster_size_std': 0\n",
    "                    })\n",
    "                \n",
    "                # Custom quality score (weighted combination of metrics)\n",
    "                quality_score = 0\n",
    "                \n",
    "                # Prefer reasonable number of clusters\n",
    "                if 2 <= n_clusters <= min(10, len(X) // 10):\n",
    "                    quality_score += 2\n",
    "                elif n_clusters == 1:\n",
    "                    quality_score += 0.5\n",
    "                elif n_clusters == 0:\n",
    "                    quality_score -= 2\n",
    "                else:\n",
    "                    quality_score -= 1\n",
    "                \n",
    "                # Prefer reasonable outlier ratio\n",
    "                if 0.01 <= outlier_ratio <= 0.2:\n",
    "                    quality_score += 2\n",
    "                elif outlier_ratio <= 0.3:\n",
    "                    quality_score += 1\n",
    "                elif outlier_ratio > 0.5:\n",
    "                    quality_score -= 2\n",
    "                \n",
    "                # Add silhouette score contribution\n",
    "                if metrics['silhouette_score'] > 0:\n",
    "                    quality_score += metrics['silhouette_score'] * 2\n",
    "                \n",
    "                # Penalize extreme parameter values\n",
    "                if eps < 0.1 or eps > 10:\n",
    "                    quality_score -= 1\n",
    "                if min_samples > len(X) // 5:\n",
    "                    quality_score -= 1\n",
    "                \n",
    "                metrics['quality_score'] = quality_score\n",
    "                \n",
    "                results.append(metrics)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with eps={eps:.3f}, min_samples={min_samples}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Apply comprehensive validation\n",
    "print(\"🧪 Running comprehensive parameter validation...\")\n",
    "validation_results = comprehensive_dbscan_validation(base_df[['Age']].values)\n",
    "\n",
    "# Find best parameters\n",
    "best_results = validation_results[\n",
    "    (validation_results['n_clusters'] > 0) &\n",
    "    (validation_results['n_clusters'] <= 8) &\n",
    "    (validation_results['outlier_ratio'] <= 0.3) &\n",
    "    (validation_results['silhouette_score'] > 0)\n",
    "].sort_values('quality_score', ascending=False)\n",
    "\n",
    "if len(best_results) > 0:\n",
    "    best_params = best_results.iloc[0]\n",
    "    print(f\"\\n🏆 Best Parameters Found:\")\n",
    "    print(f\"eps: {best_params['eps']:.3f}\")\n",
    "    print(f\"min_samples: {int(best_params['min_samples'])}\")\n",
    "    print(f\"Results: {int(best_params['n_clusters'])} clusters, {int(best_params['n_outliers'])} outliers\")\n",
    "    print(f\"Quality Score: {best_params['quality_score']:.3f}\")\n",
    "    print(f\"Silhouette Score: {best_params['silhouette_score']:.3f}\")\n",
    "else:\n",
    "    print(\"⚠️ No suitable parameters found. Consider adjusting parameter ranges.\")\n",
    "```\n",
    "\n",
    "#### **2. Stability Analysis**\n",
    "\n",
    "```python\n",
    "def dbscan_stability_analysis(X, eps, min_samples, n_trials=10, sample_fraction=0.8):\n",
    "    \"\"\"\n",
    "    Analyze DBSCAN stability across different subsamples\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    sample_size = int(n_samples * sample_fraction)\n",
    "    \n",
    "    stability_metrics = {\n",
    "        'cluster_counts': [],\n",
    "        'outlier_ratios': [],\n",
    "        'cluster_assignments': [],\n",
    "        'silhouette_scores': []\n",
    "    }\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Random subsample\n",
    "        np.random.seed(trial)\n",
    "        sample_indices = np.random.choice(n_samples, sample_size, replace=False)\n",
    "        X_sample = X[sample_indices]\n",
    "        \n",
    "        # Apply DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(X_sample)\n",
    "        \n",
    "        # Record metrics\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        outlier_ratio = np.sum(labels == -1) / len(X_sample)\n",
    "        \n",
    "        stability_metrics['cluster_counts'].append(n_clusters)\n",
    "        stability_metrics['outlier_ratios'].append(outlier_ratio)\n",
    "        \n",
    "        # Silhouette score\n",
    "        if n_clusters > 1 and outlier_ratio < 0.9:\n",
    "            mask = labels != -1\n",
    "            if np.sum(mask) > 1 and len(set(labels[mask])) > 1:\n",
    "                silhouette = silhouette_score(X_sample[mask], labels[mask])\n",
    "                stability_metrics['silhouette_scores'].append(silhouette)\n",
    "            else:\n",
    "                stability_metrics['silhouette_scores'].append(-1)\n",
    "        else:\n",
    "            stability_metrics['silhouette_scores'].append(-1)\n",
    "        \n",
    "        # Store cluster assignments for consensus analysis\n",
    "        full_labels = np.full(n_samples, -2)  # -2 for not sampled\n",
    "        full_labels[sample_indices] = labels\n",
    "        stability_metrics['cluster_assignments'].append(full_labels)\n",
    "    \n",
    "    # Calculate stability statistics\n",
    "    cluster_count_cv = np.std(stability_metrics['cluster_counts']) / np.mean(stability_metrics['cluster_counts']) if np.mean(stability_metrics['cluster_counts']) > 0 else float('inf')\n",
    "    outlier_ratio_cv = np.std(stability_metrics['outlier_ratios']) / np.mean(stability_metrics['outlier_ratios']) if np.mean(stability_metrics['outlier_ratios']) > 0 else float('inf')\n",
    "    \n",
    "    valid_silhouettes = [s for s in stability_metrics['silhouette_scores'] if s > -1]\n",
    "    avg_silhouette = np.mean(valid_silhouettes) if valid_silhouettes else -1\n",
    "    silhouette_cv = np.std(valid_silhouettes) / np.mean(valid_silhouettes) if valid_silhouettes and np.mean(valid_silhouettes) > 0 else float('inf')\n",
    "    \n",
    "    # Consensus outlier detection\n",
    "    outlier_consensus = np.zeros(n_samples)\n",
    "    for assignment in stability_metrics['cluster_assignments']:\n",
    "        outlier_consensus += (assignment == -1).astype(int)\n",
    "    \n",
    "    consensus_outliers = outlier_consensus >= (n_trials * 0.5)  # Majority vote\n",
    "    \n",
    "    results = {\n",
    "        'cluster_count_stability': cluster_count_cv,\n",
    "        'outlier_ratio_stability': outlier_ratio_cv,\n",
    "        'avg_silhouette': avg_silhouette,\n",
    "        'silhouette_stability': silhouette_cv,\n",
    "        'consensus_outliers': consensus_outliers,\n",
    "        'stability_metrics': stability_metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"Stability Analysis (eps={eps:.3f}, min_samples={min_samples}):\")\n",
    "    print(f\"  Cluster count CV: {cluster_count_cv:.3f} (lower is more stable)\")\n",
    "    print(f\"  Outlier ratio CV: {outlier_ratio_cv:.3f} (lower is more stable)\")\n",
    "    print(f\"  Average silhouette: {avg_silhouette:.3f}\")\n",
    "    print(f\"  Silhouette CV: {silhouette_cv:.3f} (lower is more stable)\")\n",
    "    print(f\"  Consensus outliers: {np.sum(consensus_outliers)} points\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test stability of best parameters\n",
    "if len(best_results) > 0:\n",
    "    stability_results = dbscan_stability_analysis(\n",
    "        base_df[['Age']].values,\n",
    "        best_params['eps'],\n",
    "        int(best_params['min_samples'])\n",
    "    )\n",
    "```\n",
    "\n",
    "#### **3. Business Logic Validation**\n",
    "\n",
    "```python\n",
    "def business_validation_dbscan(X, labels, domain_constraints, feature_names=None):\n",
    "    \"\"\"\n",
    "    Validate DBSCAN results against business logic\n",
    "    \"\"\"\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # Overall cluster analysis\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_outliers = np.sum(labels == -1)\n",
    "    outlier_ratio = n_outliers / len(X)\n",
    "    \n",
    "    validation_results['cluster_summary'] = {\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_outliers': n_outliers,\n",
    "        'outlier_ratio': outlier_ratio\n",
    "    }\n",
    "    \n",
    "    # Check against business constraints\n",
    "    expected_clusters = domain_constraints.get('expected_clusters', None)\n",
    "    max_outlier_ratio = domain_constraints.get('max_outlier_ratio', 0.2)\n",
    "    min_cluster_size = domain_constraints.get('min_cluster_size', 3)\n",
    "    \n",
    "    # Validation flags\n",
    "    validation_results['validations'] = {}\n",
    "    \n",
    "    if expected_clusters:\n",
    "        cluster_count_ok = abs(n_clusters - expected_clusters) <= 2\n",
    "        validation_results['validations']['cluster_count_reasonable'] = cluster_count_ok\n",
    "    \n",
    "    outlier_ratio_ok = outlier_ratio <= max_outlier_ratio\n",
    "    validation_results['validations']['outlier_ratio_acceptable'] = outlier_ratio_ok\n",
    "    \n",
    "    # Cluster size validation\n",
    "    cluster_sizes = []\n",
    "    cluster_details = {}\n",
    "    \n",
    "    for cluster_id in set(labels):\n",
    "        if cluster_id != -1:\n",
    "            cluster_mask = labels == cluster_id\n",
    "            cluster_data = X[cluster_mask]\n",
    "            cluster_size = np.sum(cluster_mask)\n",
    "            cluster_sizes.append(cluster_size)\n",
    "            \n",
    "            cluster_details[cluster_id] = {\n",
    "                'size': cluster_size,\n",
    "                'percentage': cluster_size / len(X) * 100,\n",
    "                'centroid': np.mean(cluster_data, axis=0),\n",
    "                'spread': np.std(cluster_data, axis=0)\n",
    "            }\n",
    "    \n",
    "    min_size_ok = all(size >= min_cluster_size for size in cluster_sizes)\n",
    "    validation_results['validations']['min_cluster_size_met'] = min_size_ok\n",
    "    \n",
    "    # Feature-specific validations (for customer age analysis)\n",
    "    if 'Age' in feature_names or len(feature_names) == 1:\n",
    "        age_idx = feature_names.index('Age') if 'Age' in feature_names else 0\n",
    "        \n",
    "        # Outlier age analysis\n",
    "        if n_outliers > 0:\n",
    "            outlier_ages = X[labels == -1, age_idx]\n",
    "            min_reasonable_age = domain_constraints.get('min_reasonable_age', 16)\n",
    "            max_reasonable_age = domain_constraints.get('max_reasonable_age', 80)\n",
    "            \n",
    "            reasonable_outliers = outlier_ages[\n",
    "                (outlier_ages >= min_reasonable_age) & \n",
    "                (outlier_ages <= max_reasonable_age)\n",
    "            ]\n",
    "            \n",
    "            validation_results['outlier_analysis'] = {\n",
    "                'outlier_ages': sorted(outlier_ages),\n",
    "                'reasonable_outliers_ratio': len(reasonable_outliers) / len(outlier_ages),\n",
    "                'age_range': (outlier_ages.min(), outlier_ages.max())\n",
    "            }\n",
    "        \n",
    "        # Cluster age analysis\n",
    "        for cluster_id, details in cluster_details.items():\n",
    "            age_mean = details['centroid'][age_idx]\n",
    "            age_std = details['spread'][age_idx]\n",
    "            \n",
    "            # Check if cluster represents a reasonable age group\n",
    "            cluster_reasonable = (\n",
    "                age_mean >= min_reasonable_age and \n",
    "                age_mean <= max_reasonable_age and\n",
    "                age_std < 15  # Age groups shouldn't be too spread out\n",
    "            )\n",
    "            \n",
    "            cluster_details[cluster_id]['age_reasonable'] = cluster_reasonable\n",
    "            cluster_details[cluster_id]['age_stats'] = {\n",
    "                'mean': age_mean,\n",
    "                'std': age_std\n",
    "            }\n",
    "    \n",
    "    validation_results['cluster_details'] = cluster_details\n",
    "    \n",
    "    # Overall validation score\n",
    "    validation_score = sum(validation_results['validations'].values()) / len(validation_results['validations'])\n",
    "    validation_results['overall_validation_score'] = validation_score\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Apply business validation\n",
    "if len(best_results) > 0:\n",
    "    # Run DBSCAN with best parameters\n",
    "    final_dbscan = DBSCAN(eps=best_params['eps'], min_samples=int(best_params['min_samples']))\n",
    "    final_labels = final_dbscan.fit_predict(base_df[['Age']].values)\n",
    "    \n",
    "    # Define business constraints for customer segmentation\n",
    "    business_constraints = {\n",
    "        'expected_clusters': 4,  # Expect around 3-5 customer age segments\n",
    "        'max_outlier_ratio': 0.15,  # Max 15% outliers acceptable\n",
    "        'min_cluster_size': 5,  # Each segment should have at least 5 customers\n",
    "        'min_reasonable_age': 18,\n",
    "        'max_reasonable_age': 70\n",
    "    }\n",
    "    \n",
    "    business_validation = business_validation_dbscan(\n",
    "        base_df[['Age']].values,\n",
    "        final_labels,\n",
    "        business_constraints,\n",
    "        ['Age']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📋 Business Validation Results:\")\n",
    "    print(f\"Overall validation score: {business_validation['overall_validation_score']:.2f}\")\n",
    "    \n",
    "    for validation, passed in business_validation['validations'].items():\n",
    "        status = \"✅\" if passed else \"❌\"\n",
    "        print(f\"  {status} {validation}: {passed}\")\n",
    "    \n",
    "    if 'outlier_analysis' in business_validation:\n",
    "        print(f\"\\nOutlier Analysis:\")\n",
    "        print(f\"  Outlier ages: {business_validation['outlier_analysis']['outlier_ages']}\")\n",
    "        print(f\"  Reasonable outliers: {business_validation['outlier_analysis']['reasonable_outliers_ratio']:.2f}\")\n",
    "```\n",
    "\n",
    "### 🎯 Complete DBSCAN Parameter Tuning Pipeline\n",
    "\n",
    "```python\n",
    "def complete_dbscan_tuning_pipeline(X, feature_names=None, domain_knowledge=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline for DBSCAN parameter optimization and validation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔍 Step 1: Analyzing data characteristics...\")\n",
    "    data_stats = analyze_data_characteristics(X)\n",
    "    \n",
    "    print(f\"\\n📊 Step 2: K-distance analysis for eps selection...\")\n",
    "    eps_analysis = k_distance_plot_analysis(X, plot=True)\n",
    "    \n",
    "    print(f\"\\n🎯 Step 3: Min_samples suggestions...\")\n",
    "    optimal_min_samples, min_samples_analysis = suggest_min_samples(X, domain_knowledge)\n",
    "    \n",
    "    print(f\"\\n🧪 Step 4: Comprehensive parameter validation...\")\n",
    "    validation_results = comprehensive_dbscan_validation(X)\n",
    "    \n",
    "    # Filter and rank results\n",
    "    good_results = validation_results[\n",
    "        (validation_results['n_clusters'] > 0) &\n",
    "        (validation_results['n_clusters'] <= 10) &\n",
    "        (validation_results['outlier_ratio'] <= 0.4) &\n",
    "        (validation_results['silhouette_score'] > 0)\n",
    "    ].sort_values('quality_score', ascending=False)\n",
    "    \n",
    "    if len(good_results) == 0:\n",
    "        print(\"⚠️ No good parameter combinations found. Relaxing constraints...\")\n",
    "        good_results = validation_results[\n",
    "            (validation_results['n_clusters'] > 0)\n",
    "        ].sort_values('quality_score', ascending=False)\n",
    "    \n",
    "    if len(good_results) > 0:\n",
    "        # Test top 3 candidates for stability\n",
    "        top_candidates = good_results.head(3)\n",
    "        \n",
    "        print(f\"\\n🔬 Step 5: Stability analysis of top candidates...\")\n",
    "        stability_results = []\n",
    "        \n",
    "        for _, candidate in top_candidates.iterrows():\n",
    "            print(f\"\\nTesting eps={candidate['eps']:.3f}, min_samples={int(candidate['min_samples'])}\")\n",
    "            stability = dbscan_stability_analysis(\n",
    "                X, candidate['eps'], int(candidate['min_samples']), n_trials=5\n",
    "            )\n",
    "            stability['params'] = candidate\n",
    "            stability_results.append(stability)\n",
    "        \n",
    "        # Choose most stable candidate\n",
    "        best_stability = min(stability_results, \n",
    "                           key=lambda x: x['cluster_count_stability'] + x['outlier_ratio_stability'])\n",
    "        \n",
    "        final_params = {\n",
    "            'eps': best_stability['params']['eps'],\n",
    "            'min_samples': int(best_stability['params']['min_samples'])\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n🏆 Final Recommended Parameters:\")\n",
    "        print(f\"eps: {final_params['eps']:.3f}\")\n",
    "        print(f\"min_samples: {final_params['min_samples']}\")\n",
    "        \n",
    "        # Create final model and validate\n",
    "        final_dbscan = DBSCAN(**final_params)\n",
    "        final_labels = final_dbscan.fit_predict(X)\n",
    "        \n",
    "        print(f\"\\n📋 Step 6: Business validation...\")\n",
    "        if domain_knowledge and 'business_constraints' in domain_knowledge:\n",
    "            business_validation = business_validation_dbscan(\n",
    "                X, final_labels, domain_knowledge['business_constraints'], feature_names\n",
    "            )\n",
    "            \n",
    "            print(f\"Business validation score: {business_validation['overall_validation_score']:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'recommended_params': final_params,\n",
    "            'final_model': final_dbscan,\n",
    "            'final_labels': final_labels,\n",
    "            'validation_results': validation_results,\n",
    "            'stability_analysis': best_stability,\n",
    "            'data_analysis': data_stats\n",
    "        }\n",
    "    else:\n",
    "        print(\"❌ No suitable parameters found. Consider:\")\n",
    "        print(\"  - Checking data preprocessing\")\n",
    "        print(\"  - Using different distance metrics\")\n",
    "        print(\"  - Considering alternative clustering methods\")\n",
    "        return None\n",
    "\n",
    "# Apply complete pipeline\n",
    "domain_knowledge_full = {\n",
    "    'noise_level': 'medium',\n",
    "    'min_cluster_size': 3,\n",
    "    'business_constraints': {\n",
    "        'expected_clusters': 4,\n",
    "        'max_outlier_ratio': 0.15,\n",
    "        'min_cluster_size': 5,\n",
    "        'min_reasonable_age': 18,\n",
    "        'max_reasonable_age': 70\n",
    "    }\n",
    "}\n",
    "\n",
    "optimal_dbscan_results = complete_dbscan_tuning_pipeline(\n",
    "    base_df[['Age']].values,\n",
    "    feature_names=['Age'],\n",
    "    domain_knowledge=domain_knowledge_full\n",
    ")\n",
    "```\n",
    "\n",
    "### 📝 DBSCAN Parameter Validation Checklist\n",
    "\n",
    "#### **✅ Parameters are Well-Tuned When:**\n",
    "\n",
    "1. **Eps Parameter Validation:**\n",
    "   - Clear elbow in k-distance plot\n",
    "   - Reasonable number of clusters (2-10 for most business cases)\n",
    "   - Not too many tiny clusters or single giant cluster\n",
    "   - Stable across different k values in k-distance analysis\n",
    "\n",
    "2. **Min_samples Parameter Validation:**\n",
    "   - Clusters have meaningful minimum size for business context\n",
    "   - Not too sensitive to noise (stable results)\n",
    "   - Balanced between over-clustering and under-clustering\n",
    "\n",
    "3. **Overall Model Validation:**\n",
    "   - High silhouette score (> 0.3)\n",
    "   - Reasonable outlier ratio (typically 5-20%)\n",
    "   - Stable results across subsamples\n",
    "   - Business logic validation passes\n",
    "   - Cluster sizes make business sense\n",
    "\n",
    "#### **🚨 Red Flags (Poor Tuning):**\n",
    "\n",
    "- **No clear clusters**: eps too small or too large\n",
    "- **Everything is noise**: eps too small or min_samples too large  \n",
    "- **Single giant cluster**: eps too large\n",
    "- **Highly unstable**: Different results on subsamples\n",
    "- **Unreasonable outliers**: Outliers don't make business sense\n",
    "- **Poor separation**: Clusters overlap significantly\n",
    "\n",
    "### 🎯 Specific Recommendations for Customer Segmentation\n",
    "\n",
    "```python\n",
    "def customer_segmentation_dbscan_optimizer(customer_data):\n",
    "    \"\"\"\n",
    "    Specialized DBSCAN optimizer for customer segmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🎯 Customer Segmentation DBSCAN Optimization\")\n",
    "    \n",
    "    # Customer-specific parameter ranges\n",
    "    age_range = customer_data['Age'].max() - customer_data['Age'].min()\n",
    "    \n",
    "    # Conservative eps range (5-15% of age range)\n",
    "    eps_range = np.linspace(age_range * 0.05, age_range * 0.15, 10)\n",
    "    \n",
    "    # Business-appropriate min_samples\n",
    "    n_customers = len(customer_data)\n",
    "    if n_customers < 100:\n",
    "        min_samples_range = [2, 3, 4]\n",
    "    elif n_customers < 300:\n",
    "        min_samples_range = [3, 4, 5, 6]\n",
    "    else:\n",
    "        min_samples_range = [4, 5, 6, 8, 10]\n",
    "    \n",
    "    print(f\"Testing eps range: {eps_range.min():.2f} - {eps_range.max():.2f}\")\n",
    "    print(f\"Testing min_samples: {min_samples_range}\")\n",
    "    \n",
    "    # Run validation\n",
    "    results = comprehensive_dbscan_validation(\n",
    "        customer_data[['Age']].values,\n",
    "        eps_range,\n",
    "        min_samples_range\n",
    "    )\n",
    "    \n",
    "    # Business-focused filtering\n",
    "    business_results = results[\n",
    "        (results['n_clusters'] >= 2) &  # At least 2 customer segments\n",
    "        (results['n_clusters'] <= 6) &  # Not too many segments\n",
    "        (results['outlier_ratio'] <= 0.2) &  # Max 20% outliers\n",
    "        (results['min_cluster_size'] >= 3)  # Each segment has at least 3 customers\n",
    "    ]\n",
    "    \n",
    "    if len(business_results) > 0:\n",
    "        # Prefer solutions with good silhouette and reasonable cluster count\n",
    "        business_results['business_score'] = (\n",
    "            business_results['silhouette_score'] * 2 +\n",
    "            (1 / business_results['n_clusters']) * 0.5 +  # Prefer fewer clusters\n",
    "            (1 - business_results['outlier_ratio']) * 1  # Prefer fewer outliers\n",
    "        )\n",
    "        \n",
    "        best_business = business_results.sort_values('business_score', ascending=False).iloc[0]\n",
    "        \n",
    "        print(f\"\\n🎯 Customer Segmentation Results:\")\n",
    "        print(f\"Optimal eps: {best_business['eps']:.2f}\")\n",
    "        print(f\"Optimal min_samples: {int(best_business['min_samples'])}\")\n",
    "        print(f\"Customer segments: {int(best_business['n_clusters'])}\")\n",
    "        print(f\"Outlier customers: {int(best_business['n_outliers'])} ({best_business['outlier_ratio']*100:.1f}%)\")\n",
    "        print(f\"Silhouette score: {best_business['silhouette_score']:.3f}\")\n",
    "        \n",
    "        # Apply final model\n",
    "        final_dbscan = DBSCAN(eps=best_business['eps'], min_samples=int(best_business['min_samples']))\n",
    "        labels = final_dbscan.fit_predict(customer_data[['Age']].values)\n",
    "        \n",
    "        # Segment analysis\n",
    "        print(f\"\\n📊 Customer Segment Analysis:\")\n",
    "        for cluster_id in sorted(set(labels)):\n",
    "            if cluster_id == -1:\n",
    "                segment_customers = customer_data[labels == cluster_id]\n",
    "                print(f\"Outlier customers: {len(segment_customers)} customers\")\n",
    "                if len(segment_customers) > 0:\n",
    "                    ages = segment_customers['Age'].values\n",
    "                    print(f\"  Ages: {sorted(ages)}\")\n",
    "            else:\n",
    "                segment_customers = customer_data[labels == cluster_id]\n",
    "                ages = segment_customers['Age']\n",
    "                print(f\"Segment {cluster_id}: {len(segment_customers)} customers\")\n",
    "                print(f\"  Age range: {ages.min():.0f} - {ages.max():.0f}\")\n",
    "                print(f\"  Average age: {ages.mean():.1f} ± {ages.std():.1f}\")\n",
    "        \n",
    "        return final_dbscan, labels, best_business\n",
    "    else:\n",
    "        print(\"⚠️ No suitable parameters found for customer segmentation\")\n",
    "        return None, None, None\n",
    "\n",
    "# Apply customer-specific optimization\n",
    "customer_dbscan, customer_labels, customer_results = customer_segmentation_dbscan_optimizer(base_df)\n",
    "```\n",
    "\n",
    "### 🎯 Summary: DBSCAN Parameter Tuning Best Practices\n",
    "\n",
    "#### **🔧 Parameter Selection Rules:**\n",
    "\n",
    "**For `eps`:**\n",
    "1. **Use k-distance plots** - look for elbow point\n",
    "2. **Test multiple k values** (2, 3, 4, 5) for robustness\n",
    "3. **Consider data density** - sparse data needs larger eps\n",
    "4. **Domain scaling** - eps should be meaningful in your domain units\n",
    "\n",
    "**For `min_samples`:**\n",
    "1. **Start with 2 × dimensions** (classic heuristic)\n",
    "2. **Consider noise level** - higher noise needs larger min_samples\n",
    "3. **Business constraints** - minimum meaningful cluster size\n",
    "4. **Sample size** - 1-2% of dataset size as starting point\n",
    "\n",
    "#### **🧪 Validation Methodology:**\n",
    "\n",
    "1. **Multi-metric evaluation** - silhouette, Calinski-Harabasz, Davies-Bouldin\n",
    "2. **Stability analysis** - consistent results across subsamples\n",
    "3. **Business validation** - clusters make domain sense\n",
    "4. **Parameter sensitivity** - robust to small parameter changes\n",
    "\n",
    "#### **🎯 For Customer Segmentation:**\n",
    "\n",
    "- **eps**: 5-15% of age range (typically 2-8 years for age data)\n",
    "- **min_samples**: 3-6 customers per segment\n",
    "- **Expected clusters**: 2-6 customer segments\n",
    "- **Max outliers**: 10-20% of customers\n",
    "\n",
    "The key is **iterative refinement** - start with data-driven estimates, validate with multiple metrics, and adjust based on business requirements! 🎯"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
