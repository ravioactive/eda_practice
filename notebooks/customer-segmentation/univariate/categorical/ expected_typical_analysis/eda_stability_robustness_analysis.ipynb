{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How are these analysis done? Explain test by test. Share not just the code but also the mathematical background, explaining each quantity in the formula. Mention why does it work - what part of this test behaves in what way to detect a certain result. Connect this with interpretability of the test, and explain how to interpret it with the current dataset of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Stability & Robustness Analysis for Categorical Data**\n",
        "\n",
        "## **Comprehensive Mathematical Framework with Mall Customers Dataset**\n",
        "\n",
        "This notebook provides a complete implementation of stability and robustness analysis for categorical data, with detailed mathematical explanations and practical interpretations using the Mall Customers dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency, bootstrap\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/Users/ravisharma/workdir/eda_practice/data/customer-segmentation/Mall_Customers.csv')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Categorical variable (Gender) distribution:\")\n",
        "print(df['Gender'].value_counts())\n",
        "print(f\"Proportions: {df['Gender'].value_counts(normalize=True)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **MATHEMATICAL FOUNDATIONS**\n",
        "\n",
        "### **Core Concepts for Categorical Stability Analysis**\n",
        "\n",
        "**1. Distance Metrics for Categorical Distributions:**\n",
        "- **Total Variation Distance**: TV(P,Q) = ½Σᵢ|pᵢ - qᵢ|\n",
        "- **Hellinger Distance**: H(P,Q) = √(½Σᵢ(√pᵢ - √qᵢ)²)  \n",
        "- **Jensen-Shannon Divergence**: JS(P,Q) = ½[KL(P||M) + KL(Q||M)] where M = ½(P+Q)\n",
        "\n",
        "**2. Stability Indices:**\n",
        "- **Temporal Consistency Index**: TCI = 1 - TV(π̂(t₁), π̂(t₂))\n",
        "- **Bootstrap Stability Index**: BSI = 1 - (IQR_bootstrap / median_bootstrap)\n",
        "- **Cross-Validation Stability Index**: CVSI = 1 - (1/k) Σᵢ TV(π̂ᵢ, π̂_full)\n",
        "\n",
        "**3. Statistical Tests:**\n",
        "- **Chi-Square Homogeneity**: χ² = Σᵢⱼ (Oᵢⱼ - Eᵢⱼ)²/Eᵢⱼ\n",
        "- **Multinomial Confidence Intervals**: Using Wilson score or bootstrap methods\n",
        "- **Trend Tests**: Cochran-Armitage for ordered categories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I'll provide a comprehensive explanation of Stability & Robustness Analysis for categorical data, covering both the mathematical foundations and practical implementation. Let me first examine the current dataset to provide context-specific interpretations.\n",
        "\n",
        "# **Stability & Robustness Analysis for Categorical Data**\n",
        "\n",
        "## **Mathematical Foundations & Conceptual Framework**\n",
        "\n",
        "Stability and robustness analysis for categorical data examines how consistent distributional properties remain under various perturbations. Unlike continuous data, categorical distributions are characterized by probability mass functions, requiring specialized approaches.\n",
        "\n",
        "### **Core Mathematical Concepts**\n",
        "\n",
        "**1. Categorical Distribution Representation**\n",
        "For a categorical variable with k categories, the distribution is characterized by:\n",
        "- **Probability vector**: π = (π₁, π₂, ..., πₖ) where Σπᵢ = 1\n",
        "- **Sample proportions**: p̂ = (p̂₁, p̂₂, ..., p̂ₖ) where p̂ᵢ = nᵢ/n\n",
        "\n",
        "**2. Distance Metrics for Categorical Distributions**\n",
        "- **Total Variation Distance**: TV(P,Q) = ½Σᵢ|pᵢ - qᵢ|\n",
        "- **Hellinger Distance**: H(P,Q) = √(½Σᵢ(√pᵢ - √qᵢ)²)\n",
        "- **Jensen-Shannon Divergence**: JS(P,Q) = ½[KL(P||M) + KL(Q||M)] where M = ½(P+Q)\n",
        "\n",
        "---\n",
        "\n",
        "## **1. TEMPORAL STABILITY ANALYSIS**\n",
        "\n",
        "### **Mathematical Background**\n",
        "\n",
        "**Objective**: Assess whether categorical distributions remain consistent across time periods.\n",
        "\n",
        "**Key Statistic: Temporal Consistency Index (TCI)**\n",
        "```\n",
        "TCI(t₁,t₂) = 1 - TV(π̂(t₁), π̂(t₂))\n",
        "```\n",
        "Where π̂(t) is the estimated probability vector at time t.\n",
        "\n",
        "**Why it works**: \n",
        "- Total Variation Distance measures maximum difference in probability assignment\n",
        "- TCI ∈ [0,1]: TCI = 1 indicates perfect stability, TCI = 0 indicates maximum instability\n",
        "- Sensitive to both systematic shifts and random fluctuations\n",
        "\n",
        "### **Implementation & Interpretation**\n",
        "\n",
        "Now let me continue building the comprehensive notebook with all the analysis methods. Let me add the implementation code:\n",
        "\n",
        "Let me continue with a comprehensive explanation of each analysis method. I'll provide the complete mathematical framework and implementation:\n",
        "\n",
        "## **Comprehensive Explanation of Stability & Robustness Analysis**\n",
        "\n",
        "### **1. TEMPORAL STABILITY ANALYSIS**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "The **Temporal Consistency Index (TCI)** measures how consistent categorical distributions remain across different time periods:\n",
        "\n",
        "```\n",
        "TCI(t₁,t₂) = 1 - TV(π̂(t₁), π̂(t₂))\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `TV(P,Q) = ½Σᵢ|pᵢ - qᵢ|` is the **Total Variation Distance**\n",
        "- `π̂(t)` is the estimated probability vector at time t\n",
        "- TCI ∈ [0,1]: 1 = perfect stability, 0 = maximum instability\n",
        "\n",
        "**Why it works:**\n",
        "- Total Variation Distance measures the **maximum difference** in probability assignment between two distributions\n",
        "- It's **sensitive to both systematic shifts** (trend changes) and **random fluctuations**\n",
        "- The complement (1 - TV) gives us a **stability measure** where higher values indicate more consistent distributions\n",
        "\n",
        "**Statistical Test:** Pearson's Chi-Square for temporal homogeneity tests whether distributions are the same across time periods:\n",
        "```\n",
        "χ² = Σᵢⱼ (Oᵢⱼ - Eᵢⱼ)²/Eᵢⱼ\n",
        "```\n",
        "\n",
        "**Interpretation for Mall Customers Dataset:**\n",
        "- Since the dataset lacks temporal information, we simulate time periods\n",
        "- **High TCI (>0.9)**: Gender distribution is very stable across time periods\n",
        "- **Low TCI (<0.7)**: Significant temporal variation in gender proportions\n",
        "- **Chi-square p-value < 0.05**: Reject temporal homogeneity (distributions differ significantly across time)\n",
        "\n",
        "### **2. SAMPLE STABILITY (Bootstrap Analysis)**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "Bootstrap analysis estimates the **sampling distribution** of categorical proportions by resampling with replacement.\n",
        "\n",
        "**Key Metrics:**\n",
        "\n",
        "1. **Bootstrap Confidence Intervals:**\n",
        "   ```\n",
        "   CI_α = [θ̂*(α/2), θ̂*(1-α/2)]\n",
        "   ```\n",
        "   Where θ̂* are bootstrap estimates of proportions\n",
        "\n",
        "2. **Bootstrap Stability Index (BSI):**\n",
        "   ```\n",
        "   BSI = 1 - (IQR_bootstrap / median_bootstrap)\n",
        "   ```\n",
        "   Measures relative variability of bootstrap estimates\n",
        "\n",
        "3. **Bias Estimation:**\n",
        "   ```\n",
        "   Bias = E[θ̂*] - θ̂_original\n",
        "   ```\n",
        "\n",
        "**Why it works:**\n",
        "- Bootstrap **approximates the true sampling distribution** without knowing the population\n",
        "- **Confidence intervals** show the range of plausible values for proportions\n",
        "- **BSI** quantifies how stable estimates are under sampling variation\n",
        "- **Bias estimation** reveals systematic over/under-estimation\n",
        "\n",
        "**Interpretation for Mall Customers Dataset:**\n",
        "- **Narrow CI**: Proportions are precisely estimated (large sample effect)\n",
        "- **High BSI (>0.9)**: Proportions are stable under resampling\n",
        "- **Low bias (<0.01)**: Estimates are unbiased\n",
        "- **Wide CI for minority categories**: Less precision for smaller groups\n",
        "\n",
        "### **3. SUBSET STABILITY ANALYSIS**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "Assesses whether categorical distributions remain consistent when analyzing different subsets of data.\n",
        "\n",
        "**Cross-Validation Stability Index (CVSI):**\n",
        "```\n",
        "CVSI = 1 - (1/k) Σᵢ TV(π̂ᵢ, π̂_full)\n",
        "```\n",
        "Where π̂ᵢ = proportion vector from fold i, π̂_full = full dataset proportions\n",
        "\n",
        "**Subset Homogeneity Test:**\n",
        "```\n",
        "H₀: π₁ = π₂ = ... = πₖ (all subsets have same distribution)\n",
        "χ² = Σᵢⱼ (Oᵢⱼ - Eᵢⱼ)²/Eᵢⱼ\n",
        "```\n",
        "\n",
        "**Why it works:**\n",
        "- **Cross-validation** reveals how much estimates change when different data portions are used\n",
        "- **Subset size analysis** shows the **minimum sample size** needed for stable estimates\n",
        "- **Homogeneity tests** detect whether subsets come from the same population\n",
        "\n",
        "**Interpretation for Mall Customers Dataset:**\n",
        "- **High CVSI (>0.9)**: Gender distribution is consistent across data subsets\n",
        "- **Decreasing stability with smaller subsets**: Natural effect of reduced sample size\n",
        "- **Chi-square significance**: Subsets have significantly different distributions\n",
        "\n",
        "### **4. SENSITIVITY ANALYSIS**\n",
        "\n",
        "#### **A. Impact of Missing Data Imputation**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "Tests how different missing data handling strategies affect distributional estimates.\n",
        "\n",
        "**Missing Data Mechanisms:**\n",
        "1. **MCAR (Missing Completely At Random)**: P(Missing|X,Y) = P(Missing)\n",
        "2. **MAR (Missing At Random)**: P(Missing|X,Y) = P(Missing|X)  \n",
        "3. **MNAR (Missing Not At Random)**: P(Missing|X,Y) depends on Y\n",
        "\n",
        "**Imputation Sensitivity Index (ISI):**\n",
        "```\n",
        "ISI = 1 - max_methods TV(π̂_method1, π̂_method2)\n",
        "```\n",
        "\n",
        "**Why it works:**\n",
        "- Different imputation methods make different assumptions about missing data\n",
        "- **High sensitivity** indicates results depend heavily on imputation choice\n",
        "- **Low sensitivity** suggests robust findings regardless of imputation method\n",
        "\n",
        "#### **B. Robustness to Outliers**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "For categorical data, \"outliers\" are **rare categories** or **unusual patterns**.\n",
        "\n",
        "**Influence Function for Proportions:**\n",
        "```\n",
        "IF(x; π̂) = (I(x=category) - π̂_category) / n\n",
        "```\n",
        "\n",
        "**Breakdown Point:** Minimum fraction of contamination that can make estimates arbitrarily bad.\n",
        "\n",
        "**Why it works:**\n",
        "- **Influence function** measures how much a single observation affects the estimate\n",
        "- **Breakdown point** for proportions is typically 50% (very robust)\n",
        "- **Rare categories** have higher influence per observation\n",
        "\n",
        "#### **C. Effect of Sample Size on Distribution**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "**Asymptotic Properties:**\n",
        "```\n",
        "√n(π̂ - π) → N(0, Σ)\n",
        "```\n",
        "Where Σ = diag(π) - ππᵀ (multinomial covariance)\n",
        "\n",
        "**Sample Size Adequacy Index (SSAI):**\n",
        "```\n",
        "SSAI = min_categories (n × π̂_category) / 5\n",
        "```\n",
        "Rule of thumb: each category should have ≥5 expected observations\n",
        "\n",
        "**Why it works:**\n",
        "- **Central Limit Theorem** ensures normal approximation for large samples\n",
        "- **Multinomial variance** decreases as O(1/n)\n",
        "- **Small categories** require larger samples for stable estimation\n",
        "\n",
        "**Interpretation for Mall Customers Dataset:**\n",
        "- **n=200, 2 categories**: Adequate sample size (each category has >50 observations)\n",
        "- **Balanced distribution**: Both genders well-represented\n",
        "- **High stability expected**: Large sample + balanced distribution = stable estimates\n",
        "\n",
        "---\n",
        "\n",
        "## **PRACTICAL IMPLEMENTATION GUIDE**\n",
        "\n",
        "Perfect! I've created a comprehensive analysis of **Stability & Robustness Analysis** for categorical data. Let me provide you with a complete summary of what each test does, the mathematical background, and how to interpret the results:\n",
        "\n",
        "## **COMPREHENSIVE SUMMARY: Stability & Robustness Analysis**\n",
        "\n",
        "### **🎯 WHAT THIS ANALYSIS CATEGORY DOES**\n",
        "\n",
        "Stability & Robustness Analysis examines how **consistent and reliable** categorical distributions remain under various conditions and perturbations. It answers critical questions:\n",
        "\n",
        "- **Temporal Stability**: Does the distribution change over time?\n",
        "- **Sample Stability**: How much do estimates vary due to sampling?\n",
        "- **Subset Stability**: Are results consistent across data partitions?\n",
        "- **Sensitivity**: How robust are findings to analytical choices?\n",
        "\n",
        "---\n",
        "\n",
        "### **📊 TEST-BY-TEST BREAKDOWN**\n",
        "\n",
        "#### **1. TEMPORAL STABILITY ANALYSIS**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "TCI(t₁,t₂) = 1 - TV(π̂(t₁), π̂(t₂))\n",
        "TV(P,Q) = ½Σᵢ|pᵢ - qᵢ|\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Systematic trends** in categorical proportions over time\n",
        "- **Cyclical patterns** (seasonal effects)\n",
        "- **Structural breaks** (sudden distribution changes)\n",
        "\n",
        "**Why it works:**\n",
        "- Total Variation Distance measures **maximum probability difference**\n",
        "- TCI ∈ [0,1]: Higher values = more temporal consistency\n",
        "- Chi-square test detects **significant temporal heterogeneity**\n",
        "\n",
        "**Interpretation for Mall Customers:**\n",
        "- **TCI > 0.95**: Gender distribution is highly stable across time periods\n",
        "- **TCI < 0.80**: Significant temporal variation (investigate causes)\n",
        "- **Chi-square p < 0.05**: Reject temporal homogeneity hypothesis\n",
        "\n",
        "#### **2. BOOTSTRAP STABILITY ANALYSIS**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "BSI = 1 - (IQR_bootstrap / median_bootstrap)\n",
        "CI_α = [θ̂*(α/2), θ̂*(1-α/2)]\n",
        "Bias = E[θ̂*] - θ̂_original\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Sampling variability** of proportion estimates\n",
        "- **Bias** in estimation procedures\n",
        "- **Precision** of categorical distribution estimates\n",
        "\n",
        "**Why it works:**\n",
        "- Bootstrap **approximates true sampling distribution**\n",
        "- Resampling reveals **natural uncertainty** in estimates\n",
        "- BSI quantifies **relative stability** under sampling variation\n",
        "\n",
        "**Interpretation for Mall Customers:**\n",
        "- **Narrow CI**: Large sample provides precise estimates\n",
        "- **High BSI (>0.9)**: Proportions are stable under resampling\n",
        "- **Low bias (<0.01)**: Unbiased estimation\n",
        "\n",
        "#### **3. SUBSET STABILITY ANALYSIS**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "CVSI = 1 - (1/k) Σᵢ TV(π̂ᵢ, π̂_full)\n",
        "H₀: π₁ = π₂ = ... = πₖ (subset homogeneity)\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Consistency** across different data partitions\n",
        "- **Minimum sample size** requirements\n",
        "- **Data quality** issues (non-representative subsets)\n",
        "\n",
        "**Why it works:**\n",
        "- Cross-validation reveals **partition-dependent variation**\n",
        "- Subset size analysis shows **sample size adequacy**\n",
        "- Homogeneity tests detect **systematic subset differences**\n",
        "\n",
        "**Interpretation for Mall Customers:**\n",
        "- **High CVSI (>0.9)**: Results are consistent across data subsets\n",
        "- **Decreasing stability with smaller subsets**: Natural sample size effect\n",
        "- **Significant chi-square**: Subsets may not be representative\n",
        "\n",
        "#### **4. SENSITIVITY ANALYSIS**\n",
        "\n",
        "##### **A. Missing Data Impact**\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "ISI = 1 - max_methods TV(π̂_method1, π̂_method2)\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Robustness** to missing data handling choices\n",
        "- **MCAR vs MAR vs MNAR** mechanism effects\n",
        "- **Imputation method sensitivity**\n",
        "\n",
        "**Why it works:**\n",
        "- Different imputation methods make **different assumptions**\n",
        "- High sensitivity indicates **method-dependent results**\n",
        "- ISI quantifies **robustness** to analytical choices\n",
        "\n",
        "##### **B. Sample Size Effects**\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "SSAI = min_categories (n × π̂_category) / 5\n",
        "√n(π̂ - π) → N(0, Σ)\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Minimum sample size** for stable estimation\n",
        "- **Asymptotic behavior** of proportion estimates\n",
        "- **Category-specific** sample requirements\n",
        "\n",
        "**Why it works:**\n",
        "- **Central Limit Theorem** ensures normal approximation\n",
        "- Rule of thumb: ≥5 observations per category\n",
        "- Variance decreases as **O(1/n)**\n",
        "\n",
        "##### **C. Outlier Robustness**\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "IF(x; π̂) = (I(x=category) - π̂_category) / n\n",
        "Breakdown Point ≈ 50% for proportions\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Influence** of individual observations\n",
        "- **Rare category** effects\n",
        "- **Data contamination** robustness\n",
        "\n",
        "**Why it works:**\n",
        "- Influence function measures **per-observation impact**\n",
        "- Proportions have **high breakdown point** (very robust)\n",
        "- Rare categories have **higher influence per observation**\n",
        "\n",
        "---\n",
        "\n",
        "### **🔍 INTERPRETATION FRAMEWORK**\n",
        "\n",
        "#### **For Mall Customers Dataset (Gender Analysis):**\n",
        "\n",
        "**Expected Results:**\n",
        "- **High temporal stability** (TCI > 0.95): Gender distribution is inherently stable\n",
        "- **Excellent bootstrap stability** (BSI > 0.95): Large sample (n=200) provides precise estimates\n",
        "- **High subset consistency** (CVSI > 0.90): Balanced distribution ensures consistent subsets\n",
        "- **Low sensitivity** to missing data: Robust binary categorical variable\n",
        "- **Adequate sample size**: Both categories well-represented (>50 observations each)\n",
        "\n",
        "**Key Insights:**\n",
        "1. **Binary variables** are generally more stable than multi-category variables\n",
        "2. **Balanced distributions** show higher stability than skewed ones\n",
        "3. **Large samples** (n>100) typically show excellent bootstrap stability\n",
        "4. **Missing data impact** is minimal for well-represented categories\n",
        "\n",
        "#### **Decision Criteria:**\n",
        "\n",
        "| Metric | Excellent | High | Moderate | Low |\n",
        "|--------|-----------|------|----------|-----|\n",
        "| **TCI/CVSI/BSI** | >0.95 | >0.90 | >0.80 | ≤0.80 |\n",
        "| **CI Width** | <0.05 | <0.10 | <0.15 | ≥0.15 |\n",
        "| **Bias** | <0.001 | <0.01 | <0.05 | ≥0.05 |\n",
        "| **Sample Size** | >200 | >100 | >50 | ≤50 |\n",
        "\n",
        "#### **Practical Recommendations:**\n",
        "\n",
        "**✅ High Stability (Overall Score >0.90):**\n",
        "- Proceed with confidence in analytical results\n",
        "- Current sample size is adequate\n",
        "- Results are robust to analytical choices\n",
        "\n",
        "**⚠️ Moderate Stability (0.80-0.90):**\n",
        "- Consider increasing sample size\n",
        "- Use robust statistical methods\n",
        "- Validate with different subsets\n",
        "\n",
        "**🚨 Low Stability (<0.80):**\n",
        "- Collect more data before final analysis\n",
        "- Investigate sources of instability\n",
        "- Use bootstrap confidence intervals\n",
        "- Consider alternative approaches\n",
        "\n",
        "---\n",
        "\n",
        "### **🎯 PRACTICAL VALUE**\n",
        "\n",
        "This analysis framework provides:\n",
        "\n",
        "1. **Confidence Assessment**: How much can you trust your categorical distribution estimates?\n",
        "2. **Sample Size Planning**: What's the minimum sample needed for stable results?\n",
        "3. **Method Validation**: Are your analytical choices affecting the conclusions?\n",
        "4. **Quality Control**: Are there data quality issues affecting stability?\n",
        "5. **Robustness Testing**: How sensitive are results to various perturbations?\n",
        "\n",
        "The comprehensive implementation I've provided gives you a complete toolkit for assessing the reliability and robustness of categorical data analysis, with specific interpretations tailored to your Mall Customers dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_stability_robustness_analysis(df, cat_col, **kwargs):\n",
        "    \"\"\"\n",
        "    Complete stability and robustness analysis for categorical data\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Input dataset\n",
        "    cat_col : str\n",
        "        Categorical column name\n",
        "    **kwargs : dict\n",
        "        Additional parameters for specific analyses\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Complete analysis results\n",
        "    \"\"\"\n",
        "    \n",
        "    results = {\n",
        "        'dataset_info': {\n",
        "            'n_observations': len(df),\n",
        "            'n_categories': df[cat_col].nunique(),\n",
        "            'categories': df[cat_col].unique().tolist(),\n",
        "            'original_proportions': df[cat_col].value_counts(normalize=True).sort_index()\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # 1. Temporal Stability Analysis\n",
        "    results['temporal_stability'] = temporal_stability_analysis(\n",
        "        df, cat_col, \n",
        "        time_periods=kwargs.get('time_periods', 5),\n",
        "        random_state=kwargs.get('random_state', 42)\n",
        "    )\n",
        "    \n",
        "    # 2. Bootstrap Stability Analysis  \n",
        "    results['bootstrap_stability'] = bootstrap_stability_analysis(\n",
        "        df, cat_col,\n",
        "        n_bootstrap=kwargs.get('n_bootstrap', 1000),\n",
        "        confidence_level=kwargs.get('confidence_level', 0.95),\n",
        "        random_state=kwargs.get('random_state', 42)\n",
        "    )\n",
        "    \n",
        "    # 3. Subset Stability Analysis\n",
        "    results['subset_stability'] = subset_stability_analysis(\n",
        "        df, cat_col,\n",
        "        n_folds=kwargs.get('n_folds', 5),\n",
        "        subset_sizes=kwargs.get('subset_sizes', [0.5, 0.7, 0.8]),\n",
        "        random_state=kwargs.get('random_state', 42)\n",
        "    )\n",
        "    \n",
        "    # 4. Sensitivity Analysis\n",
        "    results['sensitivity_analysis'] = sensitivity_analysis(\n",
        "        df, cat_col,\n",
        "        missing_rates=kwargs.get('missing_rates', [0.05, 0.1, 0.2]),\n",
        "        sample_sizes=kwargs.get('sample_sizes', [50, 100, 150]),\n",
        "        random_state=kwargs.get('random_state', 42)\n",
        "    )\n",
        "    \n",
        "    return results\n",
        "\n",
        "def temporal_stability_analysis(df, cat_col, time_periods=5, random_state=42):\n",
        "    \"\"\"Temporal stability analysis implementation\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Create artificial time periods\n",
        "    df_temp = df.copy()\n",
        "    df_temp['time_period'] = np.random.randint(1, time_periods + 1, len(df_temp))\n",
        "    \n",
        "    # Calculate proportions for each time period\n",
        "    time_proportions = []\n",
        "    period_data = []\n",
        "    \n",
        "    for period in range(1, time_periods + 1):\n",
        "        period_df = df_temp[df_temp['time_period'] == period]\n",
        "        if len(period_df) > 0:\n",
        "            proportions = period_df[cat_col].value_counts(normalize=True).sort_index()\n",
        "            time_proportions.append(proportions.values)\n",
        "            period_data.append(period_df[cat_col].value_counts().sort_index())\n",
        "    \n",
        "    time_proportions = np.array(time_proportions)\n",
        "    \n",
        "    # Calculate Temporal Consistency Index (TCI)\n",
        "    def total_variation_distance(p, q):\n",
        "        return 0.5 * np.sum(np.abs(p - q))\n",
        "    \n",
        "    def temporal_consistency_index(p1, p2):\n",
        "        return 1 - total_variation_distance(p1, p2)\n",
        "    \n",
        "    # Pairwise TCI matrix\n",
        "    n_periods = len(time_proportions)\n",
        "    tci_matrix = np.zeros((n_periods, n_periods))\n",
        "    \n",
        "    for i in range(n_periods):\n",
        "        for j in range(n_periods):\n",
        "            tci_matrix[i, j] = temporal_consistency_index(time_proportions[i], time_proportions[j])\n",
        "    \n",
        "    # Average TCI (excluding diagonal)\n",
        "    mask = ~np.eye(n_periods, dtype=bool)\n",
        "    avg_tci = np.mean(tci_matrix[mask])\n",
        "    \n",
        "    # Chi-Square Test for Temporal Homogeneity\n",
        "    contingency_table = np.array([counts.values for counts in period_data])\n",
        "    chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "    \n",
        "    # Coefficient of Variation for each category\n",
        "    cv_by_category = {}\n",
        "    categories = df[cat_col].unique()\n",
        "    \n",
        "    for i, category in enumerate(categories):\n",
        "        proportions_cat = time_proportions[:, i]\n",
        "        cv = np.std(proportions_cat) / np.mean(proportions_cat) if np.mean(proportions_cat) > 0 else np.inf\n",
        "        cv_by_category[category] = cv\n",
        "    \n",
        "    return {\n",
        "        'tci_matrix': tci_matrix,\n",
        "        'average_tci': avg_tci,\n",
        "        'chi2_homogeneity': {\n",
        "            'statistic': chi2_stat,\n",
        "            'p_value': p_value,\n",
        "            'degrees_of_freedom': dof\n",
        "        },\n",
        "        'coefficient_of_variation': cv_by_category,\n",
        "        'time_proportions': time_proportions,\n",
        "        'categories': categories,\n",
        "        'contingency_table': contingency_table\n",
        "    }\n",
        "\n",
        "def bootstrap_stability_analysis(df, cat_col, n_bootstrap=1000, confidence_level=0.95, random_state=42):\n",
        "    \"\"\"Bootstrap stability analysis implementation\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Original proportions\n",
        "    original_props = df[cat_col].value_counts(normalize=True).sort_index()\n",
        "    categories = original_props.index.tolist()\n",
        "    \n",
        "    # Bootstrap sampling\n",
        "    bootstrap_proportions = []\n",
        "    bootstrap_entropies = []\n",
        "    bootstrap_gini = []\n",
        "    \n",
        "    for i in range(n_bootstrap):\n",
        "        # Resample with replacement\n",
        "        bootstrap_sample = resample(df[cat_col].values, n_samples=len(df), random_state=i)\n",
        "        \n",
        "        # Calculate proportions\n",
        "        bootstrap_props = pd.Series(bootstrap_sample).value_counts(normalize=True)\n",
        "        \n",
        "        # Ensure all categories are represented\n",
        "        props_vector = np.array([bootstrap_props.get(cat, 0) for cat in categories])\n",
        "        bootstrap_proportions.append(props_vector)\n",
        "        \n",
        "        # Calculate entropy: H = -Σ p_i * log(p_i)\n",
        "        entropy = -np.sum([p * np.log(p) if p > 0 else 0 for p in props_vector])\n",
        "        bootstrap_entropies.append(entropy)\n",
        "        \n",
        "        # Calculate Gini coefficient: G = 1 - Σ p_i²\n",
        "        gini = 1 - np.sum(props_vector**2)\n",
        "        bootstrap_gini.append(gini)\n",
        "    \n",
        "    bootstrap_proportions = np.array(bootstrap_proportions)\n",
        "    bootstrap_entropies = np.array(bootstrap_entropies)\n",
        "    bootstrap_gini = np.array(bootstrap_gini)\n",
        "    \n",
        "    # Calculate confidence intervals and stability indices\n",
        "    alpha = 1 - confidence_level\n",
        "    lower_percentile = (alpha/2) * 100\n",
        "    upper_percentile = (1 - alpha/2) * 100\n",
        "    \n",
        "    confidence_intervals = {}\n",
        "    bootstrap_stability_indices = {}\n",
        "    bias_estimates = {}\n",
        "    \n",
        "    for i, category in enumerate(categories):\n",
        "        cat_proportions = bootstrap_proportions[:, i]\n",
        "        \n",
        "        # Confidence intervals\n",
        "        ci_lower = np.percentile(cat_proportions, lower_percentile)\n",
        "        ci_upper = np.percentile(cat_proportions, upper_percentile)\n",
        "        confidence_intervals[category] = (ci_lower, ci_upper)\n",
        "        \n",
        "        # Bootstrap Stability Index (BSI)\n",
        "        median_prop = np.median(cat_proportions)\n",
        "        iqr_prop = np.percentile(cat_proportions, 75) - np.percentile(cat_proportions, 25)\n",
        "        bsi = 1 - (iqr_prop / median_prop) if median_prop > 0 else 0\n",
        "        bootstrap_stability_indices[category] = bsi\n",
        "        \n",
        "        # Bias estimation\n",
        "        bias = np.mean(cat_proportions) - original_props[category]\n",
        "        bias_estimates[category] = bias\n",
        "    \n",
        "    return {\n",
        "        'original_proportions': original_props,\n",
        "        'bootstrap_proportions': bootstrap_proportions,\n",
        "        'confidence_intervals': confidence_intervals,\n",
        "        'bootstrap_stability_indices': bootstrap_stability_indices,\n",
        "        'bias_estimates': bias_estimates,\n",
        "        'entropy_analysis': {\n",
        "            'original': -np.sum([p * np.log(p) if p > 0 else 0 for p in original_props.values]),\n",
        "            'bootstrap_values': bootstrap_entropies,\n",
        "            'confidence_interval': (np.percentile(bootstrap_entropies, lower_percentile), \n",
        "                                  np.percentile(bootstrap_entropies, upper_percentile))\n",
        "        },\n",
        "        'categories': categories\n",
        "    }\n",
        "\n",
        "# Run the comprehensive analysis\n",
        "print(\"=== COMPREHENSIVE STABILITY & ROBUSTNESS ANALYSIS ===\")\n",
        "results = comprehensive_stability_robustness_analysis(df, 'Gender')\n",
        "\n",
        "print(f\"\\nDataset Info:\")\n",
        "print(f\"- Observations: {results['dataset_info']['n_observations']}\")\n",
        "print(f\"- Categories: {results['dataset_info']['n_categories']}\")\n",
        "print(f\"- Original proportions: {dict(results['dataset_info']['original_proportions'])}\")\n",
        "\n",
        "print(f\"\\nTemporal Stability:\")\n",
        "print(f\"- Average TCI: {results['temporal_stability']['average_tci']:.4f}\")\n",
        "print(f\"- Interpretation: {'HIGH' if results['temporal_stability']['average_tci'] > 0.9 else 'MODERATE' if results['temporal_stability']['average_tci'] > 0.7 else 'LOW'} temporal stability\")\n",
        "\n",
        "print(f\"\\nBootstrap Stability:\")\n",
        "for cat in results['bootstrap_stability']['categories']:\n",
        "    bsi = results['bootstrap_stability']['bootstrap_stability_indices'][cat]\n",
        "    ci = results['bootstrap_stability']['confidence_intervals'][cat]\n",
        "    print(f\"- {cat}: BSI = {bsi:.4f}, 95% CI = [{ci[0]:.4f}, {ci[1]:.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def subset_stability_analysis(df, cat_col, n_folds=5, subset_sizes=[0.5, 0.7, 0.8], random_state=42):\n",
        "    \"\"\"Subset stability analysis implementation\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Full dataset proportions\n",
        "    full_props = df[cat_col].value_counts(normalize=True).sort_index()\n",
        "    categories = full_props.index.tolist()\n",
        "    \n",
        "    results = {\n",
        "        'full_proportions': full_props,\n",
        "        'categories': categories,\n",
        "        'cv_analysis': {},\n",
        "        'subset_size_analysis': {}\n",
        "    }\n",
        "    \n",
        "    # Cross-Validation Stability Analysis\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
        "    \n",
        "    fold_proportions = []\n",
        "    fold_tv_distances = []\n",
        "    \n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(df)):\n",
        "        fold_data = df.iloc[train_idx]\n",
        "        fold_props = fold_data[cat_col].value_counts(normalize=True)\n",
        "        \n",
        "        # Ensure all categories are represented\n",
        "        fold_props_vector = np.array([fold_props.get(cat, 0) for cat in categories])\n",
        "        fold_proportions.append(fold_props_vector)\n",
        "        \n",
        "        # Calculate Total Variation Distance from full dataset\n",
        "        tv_distance = 0.5 * np.sum(np.abs(fold_props_vector - full_props.values))\n",
        "        fold_tv_distances.append(tv_distance)\n",
        "    \n",
        "    fold_proportions = np.array(fold_proportions)\n",
        "    \n",
        "    # Cross-Validation Stability Index\n",
        "    cvsi = 1 - np.mean(fold_tv_distances)\n",
        "    \n",
        "    results['cv_analysis'] = {\n",
        "        'cvsi': cvsi,\n",
        "        'fold_proportions': fold_proportions,\n",
        "        'tv_distances': fold_tv_distances\n",
        "    }\n",
        "    \n",
        "    # Subset Size Stability Analysis\n",
        "    for subset_size in subset_sizes:\n",
        "        n_samples = int(len(df) * subset_size)\n",
        "        subset_proportions = []\n",
        "        subset_tv_distances = []\n",
        "        \n",
        "        # Generate multiple random subsets\n",
        "        for i in range(50):  # 50 random subsets for each size\n",
        "            subset_df = df.sample(n=n_samples, random_state=i)\n",
        "            subset_props = subset_df[cat_col].value_counts(normalize=True)\n",
        "            \n",
        "            subset_props_vector = np.array([subset_props.get(cat, 0) for cat in categories])\n",
        "            subset_proportions.append(subset_props_vector)\n",
        "            \n",
        "            tv_distance = 0.5 * np.sum(np.abs(subset_props_vector - full_props.values))\n",
        "            subset_tv_distances.append(tv_distance)\n",
        "        \n",
        "        subset_proportions = np.array(subset_proportions)\n",
        "        \n",
        "        # Subset Stability Index\n",
        "        ssi = 1 - np.mean(subset_tv_distances)\n",
        "        \n",
        "        results['subset_size_analysis'][subset_size] = {\n",
        "            'ssi': ssi,\n",
        "            'subset_proportions': subset_proportions,\n",
        "            'tv_distances': subset_tv_distances,\n",
        "            'mean_tv_distance': np.mean(subset_tv_distances),\n",
        "            'std_tv_distance': np.std(subset_tv_distances)\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "def sensitivity_analysis(df, cat_col, missing_rates=[0.05, 0.1, 0.2], sample_sizes=[50, 100, 150], random_state=42):\n",
        "    \"\"\"Comprehensive sensitivity analysis implementation\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    original_props = df[cat_col].value_counts(normalize=True).sort_index()\n",
        "    categories = original_props.index.tolist()\n",
        "    \n",
        "    results = {\n",
        "        'original_proportions': original_props,\n",
        "        'missing_data_sensitivity': {},\n",
        "        'sample_size_sensitivity': {},\n",
        "        'outlier_sensitivity': {}\n",
        "    }\n",
        "    \n",
        "    # 1. Missing Data Sensitivity\n",
        "    for missing_rate in missing_rates:\n",
        "        n_missing = int(len(df) * missing_rate)\n",
        "        \n",
        "        # Different imputation strategies\n",
        "        imputation_results = {}\n",
        "        \n",
        "        # Create missing data (MCAR)\n",
        "        df_missing = df.copy()\n",
        "        missing_indices = np.random.choice(len(df), n_missing, replace=False)\n",
        "        df_missing.loc[missing_indices, cat_col] = np.nan\n",
        "        \n",
        "        # Strategy 1: Drop missing values\n",
        "        df_drop = df_missing.dropna(subset=[cat_col])\n",
        "        if len(df_drop) > 0:\n",
        "            props_drop = df_drop[cat_col].value_counts(normalize=True).sort_index()\n",
        "            props_vector_drop = np.array([props_drop.get(cat, 0) for cat in categories])\n",
        "            tv_drop = 0.5 * np.sum(np.abs(props_vector_drop - original_props.values))\n",
        "            imputation_results['drop_missing'] = {\n",
        "                'proportions': props_vector_drop,\n",
        "                'tv_distance': tv_drop\n",
        "            }\n",
        "        \n",
        "        # Strategy 2: Mode imputation\n",
        "        mode_value = df[cat_col].mode()[0]\n",
        "        df_mode = df_missing.copy()\n",
        "        df_mode[cat_col].fillna(mode_value, inplace=True)\n",
        "        props_mode = df_mode[cat_col].value_counts(normalize=True).sort_index()\n",
        "        props_vector_mode = np.array([props_mode.get(cat, 0) for cat in categories])\n",
        "        tv_mode = 0.5 * np.sum(np.abs(props_vector_mode - original_props.values))\n",
        "        imputation_results['mode_imputation'] = {\n",
        "            'proportions': props_vector_mode,\n",
        "            'tv_distance': tv_mode\n",
        "        }\n",
        "        \n",
        "        # Strategy 3: Random imputation\n",
        "        df_random = df_missing.copy()\n",
        "        missing_mask = df_random[cat_col].isna()\n",
        "        random_values = np.random.choice(categories, sum(missing_mask), \n",
        "                                       p=original_props.values)\n",
        "        df_random.loc[missing_mask, cat_col] = random_values\n",
        "        props_random = df_random[cat_col].value_counts(normalize=True).sort_index()\n",
        "        props_vector_random = np.array([props_random.get(cat, 0) for cat in categories])\n",
        "        tv_random = 0.5 * np.sum(np.abs(props_vector_random - original_props.values))\n",
        "        imputation_results['random_imputation'] = {\n",
        "            'proportions': props_vector_random,\n",
        "            'tv_distance': tv_random\n",
        "        }\n",
        "        \n",
        "        # Calculate Imputation Sensitivity Index\n",
        "        tv_distances = [result['tv_distance'] for result in imputation_results.values()]\n",
        "        isi = 1 - (max(tv_distances) - min(tv_distances))\n",
        "        \n",
        "        results['missing_data_sensitivity'][missing_rate] = {\n",
        "            'imputation_results': imputation_results,\n",
        "            'imputation_sensitivity_index': isi,\n",
        "            'max_tv_difference': max(tv_distances) - min(tv_distances)\n",
        "        }\n",
        "    \n",
        "    # 2. Sample Size Sensitivity\n",
        "    for sample_size in sample_sizes:\n",
        "        if sample_size >= len(df):\n",
        "            continue\n",
        "            \n",
        "        sample_proportions = []\n",
        "        sample_tv_distances = []\n",
        "        \n",
        "        # Generate multiple samples of given size\n",
        "        for i in range(100):\n",
        "            sample_df = df.sample(n=sample_size, random_state=i)\n",
        "            sample_props = sample_df[cat_col].value_counts(normalize=True)\n",
        "            \n",
        "            sample_props_vector = np.array([sample_props.get(cat, 0) for cat in categories])\n",
        "            sample_proportions.append(sample_props_vector)\n",
        "            \n",
        "            tv_distance = 0.5 * np.sum(np.abs(sample_props_vector - original_props.values))\n",
        "            sample_tv_distances.append(tv_distance)\n",
        "        \n",
        "        sample_proportions = np.array(sample_proportions)\n",
        "        \n",
        "        # Sample Size Adequacy Index\n",
        "        min_expected = min([sample_size * original_props[cat] for cat in categories])\n",
        "        ssai = min_expected / 5  # Rule of thumb: ≥5 expected observations per category\n",
        "        \n",
        "        results['sample_size_sensitivity'][sample_size] = {\n",
        "            'sample_proportions': sample_proportions,\n",
        "            'tv_distances': sample_tv_distances,\n",
        "            'mean_tv_distance': np.mean(sample_tv_distances),\n",
        "            'std_tv_distance': np.std(sample_tv_distances),\n",
        "            'sample_size_adequacy_index': ssai,\n",
        "            'cv_by_category': {\n",
        "                cat: np.std(sample_proportions[:, i]) / np.mean(sample_proportions[:, i])\n",
        "                for i, cat in enumerate(categories)\n",
        "                if np.mean(sample_proportions[:, i]) > 0\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Complete the analysis with remaining functions\n",
        "def subset_stability_analysis_complete(df, cat_col, n_folds=5, subset_sizes=[0.5, 0.7, 0.8], random_state=42):\n",
        "    \"\"\"Complete subset stability analysis with additional metrics\"\"\"\n",
        "    base_results = subset_stability_analysis(df, cat_col, n_folds, subset_sizes, random_state)\n",
        "    \n",
        "    # Add chi-square test for fold homogeneity\n",
        "    fold_counts = []\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
        "    \n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(df)):\n",
        "        fold_data = df.iloc[train_idx]\n",
        "        fold_count = fold_data[cat_col].value_counts().sort_index()\n",
        "        fold_counts.append([fold_count.get(cat, 0) for cat in base_results['categories']])\n",
        "    \n",
        "    fold_contingency = np.array(fold_counts)\n",
        "    chi2_folds, p_val_folds, dof_folds, expected_folds = chi2_contingency(fold_contingency)\n",
        "    \n",
        "    base_results['cv_analysis']['chi2_homogeneity'] = {\n",
        "        'statistic': chi2_folds,\n",
        "        'p_value': p_val_folds,\n",
        "        'degrees_of_freedom': dof_folds\n",
        "    }\n",
        "    \n",
        "    return base_results\n",
        "\n",
        "# Update the main analysis function\n",
        "print(\"\\n=== RUNNING COMPLETE ANALYSIS ===\")\n",
        "\n",
        "# Re-run with complete functions\n",
        "results_complete = {\n",
        "    'dataset_info': {\n",
        "        'n_observations': len(df),\n",
        "        'n_categories': df['Gender'].nunique(),\n",
        "        'categories': df['Gender'].unique().tolist(),\n",
        "        'original_proportions': df['Gender'].value_counts(normalize=True).sort_index()\n",
        "    },\n",
        "    'temporal_stability': temporal_stability_analysis(df, 'Gender'),\n",
        "    'bootstrap_stability': bootstrap_stability_analysis(df, 'Gender'),\n",
        "    'subset_stability': subset_stability_analysis_complete(df, 'Gender'),\n",
        "    'sensitivity_analysis': sensitivity_analysis(df, 'Gender')\n",
        "}\n",
        "\n",
        "print(\"Analysis complete! Results stored in 'results_complete' dictionary.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## **COMPREHENSIVE RESULTS INTERPRETATION**\n",
        "\n",
        "def interpret_stability_results(results):\n",
        "    \"\"\"\n",
        "    Provide comprehensive interpretation of stability and robustness analysis results\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPREHENSIVE STABILITY & ROBUSTNESS ANALYSIS INTERPRETATION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Dataset Overview\n",
        "    print(f\"\\n📊 DATASET OVERVIEW:\")\n",
        "    print(f\"   • Sample Size: {results['dataset_info']['n_observations']}\")\n",
        "    print(f\"   • Categories: {results['dataset_info']['n_categories']} ({', '.join(results['dataset_info']['categories'])})\")\n",
        "    print(f\"   • Original Distribution: {dict(results['dataset_info']['original_proportions'])}\")\n",
        "    \n",
        "    # 1. Temporal Stability Interpretation\n",
        "    print(f\"\\n⏰ TEMPORAL STABILITY ANALYSIS:\")\n",
        "    tci = results['temporal_stability']['average_tci']\n",
        "    chi2_p = results['temporal_stability']['chi2_homogeneity']['p_value']\n",
        "    \n",
        "    print(f\"   • Temporal Consistency Index (TCI): {tci:.4f}\")\n",
        "    if tci > 0.95:\n",
        "        print(f\"     → EXCELLENT temporal stability - distributions are highly consistent across time\")\n",
        "    elif tci > 0.90:\n",
        "        print(f\"     → HIGH temporal stability - minor variations across time periods\")\n",
        "    elif tci > 0.80:\n",
        "        print(f\"     → MODERATE temporal stability - some temporal variation present\")\n",
        "    else:\n",
        "        print(f\"     → LOW temporal stability - significant temporal variation detected\")\n",
        "    \n",
        "    print(f\"   • Chi-Square Homogeneity Test: p = {chi2_p:.4f}\")\n",
        "    if chi2_p < 0.001:\n",
        "        print(f\"     → STRONG evidence against temporal homogeneity (p < 0.001)\")\n",
        "    elif chi2_p < 0.01:\n",
        "        print(f\"     → MODERATE evidence against temporal homogeneity (p < 0.01)\")\n",
        "    elif chi2_p < 0.05:\n",
        "        print(f\"     → WEAK evidence against temporal homogeneity (p < 0.05)\")\n",
        "    else:\n",
        "        print(f\"     → NO significant evidence against temporal homogeneity (p ≥ 0.05)\")\n",
        "    \n",
        "    # 2. Bootstrap Stability Interpretation\n",
        "    print(f\"\\n🔄 BOOTSTRAP STABILITY ANALYSIS:\")\n",
        "    for cat in results['bootstrap_stability']['categories']:\n",
        "        bsi = results['bootstrap_stability']['bootstrap_stability_indices'][cat]\n",
        "        ci = results['bootstrap_stability']['confidence_intervals'][cat]\n",
        "        bias = results['bootstrap_stability']['bias_estimates'][cat]\n",
        "        \n",
        "        print(f\"   • {cat}:\")\n",
        "        print(f\"     - Bootstrap Stability Index: {bsi:.4f}\")\n",
        "        if bsi > 0.95:\n",
        "            print(f\"       → EXCELLENT sampling stability\")\n",
        "        elif bsi > 0.90:\n",
        "            print(f\"       → HIGH sampling stability\")\n",
        "        elif bsi > 0.80:\n",
        "            print(f\"       → MODERATE sampling stability\")\n",
        "        else:\n",
        "            print(f\"       → LOW sampling stability\")\n",
        "            \n",
        "        print(f\"     - 95% Confidence Interval: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
        "        ci_width = ci[1] - ci[0]\n",
        "        if ci_width < 0.05:\n",
        "            print(f\"       → NARROW CI (width: {ci_width:.4f}) - precise estimation\")\n",
        "        elif ci_width < 0.10:\n",
        "            print(f\"       → MODERATE CI (width: {ci_width:.4f}) - reasonable precision\")\n",
        "        else:\n",
        "            print(f\"       → WIDE CI (width: {ci_width:.4f}) - less precise estimation\")\n",
        "            \n",
        "        print(f\"     - Bias Estimate: {bias:.6f}\")\n",
        "        if abs(bias) < 0.001:\n",
        "            print(f\"       → NEGLIGIBLE bias - unbiased estimation\")\n",
        "        elif abs(bias) < 0.01:\n",
        "            print(f\"       → SMALL bias - minimal systematic error\")\n",
        "        else:\n",
        "            print(f\"       → NOTABLE bias - systematic {'over' if bias > 0 else 'under'}-estimation\")\n",
        "    \n",
        "    # 3. Subset Stability Interpretation\n",
        "    print(f\"\\n📂 SUBSET STABILITY ANALYSIS:\")\n",
        "    cvsi = results['subset_stability']['cv_analysis']['cvsi']\n",
        "    chi2_p_folds = results['subset_stability']['cv_analysis']['chi2_homogeneity']['p_value']\n",
        "    \n",
        "    print(f\"   • Cross-Validation Stability Index: {cvsi:.4f}\")\n",
        "    if cvsi > 0.95:\n",
        "        print(f\"     → EXCELLENT subset consistency - very stable across data partitions\")\n",
        "    elif cvsi > 0.90:\n",
        "        print(f\"     → HIGH subset consistency - stable across most partitions\")\n",
        "    elif cvsi > 0.80:\n",
        "        print(f\"     → MODERATE subset consistency - some variation across partitions\")\n",
        "    else:\n",
        "        print(f\"     → LOW subset consistency - significant variation across partitions\")\n",
        "    \n",
        "    print(f\"   • Fold Homogeneity Test: p = {chi2_p_folds:.4f}\")\n",
        "    print(f\"     → {'Significant' if chi2_p_folds < 0.05 else 'Non-significant'} differences between CV folds\")\n",
        "    \n",
        "    print(f\"   • Subset Size Analysis:\")\n",
        "    for size in [0.5, 0.7, 0.8]:\n",
        "        if size in results['subset_stability']['subset_size_analysis']:\n",
        "            ssi = results['subset_stability']['subset_size_analysis'][size]['ssi']\n",
        "            mean_tv = results['subset_stability']['subset_size_analysis'][size]['mean_tv_distance']\n",
        "            print(f\"     - {int(size*100)}% subset: SSI = {ssi:.4f}, Mean TV = {mean_tv:.4f}\")\n",
        "    \n",
        "    # 4. Sensitivity Analysis Interpretation\n",
        "    print(f\"\\n🔍 SENSITIVITY ANALYSIS:\")\n",
        "    \n",
        "    # Missing Data Sensitivity\n",
        "    print(f\"   • Missing Data Impact:\")\n",
        "    for rate in [0.05, 0.1, 0.2]:\n",
        "        if rate in results['sensitivity_analysis']['missing_data_sensitivity']:\n",
        "            isi = results['sensitivity_analysis']['missing_data_sensitivity'][rate]['imputation_sensitivity_index']\n",
        "            max_diff = results['sensitivity_analysis']['missing_data_sensitivity'][rate]['max_tv_difference']\n",
        "            print(f\"     - {int(rate*100)}% missing: ISI = {isi:.4f}, Max difference = {max_diff:.4f}\")\n",
        "            if isi > 0.95:\n",
        "                print(f\"       → ROBUST to imputation method choice\")\n",
        "            elif isi > 0.90:\n",
        "                print(f\"       → MODERATELY robust to imputation method\")\n",
        "            else:\n",
        "                print(f\"       → SENSITIVE to imputation method choice\")\n",
        "    \n",
        "    # Sample Size Sensitivity  \n",
        "    print(f\"   • Sample Size Impact:\")\n",
        "    for size in [50, 100, 150]:\n",
        "        if size in results['sensitivity_analysis']['sample_size_sensitivity']:\n",
        "            mean_tv = results['sensitivity_analysis']['sample_size_sensitivity'][size]['mean_tv_distance']\n",
        "            ssai = results['sensitivity_analysis']['sample_size_sensitivity'][size]['sample_size_adequacy_index']\n",
        "            print(f\"     - n={size}: Mean TV = {mean_tv:.4f}, SSAI = {ssai:.2f}\")\n",
        "            if ssai >= 1.0:\n",
        "                print(f\"       → ADEQUATE sample size for stable estimation\")\n",
        "            elif ssai >= 0.5:\n",
        "                print(f\"       → MARGINAL sample size - some instability expected\")\n",
        "            else:\n",
        "                print(f\"       → INADEQUATE sample size - high instability expected\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(\"OVERALL ASSESSMENT:\")\n",
        "    \n",
        "    # Overall stability score\n",
        "    stability_scores = [tci, cvsi]\n",
        "    if 'bootstrap_stability' in results:\n",
        "        avg_bsi = np.mean([results['bootstrap_stability']['bootstrap_stability_indices'][cat] \n",
        "                          for cat in results['bootstrap_stability']['categories']])\n",
        "        stability_scores.append(avg_bsi)\n",
        "    \n",
        "    overall_stability = np.mean(stability_scores)\n",
        "    \n",
        "    print(f\"   • Overall Stability Score: {overall_stability:.4f}\")\n",
        "    if overall_stability > 0.95:\n",
        "        print(f\"     → EXCELLENT overall stability - highly reliable categorical distribution\")\n",
        "    elif overall_stability > 0.90:\n",
        "        print(f\"     → HIGH overall stability - reliable for most analytical purposes\")\n",
        "    elif overall_stability > 0.80:\n",
        "        print(f\"     → MODERATE overall stability - adequate with some caution\")\n",
        "    else:\n",
        "        print(f\"     → LOW overall stability - results should be interpreted carefully\")\n",
        "    \n",
        "    # Recommendations\n",
        "    print(f\"\\n📋 RECOMMENDATIONS:\")\n",
        "    if overall_stability > 0.90:\n",
        "        print(f\"   ✅ Distribution is highly stable - proceed with confidence\")\n",
        "        print(f\"   ✅ Current sample size is adequate for reliable inference\")\n",
        "        print(f\"   ✅ Results are robust to analytical choices\")\n",
        "    elif overall_stability > 0.80:\n",
        "        print(f\"   ⚠️  Distribution shows moderate stability - consider:\")\n",
        "        print(f\"      • Increasing sample size if possible\")\n",
        "        print(f\"      • Using robust statistical methods\")\n",
        "        print(f\"      • Validating results with different subsets\")\n",
        "    else:\n",
        "        print(f\"   🚨 Distribution shows low stability - recommend:\")\n",
        "        print(f\"      • Collecting more data before final analysis\")\n",
        "        print(f\"      • Investigating sources of instability\")\n",
        "        print(f\"      • Using bootstrap confidence intervals\")\n",
        "        print(f\"      • Considering alternative analytical approaches\")\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# Run the interpretation\n",
        "interpret_stability_results(results_complete)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comprehensive Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## **COMPREHENSIVE VISUALIZATIONS**\n",
        "\n",
        "def create_comprehensive_visualizations(results):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualizations for all stability and robustness analyses\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set up the figure with subplots\n",
        "    fig = plt.figure(figsize=(20, 24))\n",
        "    gs = fig.add_gridspec(6, 4, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # Color palette\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, 12))\n",
        "    \n",
        "    # 1. Dataset Overview (Top row)\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    original_props = results['dataset_info']['original_proportions']\n",
        "    categories = results['dataset_info']['categories']\n",
        "    \n",
        "    bars = ax1.bar(categories, original_props.values, color=colors[:len(categories)], alpha=0.8)\n",
        "    ax1.set_title('Original Distribution', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Proportion')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, original_props.values):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 2. Temporal Consistency Index Matrix\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    tci_matrix = results['temporal_stability']['tci_matrix']\n",
        "    im = ax2.imshow(tci_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
        "    ax2.set_title('Temporal Consistency Matrix', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Time Period')\n",
        "    ax2.set_ylabel('Time Period')\n",
        "    \n",
        "    # Add text annotations\n",
        "    for i in range(tci_matrix.shape[0]):\n",
        "        for j in range(tci_matrix.shape[1]):\n",
        "            ax2.text(j, i, f'{tci_matrix[i, j]:.3f}', ha='center', va='center',\n",
        "                    color='white' if tci_matrix[i, j] < 0.5 else 'black', fontweight='bold')\n",
        "    \n",
        "    plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
        "    \n",
        "    # 3. Bootstrap Distributions\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    bootstrap_props = results['bootstrap_stability']['bootstrap_proportions']\n",
        "    \n",
        "    for i, cat in enumerate(categories):\n",
        "        ax3.hist(bootstrap_props[:, i], bins=30, alpha=0.6, label=cat, \n",
        "                color=colors[i], density=True)\n",
        "        # Add original proportion line\n",
        "        ax3.axvline(original_props[cat], color=colors[i], linestyle='--', \n",
        "                   linewidth=2, alpha=0.8)\n",
        "    \n",
        "    ax3.set_title('Bootstrap Distributions', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('Proportion')\n",
        "    ax3.set_ylabel('Density')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Stability Indices Comparison\n",
        "    ax4 = fig.add_subplot(gs[0, 3])\n",
        "    \n",
        "    # Collect all stability indices\n",
        "    tci = results['temporal_stability']['average_tci']\n",
        "    cvsi = results['subset_stability']['cv_analysis']['cvsi']\n",
        "    avg_bsi = np.mean([results['bootstrap_stability']['bootstrap_stability_indices'][cat] \n",
        "                      for cat in categories])\n",
        "    \n",
        "    stability_metrics = ['TCI', 'CVSI', 'Avg BSI']\n",
        "    stability_values = [tci, cvsi, avg_bsi]\n",
        "    \n",
        "    bars = ax4.bar(stability_metrics, stability_values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "    ax4.set_title('Stability Indices Comparison', fontsize=14, fontweight='bold')\n",
        "    ax4.set_ylabel('Stability Index')\n",
        "    ax4.set_ylim(0, 1)\n",
        "    ax4.axhline(y=0.9, color='green', linestyle='--', alpha=0.7, label='High stability')\n",
        "    ax4.axhline(y=0.8, color='orange', linestyle='--', alpha=0.7, label='Moderate stability')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, stability_values):\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 5. Temporal Evolution (Second row)\n",
        "    ax5 = fig.add_subplot(gs[1, :2])\n",
        "    time_proportions = results['temporal_stability']['time_proportions']\n",
        "    time_points = range(1, len(time_proportions) + 1)\n",
        "    \n",
        "    for i, cat in enumerate(categories):\n",
        "        ax5.plot(time_points, time_proportions[:, i], marker='o', linewidth=3,\n",
        "                markersize=8, label=cat, color=colors[i])\n",
        "        # Add confidence bands (simulated)\n",
        "        std_dev = np.std(time_proportions[:, i])\n",
        "        ax5.fill_between(time_points, \n",
        "                        time_proportions[:, i] - std_dev,\n",
        "                        time_proportions[:, i] + std_dev,\n",
        "                        alpha=0.2, color=colors[i])\n",
        "    \n",
        "    ax5.set_title('Temporal Evolution of Proportions', fontsize=14, fontweight='bold')\n",
        "    ax5.set_xlabel('Time Period')\n",
        "    ax5.set_ylabel('Proportion')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Cross-Validation Fold Analysis\n",
        "    ax6 = fig.add_subplot(gs[1, 2:])\n",
        "    fold_props = results['subset_stability']['cv_analysis']['fold_proportions']\n",
        "    tv_distances = results['subset_stability']['cv_analysis']['tv_distances']\n",
        "    \n",
        "    x_pos = np.arange(len(categories))\n",
        "    width = 0.15\n",
        "    \n",
        "    for i in range(len(fold_props)):\n",
        "        ax6.bar(x_pos + i*width, fold_props[i], width, alpha=0.7, \n",
        "               label=f'Fold {i+1}', color=colors[i])\n",
        "    \n",
        "    # Add full dataset proportions\n",
        "    ax6.bar(x_pos + len(fold_props)*width, original_props.values, width, \n",
        "           color='red', alpha=0.8, label='Full Dataset')\n",
        "    \n",
        "    ax6.set_title('Cross-Validation Fold Proportions', fontsize=14, fontweight='bold')\n",
        "    ax6.set_xlabel('Category')\n",
        "    ax6.set_ylabel('Proportion')\n",
        "    ax6.set_xticks(x_pos + width * len(fold_props)/2)\n",
        "    ax6.set_xticklabels(categories)\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 7. Subset Size Analysis (Third row)\n",
        "    ax7 = fig.add_subplot(gs[2, :2])\n",
        "    subset_sizes = [0.5, 0.7, 0.8]\n",
        "    ssi_values = []\n",
        "    mean_tv_values = []\n",
        "    \n",
        "    for size in subset_sizes:\n",
        "        if size in results['subset_stability']['subset_size_analysis']:\n",
        "            ssi_values.append(results['subset_stability']['subset_size_analysis'][size]['ssi'])\n",
        "            mean_tv_values.append(results['subset_stability']['subset_size_analysis'][size]['mean_tv_distance'])\n",
        "    \n",
        "    ax7_twin = ax7.twinx()\n",
        "    \n",
        "    line1 = ax7.plot(subset_sizes, ssi_values, 'bo-', linewidth=3, markersize=10, \n",
        "                     label='Subset Stability Index')\n",
        "    line2 = ax7_twin.plot(subset_sizes, mean_tv_values, 'ro-', linewidth=3, markersize=10,\n",
        "                         label='Mean TV Distance')\n",
        "    \n",
        "    ax7.set_xlabel('Subset Size')\n",
        "    ax7.set_ylabel('Subset Stability Index', color='blue')\n",
        "    ax7_twin.set_ylabel('Mean TV Distance', color='red')\n",
        "    ax7.set_title('Stability vs Subset Size', fontsize=14, fontweight='bold')\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Combine legends\n",
        "    lines = line1 + line2\n",
        "    labels = [l.get_label() for l in lines]\n",
        "    ax7.legend(lines, labels, loc='center right')\n",
        "    \n",
        "    # 8. Missing Data Sensitivity\n",
        "    ax8 = fig.add_subplot(gs[2, 2:])\n",
        "    missing_rates = [0.05, 0.1, 0.2]\n",
        "    \n",
        "    imputation_methods = ['drop_missing', 'mode_imputation', 'random_imputation']\n",
        "    method_colors = ['red', 'blue', 'green']\n",
        "    \n",
        "    x_pos = np.arange(len(missing_rates))\n",
        "    width = 0.25\n",
        "    \n",
        "    for i, method in enumerate(imputation_methods):\n",
        "        tv_values = []\n",
        "        for rate in missing_rates:\n",
        "            if rate in results['sensitivity_analysis']['missing_data_sensitivity']:\n",
        "                imputation_results = results['sensitivity_analysis']['missing_data_sensitivity'][rate]['imputation_results']\n",
        "                if method in imputation_results:\n",
        "                    tv_values.append(imputation_results[method]['tv_distance'])\n",
        "                else:\n",
        "                    tv_values.append(0)\n",
        "            else:\n",
        "                tv_values.append(0)\n",
        "        \n",
        "        ax8.bar(x_pos + i*width, tv_values, width, alpha=0.7, \n",
        "               label=method.replace('_', ' ').title(), color=method_colors[i])\n",
        "    \n",
        "    ax8.set_title('Missing Data Sensitivity', fontsize=14, fontweight='bold')\n",
        "    ax8.set_xlabel('Missing Rate')\n",
        "    ax8.set_ylabel('TV Distance from Original')\n",
        "    ax8.set_xticks(x_pos + width)\n",
        "    ax8.set_xticklabels([f'{int(rate*100)}%' for rate in missing_rates])\n",
        "    ax8.legend()\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 9. Sample Size Sensitivity (Fourth row)\n",
        "    ax9 = fig.add_subplot(gs[3, :2])\n",
        "    sample_sizes = [50, 100, 150]\n",
        "    \n",
        "    for size in sample_sizes:\n",
        "        if size in results['sensitivity_analysis']['sample_size_sensitivity']:\n",
        "            tv_distances = results['sensitivity_analysis']['sample_size_sensitivity'][size]['tv_distances']\n",
        "            ax9.hist(tv_distances, bins=20, alpha=0.6, label=f'n={size}', density=True)\n",
        "    \n",
        "    ax9.set_title('Sample Size Sensitivity - TV Distance Distributions', fontsize=14, fontweight='bold')\n",
        "    ax9.set_xlabel('TV Distance from Full Dataset')\n",
        "    ax9.set_ylabel('Density')\n",
        "    ax9.legend()\n",
        "    ax9.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 10. Bootstrap Confidence Intervals\n",
        "    ax10 = fig.add_subplot(gs[3, 2:])\n",
        "    \n",
        "    for i, cat in enumerate(categories):\n",
        "        ci = results['bootstrap_stability']['confidence_intervals'][cat]\n",
        "        original_prop = original_props[cat]\n",
        "        \n",
        "        # Plot confidence interval as error bar\n",
        "        ax10.errorbar(i, original_prop, yerr=[[original_prop - ci[0]], [ci[1] - original_prop]], \n",
        "                     fmt='o', markersize=10, capsize=10, capthick=3, \n",
        "                     color=colors[i], label=cat)\n",
        "        \n",
        "        # Add CI width annotation\n",
        "        ci_width = ci[1] - ci[0]\n",
        "        ax10.text(i, ci[1] + 0.02, f'Width: {ci_width:.3f}', \n",
        "                 ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    ax10.set_title('Bootstrap 95% Confidence Intervals', fontsize=14, fontweight='bold')\n",
        "    ax10.set_xlabel('Category')\n",
        "    ax10.set_ylabel('Proportion')\n",
        "    ax10.set_xticks(range(len(categories)))\n",
        "    ax10.set_xticklabels(categories)\n",
        "    ax10.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 11. Comprehensive Stability Heatmap (Fifth row)\n",
        "    ax11 = fig.add_subplot(gs[4, :2])\n",
        "    \n",
        "    # Create stability matrix\n",
        "    stability_data = []\n",
        "    row_labels = []\n",
        "    \n",
        "    # Temporal stability by category\n",
        "    for cat in categories:\n",
        "        cv = results['temporal_stability']['coefficient_of_variation'][cat]\n",
        "        stability_data.append([1 - cv if cv != np.inf else 0])  # Convert CV to stability\n",
        "        row_labels.append(f'Temporal-{cat}')\n",
        "    \n",
        "    # Bootstrap stability by category\n",
        "    for cat in categories:\n",
        "        bsi = results['bootstrap_stability']['bootstrap_stability_indices'][cat]\n",
        "        stability_data.append([bsi])\n",
        "        row_labels.append(f'Bootstrap-{cat}')\n",
        "    \n",
        "    # Subset stability\n",
        "    for size in subset_sizes:\n",
        "        if size in results['subset_stability']['subset_size_analysis']:\n",
        "            ssi = results['subset_stability']['subset_size_analysis'][size]['ssi']\n",
        "            stability_data.append([ssi])\n",
        "            row_labels.append(f'Subset-{int(size*100)}%')\n",
        "    \n",
        "    stability_matrix = np.array(stability_data)\n",
        "    \n",
        "    im = ax11.imshow(stability_matrix, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
        "    ax11.set_title('Comprehensive Stability Heatmap', fontsize=14, fontweight='bold')\n",
        "    ax11.set_yticks(range(len(row_labels)))\n",
        "    ax11.set_yticklabels(row_labels)\n",
        "    ax11.set_xticks([0])\n",
        "    ax11.set_xticklabels(['Stability Index'])\n",
        "    \n",
        "    # Add text annotations\n",
        "    for i in range(len(row_labels)):\n",
        "        ax11.text(0, i, f'{stability_matrix[i, 0]:.3f}', ha='center', va='center',\n",
        "                 color='white' if stability_matrix[i, 0] < 0.5 else 'black', fontweight='bold')\n",
        "    \n",
        "    plt.colorbar(im, ax=ax11, fraction=0.046, pad=0.04)\n",
        "    \n",
        "    # 12. Overall Assessment Radar Chart\n",
        "    ax12 = fig.add_subplot(gs[4, 2:], projection='polar')\n",
        "    \n",
        "    # Metrics for radar chart\n",
        "    metrics = ['Temporal\\nStability', 'Bootstrap\\nStability', 'Subset\\nStability', \n",
        "              'Missing Data\\nRobustness', 'Sample Size\\nAdequacy']\n",
        "    \n",
        "    values = [\n",
        "        tci,\n",
        "        avg_bsi,\n",
        "        cvsi,\n",
        "        np.mean([results['sensitivity_analysis']['missing_data_sensitivity'][rate]['imputation_sensitivity_index'] \n",
        "                for rate in [0.05, 0.1, 0.2] \n",
        "                if rate in results['sensitivity_analysis']['missing_data_sensitivity']]),\n",
        "        np.mean([results['sensitivity_analysis']['sample_size_sensitivity'][size]['sample_size_adequacy_index'] \n",
        "                for size in [50, 100, 150] \n",
        "                if size in results['sensitivity_analysis']['sample_size_sensitivity'] \n",
        "                and results['sensitivity_analysis']['sample_size_sensitivity'][size]['sample_size_adequacy_index'] <= 2])\n",
        "    ]\n",
        "    \n",
        "    # Normalize the last value to [0,1] range\n",
        "    values[-1] = min(values[-1], 1.0)\n",
        "    \n",
        "    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "    values += values[:1]  # Complete the circle\n",
        "    angles += angles[:1]\n",
        "    \n",
        "    ax12.plot(angles, values, 'o-', linewidth=3, markersize=8, color='blue', alpha=0.7)\n",
        "    ax12.fill(angles, values, alpha=0.25, color='blue')\n",
        "    ax12.set_xticks(angles[:-1])\n",
        "    ax12.set_xticklabels(metrics)\n",
        "    ax12.set_ylim(0, 1)\n",
        "    ax12.set_title('Overall Stability Assessment', fontsize=14, fontweight='bold', pad=20)\n",
        "    ax12.grid(True)\n",
        "    \n",
        "    # Add reference circles\n",
        "    ax12.axhline(y=0.9, color='green', linestyle='--', alpha=0.7)\n",
        "    ax12.axhline(y=0.8, color='orange', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # 13. Summary Statistics Table (Bottom row)\n",
        "    ax13 = fig.add_subplot(gs[5, :])\n",
        "    ax13.axis('off')\n",
        "    \n",
        "    # Create summary table\n",
        "    summary_data = [\n",
        "        ['Metric', 'Value', 'Interpretation'],\n",
        "        ['Overall Stability Score', f'{np.mean([tci, cvsi, avg_bsi]):.4f}', \n",
        "         'EXCELLENT' if np.mean([tci, cvsi, avg_bsi]) > 0.95 else \n",
        "         'HIGH' if np.mean([tci, cvsi, avg_bsi]) > 0.90 else \n",
        "         'MODERATE' if np.mean([tci, cvsi, avg_bsi]) > 0.80 else 'LOW'],\n",
        "        ['Temporal Consistency', f'{tci:.4f}', \n",
        "         'EXCELLENT' if tci > 0.95 else 'HIGH' if tci > 0.90 else 'MODERATE' if tci > 0.80 else 'LOW'],\n",
        "        ['Bootstrap Stability', f'{avg_bsi:.4f}', \n",
        "         'EXCELLENT' if avg_bsi > 0.95 else 'HIGH' if avg_bsi > 0.90 else 'MODERATE' if avg_bsi > 0.80 else 'LOW'],\n",
        "        ['Subset Consistency', f'{cvsi:.4f}', \n",
        "         'EXCELLENT' if cvsi > 0.95 else 'HIGH' if cvsi > 0.90 else 'MODERATE' if cvsi > 0.80 else 'LOW'],\n",
        "        ['Sample Size', f'{len(results[\"dataset_info\"][\"original_proportions\"])} obs, {results[\"dataset_info\"][\"n_categories\"]} categories', \n",
        "         'ADEQUATE' if len(results[\"dataset_info\"][\"original_proportions\"]) > 100 else 'MARGINAL'],\n",
        "    ]\n",
        "    \n",
        "    table = ax13.table(cellText=summary_data[1:], colLabels=summary_data[0],\n",
        "                      cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(12)\n",
        "    table.scale(1, 2)\n",
        "    \n",
        "    # Color code the interpretation column\n",
        "    for i in range(1, len(summary_data)):\n",
        "        interpretation = summary_data[i][2]\n",
        "        if 'EXCELLENT' in interpretation:\n",
        "            table[(i, 2)].set_facecolor('#90EE90')  # Light green\n",
        "        elif 'HIGH' in interpretation:\n",
        "            table[(i, 2)].set_facecolor('#87CEEB')  # Sky blue\n",
        "        elif 'MODERATE' in interpretation:\n",
        "            table[(i, 2)].set_facecolor('#FFE4B5')  # Moccasin\n",
        "        elif 'LOW' in interpretation:\n",
        "            table[(i, 2)].set_facecolor('#FFB6C1')  # Light pink\n",
        "        else:\n",
        "            table[(i, 2)].set_facecolor('#F0F0F0')  # Light gray\n",
        "    \n",
        "    plt.suptitle('Comprehensive Stability & Robustness Analysis Dashboard', \n",
        "                fontsize=20, fontweight='bold', y=0.98)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create the comprehensive visualizations\n",
        "create_comprehensive_visualizations(results_complete)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Dive on Bootstrap Analysis\n",
        "\n",
        "**Why it works:**\n",
        "- Bootstrap **approximates the true sampling distribution** without knowing the population\n",
        "- **Confidence intervals** show the range of plausible values for proportions\n",
        "- **BSI** quantifies how stable estimates are under sampling variation\n",
        "- **Bias estimation** reveals systematic over/under-estimation\n",
        "\n",
        "**Interpretation for Mall Customers Dataset:**\n",
        "- **Narrow CI**: Proportions are precisely estimated (large sample effect)\n",
        "- **High BSI (>0.9)**: Proportions are stable under resampling\n",
        "- **Low bias (<0.01)**: Estimates are unbiased\n",
        "- **Wide CI for minority categories**: Less precision for smaller groups\n",
        "\n",
        "----\n",
        "\n",
        "I'll explain the deep intuition behind Bootstrap analysis and why each formula works to provide these specific interpretations. Let me break this down systematically.\n",
        "\n",
        "## **The Core Intuition Behind Bootstrap Analysis**\n",
        "\n",
        "### **🎯 The Fundamental Problem Bootstrap Solves**\n",
        "\n",
        "**The Challenge:**\n",
        "- We have a **single sample** from an unknown population\n",
        "- We want to know: \"How reliable are our categorical proportion estimates?\"\n",
        "- **Traditional approach**: Assume normal distribution (often invalid for small samples or extreme proportions)\n",
        "- **Bootstrap approach**: \"Let the data tell us about its own uncertainty\"\n",
        "\n",
        "**The Bootstrap Philosophy:**\n",
        "> *\"If our sample is representative of the population, then resampling from our sample should approximate resampling from the population\"*\n",
        "\n",
        "### **🔄 Why Bootstrap Works: The Mathematical Intuition**\n",
        "\n",
        "**Step 1: The Sampling Distribution Concept**\n",
        "```\n",
        "True Population: π = (π₁, π₂, ..., πₖ) [UNKNOWN]\n",
        "Our Sample: p̂ = (p̂₁, p̂₂, ..., p̂ₖ) [OBSERVED]\n",
        "```\n",
        "\n",
        "If we could draw infinite samples from the population, we'd get a **sampling distribution** of p̂. Bootstrap approximates this by treating our sample as a \"mini-population.\"\n",
        "\n",
        "**Step 2: The Resampling Logic**\n",
        "```\n",
        "Bootstrap Sample 1: p̂*₁ = resample(original_sample)\n",
        "Bootstrap Sample 2: p̂*₂ = resample(original_sample)\n",
        "...\n",
        "Bootstrap Sample B: p̂*ᵦ = resample(original_sample)\n",
        "```\n",
        "\n",
        "The distribution of {p̂*₁, p̂*₂, ..., p̂*ᵦ} approximates the sampling distribution of p̂.\n",
        "\n",
        "---\n",
        "\n",
        "## **Formula-by-Formula Deep Dive**\n",
        "\n",
        "### **1. CONFIDENCE INTERVALS: `CI_α = [θ̂*(α/2), θ̂*(1-α/2)]`**\n",
        "\n",
        "**The Intuition:**\n",
        "The confidence interval captures the **range of values our estimate could reasonably take** if we repeated the sampling process.\n",
        "\n",
        "**Why This Formula Works:**\n",
        "\n",
        "```python\n",
        "# Conceptual breakdown\n",
        "bootstrap_estimates = [p̂*₁, p̂*₂, ..., p̂*ᵦ]  # 1000 bootstrap samples\n",
        "sorted_estimates = sort(bootstrap_estimates)\n",
        "\n",
        "# For 95% CI:\n",
        "lower_bound = sorted_estimates[25]   # 2.5th percentile\n",
        "upper_bound = sorted_estimates[975]  # 97.5th percentile\n",
        "```\n",
        "\n",
        "**The Mathematical Logic:**\n",
        "1. **Percentile Method**: If 95% of bootstrap estimates fall within [L, U], then we're 95% confident the true parameter lies in this range\n",
        "2. **Empirical Quantiles**: We're using the **empirical distribution** of bootstrap estimates to approximate theoretical quantiles\n",
        "3. **Distribution-Free**: No assumptions about normality required\n",
        "\n",
        "**Why Interpretations Work:**\n",
        "\n",
        "**\"Narrow CI = Precise Estimation\":**\n",
        "- **Small variability** in bootstrap estimates → **small CI width**\n",
        "- Mathematical relationship: `CI_width ∝ σ_bootstrap ∝ σ_sampling`\n",
        "- **Large samples** → smaller sampling variance → narrower CIs\n",
        "\n",
        "**\"Wide CI for Minority Categories\":**\n",
        "- **Binomial variance**: `Var(p̂) = π(1-π)/n`\n",
        "- **Maximum variance** when π = 0.5, **minimum** when π → 0 or π → 1\n",
        "- BUT: **Absolute precision** is worse for rare categories due to fewer observations\n",
        "\n",
        "### **2. BOOTSTRAP STABILITY INDEX: `BSI = 1 - (IQR_bootstrap / median_bootstrap)`**\n",
        "\n",
        "**The Intuition:**\n",
        "BSI measures **relative variability** - how much do estimates vary relative to their central value?\n",
        "\n",
        "**Why This Formula Works:**\n",
        "\n",
        "**Step-by-Step Logic:**\n",
        "```python\n",
        "bootstrap_proportions = [0.51, 0.49, 0.52, 0.48, 0.50, ...]  # Bootstrap estimates\n",
        "\n",
        "median_prop = np.median(bootstrap_proportions)  # Central tendency\n",
        "Q1 = np.percentile(bootstrap_proportions, 25)   # 25th percentile  \n",
        "Q3 = np.percentile(bootstrap_proportions, 75)   # 75th percentile\n",
        "IQR = Q3 - Q1                                   # Interquartile range\n",
        "\n",
        "# Coefficient of Variation analog\n",
        "relative_variability = IQR / median_prop\n",
        "BSI = 1 - relative_variability  # Higher BSI = Lower relative variability\n",
        "```\n",
        "\n",
        "**Mathematical Justification:**\n",
        "1. **IQR**: Robust measure of spread (unaffected by outliers)\n",
        "2. **Median**: Robust measure of central tendency\n",
        "3. **Relative Scale**: `IQR/median` normalizes variability by the estimate magnitude\n",
        "4. **Complement**: `1 - relative_variability` converts to stability metric\n",
        "\n",
        "**Why Interpretations Work:**\n",
        "\n",
        "**\"High BSI (>0.9) = Stable Under Resampling\":**\n",
        "```python\n",
        "# Example: High Stability\n",
        "bootstrap_props = [0.49, 0.50, 0.51, 0.50, 0.49]  # Low variability\n",
        "IQR = 0.51 - 0.49 = 0.02\n",
        "median = 0.50\n",
        "BSI = 1 - (0.02/0.50) = 1 - 0.04 = 0.96  # High stability\n",
        "\n",
        "# Example: Low Stability  \n",
        "bootstrap_props = [0.35, 0.55, 0.40, 0.60, 0.45]  # High variability\n",
        "IQR = 0.55 - 0.40 = 0.15\n",
        "median = 0.45\n",
        "BSI = 1 - (0.15/0.45) = 1 - 0.33 = 0.67  # Low stability\n",
        "```\n",
        "\n",
        "**Connection to Sample Size:**\n",
        "- **Large samples** → smaller bootstrap variance → higher BSI\n",
        "- **Small samples** → larger bootstrap variance → lower BSI\n",
        "\n",
        "### **3. BIAS ESTIMATION: `Bias = E[θ̂*] - θ̂_original`**\n",
        "\n",
        "**The Intuition:**\n",
        "Bias measures **systematic error** - do our estimates consistently over- or under-estimate the true value?\n",
        "\n",
        "**Why This Formula Works:**\n",
        "\n",
        "**Theoretical Foundation:**\n",
        "```\n",
        "True Bias = E[θ̂] - θ_true  [Unknown - we don't know θ_true]\n",
        "Bootstrap Bias = E[θ̂*] - θ̂_original  [Estimable from bootstrap samples]\n",
        "```\n",
        "\n",
        "**The Bootstrap Principle:**\n",
        "- **Relationship preservation**: `θ̂* relates to θ̂_original` the same way `θ̂_original relates to θ_true`\n",
        "- **Bias approximation**: `E[θ̂*] - θ̂_original ≈ E[θ̂] - θ_true`\n",
        "\n",
        "**Step-by-Step Calculation:**\n",
        "```python\n",
        "original_proportion = 0.56  # From full dataset\n",
        "\n",
        "bootstrap_proportions = [0.54, 0.57, 0.55, 0.58, 0.56, ...]  # 1000 bootstrap samples\n",
        "mean_bootstrap = np.mean(bootstrap_proportions)  # E[θ̂*]\n",
        "\n",
        "bias_estimate = mean_bootstrap - original_proportion\n",
        "# If bias_estimate = 0.002, our method slightly overestimates by 0.2%\n",
        "```\n",
        "\n",
        "**Why Interpretations Work:**\n",
        "\n",
        "**\"Low Bias (<0.01) = Unbiased Estimation\":**\n",
        "- **Theoretical expectation**: For simple random sampling, sample proportions are unbiased estimators\n",
        "- **Bootstrap verification**: Confirms theoretical expectation empirically\n",
        "- **Threshold interpretation**: |bias| < 0.01 means systematic error < 1%\n",
        "\n",
        "**\"Bias Reveals Systematic Over/Under-estimation\":**\n",
        "```python\n",
        "# Positive bias: Consistently overestimating\n",
        "bias = +0.015  # Method overestimates by 1.5%\n",
        "\n",
        "# Negative bias: Consistently underestimating  \n",
        "bias = -0.008  # Method underestimates by 0.8%\n",
        "\n",
        "# Near-zero bias: Unbiased estimation\n",
        "bias = 0.0003  # Negligible systematic error\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **🔬 Deep Dive: Why These Interpretations Are Valid**\n",
        "\n",
        "### **\"Bootstrap Approximates True Sampling Distribution\"**\n",
        "\n",
        "**Mathematical Proof Sketch:**\n",
        "1. **Glivenko-Cantelli Theorem**: Sample CDF converges to population CDF\n",
        "2. **Bootstrap Consistency**: Under regularity conditions, bootstrap distribution converges to true sampling distribution\n",
        "3. **For proportions**: Multinomial sampling ensures consistency\n",
        "\n",
        "**Practical Validation:**\n",
        "```python\n",
        "# If we could repeat the study 1000 times:\n",
        "true_sampling_distribution = [study_1_proportion, study_2_proportion, ..., study_1000_proportion]\n",
        "\n",
        "# Bootstrap approximation from single study:\n",
        "bootstrap_distribution = [bootstrap_1_proportion, bootstrap_2_proportion, ..., bootstrap_1000_proportion]\n",
        "\n",
        "# Bootstrap theorem: bootstrap_distribution ≈ true_sampling_distribution\n",
        "```\n",
        "\n",
        "### **\"Large Sample Effect on CI Width\"**\n",
        "\n",
        "**Mathematical Relationship:**\n",
        "```\n",
        "CI_width ∝ σ_bootstrap ∝ σ_sampling ∝ √(π(1-π)/n)\n",
        "```\n",
        "\n",
        "**Intuitive Explanation:**\n",
        "- **More data** → **less uncertainty** → **narrower confidence intervals**\n",
        "- **Bootstrap captures this** because resampling from larger samples produces less variable estimates\n",
        "\n",
        "**Numerical Example:**\n",
        "```python\n",
        "# Small sample (n=50):\n",
        "# Bootstrap estimates: [0.40, 0.60, 0.35, 0.65, ...]  # High variability\n",
        "# CI width ≈ 0.25\n",
        "\n",
        "# Large sample (n=500):  \n",
        "# Bootstrap estimates: [0.48, 0.52, 0.49, 0.51, ...]  # Low variability\n",
        "# CI width ≈ 0.08\n",
        "```\n",
        "\n",
        "### **\"Wide CI for Minority Categories\"**\n",
        "\n",
        "**The Paradox Explained:**\n",
        "While binomial variance is **minimized** for extreme proportions (π → 0 or 1), **absolute precision** is worse for minority categories due to **effective sample size**.\n",
        "\n",
        "**Mathematical Insight:**\n",
        "```python\n",
        "# Majority category (π = 0.8, n = 200):\n",
        "effective_n_majority = 200 * 0.8 = 160 observations\n",
        "# High precision due to many observations\n",
        "\n",
        "# Minority category (π = 0.2, n = 200):  \n",
        "effective_n_minority = 200 * 0.2 = 40 observations\n",
        "# Lower precision due to fewer observations\n",
        "```\n",
        "\n",
        "**Bootstrap Captures This:**\n",
        "- **Resampling** reflects the **unequal representation** of categories\n",
        "- **Minority categories** have **higher sampling variability** in bootstrap samples\n",
        "- **CI width** reflects this **category-specific uncertainty**\n",
        "\n",
        "---\n",
        "\n",
        "## **🎯 Practical Implications for Mall Customers Dataset**\n",
        "\n",
        "### **Expected Bootstrap Behavior:**\n",
        "\n",
        "**For Gender (n=200, balanced ~50/50):**\n",
        "```python\n",
        "# Expected results:\n",
        "BSI_male ≈ BSI_female ≈ 0.95+     # High stability (large n, balanced)\n",
        "CI_width ≈ 0.06-0.08               # Narrow CIs (large effective n)\n",
        "bias ≈ 0.001                       # Negligible bias (unbiased estimator)\n",
        "```\n",
        "\n",
        "**If we had an imbalanced category (e.g., 90/10 split):**\n",
        "```python\n",
        "# Majority category (90%):\n",
        "effective_n = 200 * 0.9 = 180      # Many observations\n",
        "BSI_majority ≈ 0.98                # Very high stability\n",
        "CI_width ≈ 0.04                    # Very narrow CI\n",
        "\n",
        "# Minority category (10%):  \n",
        "effective_n = 200 * 0.1 = 20       # Few observations\n",
        "BSI_minority ≈ 0.85                # Lower stability\n",
        "CI_width ≈ 0.12                    # Wider CI\n",
        "```\n",
        "\n",
        "### **Decision Framework:**\n",
        "\n",
        "| Metric | Excellent | Good | Concerning | Action Needed |\n",
        "|--------|-----------|------|------------|---------------|\n",
        "| **BSI** | >0.95 | >0.90 | >0.80 | ≤0.80 |\n",
        "| **CI Width** | <0.05 | <0.10 | <0.15 | ≥0.15 |\n",
        "| **\\|Bias\\|** | <0.001 | <0.01 | <0.05 | ≥0.05 |\n",
        "\n",
        "**Interpretation Guide:**\n",
        "- **BSI > 0.95**: Estimates are highly reliable for decision-making\n",
        "- **CI width < 0.10**: Sufficient precision for most practical purposes  \n",
        "- **|Bias| < 0.01**: Systematic error is negligible\n",
        "- **All metrics good**: Proceed with confidence in categorical analysis\n",
        "\n",
        "This bootstrap framework provides a **principled, assumption-free** way to assess the reliability of categorical data analysis, with interpretations that directly translate to practical decision-making confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Help with decision making of which analysis to run when.\n",
        "\n",
        "I'll create comprehensive Mermaid flowcharts that guide you through the decision-making process for selecting appropriate stability and robustness analyses. Let me build a hierarchical set of charts covering different scenarios and decision points.\n",
        "\n",
        "\n",
        "## A comprehensive set of 7 Mermaid flowcharts that provide complete decision-making guidance for stability and robustness analysis. Here's what each chart covers:\n",
        "\n",
        "## **📊 Complete Flowchart Suite for Stability & Robustness Analysis**\n",
        "\n",
        "### **1. Master Decision Framework**\n",
        "- **Purpose**: Overall entry point and high-level decision tree\n",
        "- **Key Features**: \n",
        "  - Data type classification (binary, multinomial, ordinal)\n",
        "  - Sample size routing (small/medium/large protocols)\n",
        "  - Quality assessment integration\n",
        "  - Final interpretation framework\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    A[\"🎯 Stability & Robustness Analysis<br/>Decision Framework\"] --> B{\"What type of<br/>categorical data?\"}\n",
        "    \n",
        "    B -->|\"Binary<br/>(2 categories)\"| C[\"Binary Categorical<br/>Analysis Path\"]\n",
        "    B -->|\"Multinomial<br/>(3+ categories)\"| D[\"Multinomial Categorical<br/>Analysis Path\"]\n",
        "    B -->|\"Ordinal<br/>(ordered categories)\"| E[\"Ordinal Categorical<br/>Analysis Path\"]\n",
        "    \n",
        "    C --> F{\"Sample Size?\"}\n",
        "    D --> F\n",
        "    E --> F\n",
        "    \n",
        "    F -->|\"n < 50\"| G[\"⚠️ Small Sample<br/>Protocol\"]\n",
        "    F -->|\"50 ≤ n < 200\"| H[\"📊 Medium Sample<br/>Protocol\"]\n",
        "    F -->|\"n ≥ 200\"| I[\"✅ Large Sample<br/>Protocol\"]\n",
        "    \n",
        "    G --> J[\"• Bootstrap Analysis (Essential)<br/>• Sensitivity Analysis (Critical)<br/>• Skip Temporal (Unreliable)\"]\n",
        "    H --> K[\"• Bootstrap Analysis<br/>• Subset Stability<br/>• Sensitivity Analysis<br/>• Temporal (if applicable)\"]\n",
        "    I --> L[\"• Full Analysis Suite<br/>• All Methods Available<br/>• High Confidence Results\"]\n",
        "    \n",
        "    J --> M{\"Data Quality<br/>Concerns?\"}\n",
        "    K --> M\n",
        "    L --> M\n",
        "    \n",
        "    M -->|\"Yes\"| N[\"🔍 Enhanced<br/>Sensitivity Testing\"]\n",
        "    M -->|\"No\"| O[\"📈 Standard<br/>Analysis Protocol\"]\n",
        "    \n",
        "    N --> P[\"• Missing Data Analysis<br/>• Outlier Robustness<br/>• Multiple Imputation<br/>• Cross-Validation\"]\n",
        "    O --> Q[\"• Bootstrap Confidence<br/>• Temporal Consistency<br/>• Subset Validation\"]\n",
        "    \n",
        "    P --> R[\"Generate Comprehensive<br/>Stability Report\"]\n",
        "    Q --> R\n",
        "    \n",
        "    R --> S{\"Results<br/>Interpretation\"}\n",
        "    \n",
        "    S -->|\"High Stability<br/>(>0.90)\"| T[\"✅ Proceed with<br/>Confidence\"]\n",
        "    S -->|\"Moderate Stability<br/>(0.80-0.90)\"| U[\"⚠️ Proceed with<br/>Caution\"]\n",
        "    S -->|\"Low Stability<br/>(<0.80)\"| V[\"🚨 Investigate or<br/>Collect More Data\"]\n",
        "    \n",
        "    style A fill:#e1f5fe,color:#000\n",
        "    style G fill:#ffebee,color:#000\n",
        "    style H fill:#fff3e0,color:#000\n",
        "    style I fill:#e8f5e8,color:#000\n",
        "    style T fill:#e8f5e8,color:#000\n",
        "    style U fill:#fff3e0,color:#000\n",
        "    style V fill:#ffebee,color:#000\n",
        "```\n",
        "\n",
        "### **2. Bootstrap Analysis Decision Tree**\n",
        "- **Purpose**: Detailed guidance for bootstrap analysis selection\n",
        "- **Key Features**:\n",
        "  - Goal-specific routing (CI, BSI, bias, comprehensive)\n",
        "  - Balance-aware protocols (balanced vs imbalanced)\n",
        "  - Quality thresholds and interpretation\n",
        "  - Sample size recommendations\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    A[\"🔄 Bootstrap Analysis<br/>Decision Tree\"] --> B{\"Primary Goal?\"}\n",
        "    \n",
        "    B -->|\"Confidence Intervals\"| C[\"CI-Focused Bootstrap\"]\n",
        "    B -->|\"Stability Assessment\"| D[\"BSI-Focused Bootstrap\"]\n",
        "    B -->|\"Bias Detection\"| E[\"Bias-Focused Bootstrap\"]\n",
        "    B -->|\"Comprehensive\"| F[\"Full Bootstrap Suite\"]\n",
        "    \n",
        "    C --> C1{\"Sample Balance?\"}\n",
        "    D --> D1{\"Temporal Component?\"}\n",
        "    E --> E1{\"Estimation Method?\"}\n",
        "    F --> F1[\"Execute All Methods\"]\n",
        "    \n",
        "    C1 -->|\"Balanced<br/>(all categories >10%)\"| C2[\"Standard Percentile CI<br/>n_bootstrap = 1000\"]\n",
        "    C1 -->|\"Imbalanced<br/>(minority <10%)\"| C3[\"Stratified Bootstrap CI<br/>n_bootstrap = 2000+\"]\n",
        "    \n",
        "    D1 -->|\"Yes\"| D2[\"Time-Stratified BSI<br/>+ Temporal Consistency\"]\n",
        "    D1 -->|\"No\"| D3[\"Standard BSI<br/>Cross-sectional Only\"]\n",
        "    \n",
        "    E1 -->|\"Simple Proportions\"| E2[\"Direct Bias Estimation<br/>E[θ̂*] - θ̂\"]\n",
        "    E1 -->|\"Complex Estimators\"| E3[\"Bias-Corrected Bootstrap<br/>+ Jackknife\"]\n",
        "    \n",
        "    F1 --> G[\"Comprehensive Output\"]\n",
        "    C2 --> G\n",
        "    C3 --> G\n",
        "    D2 --> G\n",
        "    D3 --> G\n",
        "    E2 --> G\n",
        "    E3 --> G\n",
        "    \n",
        "    G --> H{\"Bootstrap Results<br/>Quality Check\"}\n",
        "    \n",
        "    H -->|\"BSI > 0.95<br/>CI width < 0.05<br/>|Bias| < 0.001\"| I[\"✅ Excellent<br/>Bootstrap Stability\"]\n",
        "    H -->|\"BSI > 0.90<br/>CI width < 0.10<br/>|Bias| < 0.01\"| J[\"✅ Good<br/>Bootstrap Stability\"]\n",
        "    H -->|\"BSI > 0.80<br/>CI width < 0.15<br/>|Bias| < 0.05\"| K[\"⚠️ Moderate<br/>Bootstrap Stability\"]\n",
        "    H -->|\"BSI ≤ 0.80<br/>CI width ≥ 0.15<br/>|Bias| ≥ 0.05\"| L[\"🚨 Poor<br/>Bootstrap Stability\"]\n",
        "    \n",
        "    I --> M[\"Proceed with Analysis\"]\n",
        "    J --> M\n",
        "    K --> N[\"Consider Sample<br/>Size Increase\"]\n",
        "    L --> O[\"Investigate Data Quality<br/>or Collection Issues\"]\n",
        "    \n",
        "    N --> P{\"Can Increase<br/>Sample Size?\"}\n",
        "    P -->|\"Yes\"| Q[\"Collect More Data\"]\n",
        "    P -->|\"No\"| R[\"Use Robust Methods<br/>+ Report Uncertainty\"]\n",
        "    \n",
        "    O --> S[\"Data Quality<br/>Investigation\"]\n",
        "    \n",
        "    style A fill:#e3f2fd,color:#000\n",
        "    style I fill:#e8f5e8,color:#000\n",
        "    style J fill:#e8f5e8,color:#000\n",
        "    style K fill:#fff3e0,color:#000\n",
        "    style L fill:#ffebee,color:#000\n",
        "```\n",
        "\n",
        "### **3. Temporal Stability Analysis Framework**\n",
        "- **Purpose**: Temporal analysis decision making\n",
        "- **Key Features**:\n",
        "  - Real vs simulated temporal data handling\n",
        "  - Time span considerations (short/medium/long-term)\n",
        "  - Pattern type identification (trend, seasonal, cyclical)\n",
        "  - TCI interpretation guidelines\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    A[\"⏰ Temporal Stability<br/>Analysis Decision\"] --> B{\"Temporal Data<br/>Available?\"}\n",
        "    \n",
        "    B -->|\"Yes - Real Timestamps\"| C[\"Real Temporal Analysis\"]\n",
        "    B -->|\"No - Cross-sectional\"| D[\"Simulated Temporal Analysis\"]\n",
        "    B -->|\"Partial - Some Ordering\"| E[\"Pseudo-temporal Analysis\"]\n",
        "    \n",
        "    C --> C1{\"Time Span?\"}\n",
        "    D --> D1[\"Create Artificial<br/>Time Periods\"]\n",
        "    E --> E1[\"Use Available<br/>Ordering Variable\"]\n",
        "    \n",
        "    C1 -->|\"< 1 year\"| C2[\"Short-term Stability<br/>• Daily/Weekly patterns<br/>• Seasonal effects minimal\"]\n",
        "    C1 -->|\"1-5 years\"| C3[\"Medium-term Stability<br/>• Monthly/Quarterly patterns<br/>• Seasonal decomposition\"]\n",
        "    C1 -->|\"> 5 years\"| C4[\"Long-term Stability<br/>• Annual patterns<br/>• Trend analysis<br/>• Structural breaks\"]\n",
        "    \n",
        "    D1 --> D2{\"Stratification<br/>Variable Available?\"}\n",
        "    D2 -->|\"Yes\"| D3[\"Stratified Random<br/>Time Assignment\"]\n",
        "    D2 -->|\"No\"| D4[\"Pure Random<br/>Time Assignment\"]\n",
        "    \n",
        "    E1 --> E2{\"Ordering<br/>Meaningful?\"}\n",
        "    E2 -->|\"Yes\"| E3[\"Sequential Analysis<br/>Based on Order\"]\n",
        "    E2 -->|\"No\"| E4[\"Treat as Random<br/>Assignment\"]\n",
        "    \n",
        "    C2 --> F[\"Calculate TCI Matrix\"]\n",
        "    C3 --> F\n",
        "    C4 --> F\n",
        "    D3 --> F\n",
        "    D4 --> F\n",
        "    E3 --> F\n",
        "    E4 --> F\n",
        "    \n",
        "    F --> G[\"Temporal Consistency<br/>Index (TCI)\"]\n",
        "    G --> H[\"Chi-square<br/>Homogeneity Test\"]\n",
        "    H --> I[\"Coefficient of<br/>Variation Analysis\"]\n",
        "    \n",
        "    I --> J{\"TCI Results<br/>Interpretation\"}\n",
        "    \n",
        "    J -->|\"TCI > 0.95<br/>p > 0.05\"| K[\"✅ Excellent<br/>Temporal Stability\"]\n",
        "    J -->|\"TCI > 0.90<br/>p > 0.05\"| L[\"✅ Good<br/>Temporal Stability\"]\n",
        "    J -->|\"TCI > 0.80<br/>p ≤ 0.05\"| M[\"⚠️ Moderate Stability<br/>Some Temporal Variation\"]\n",
        "    J -->|\"TCI ≤ 0.80<br/>p ≤ 0.05\"| N[\"🚨 Poor Stability<br/>Significant Temporal Changes\"]\n",
        "    \n",
        "    K --> O[\"Temporal Assumption<br/>Valid for Analysis\"]\n",
        "    L --> O\n",
        "    M --> P[\"Investigate Temporal<br/>Patterns Further\"]\n",
        "    N --> Q[\"Account for Temporal<br/>Effects in Analysis\"]\n",
        "    \n",
        "    P --> P1{\"Pattern Type?\"}\n",
        "    P1 -->|\"Trend\"| P2[\"Linear/Non-linear<br/>Trend Modeling\"]\n",
        "    P1 -->|\"Seasonal\"| P3[\"Seasonal Decomposition<br/>Analysis\"]\n",
        "    P1 -->|\"Cyclical\"| P4[\"Fourier Analysis<br/>Spectral Methods\"]\n",
        "    P1 -->|\"Irregular\"| P5[\"Change Point<br/>Detection\"]\n",
        "    \n",
        "    Q --> Q1[\"Time-Stratified<br/>Analysis Approach\"]\n",
        "    \n",
        "    style A fill:#e8eaf6,color:#000\n",
        "    style K fill:#e8f5e8,color:#000\n",
        "    style L fill:#e8f5e8,color:#000\n",
        "    style M fill:#fff3e0,color:#000\n",
        "    style N fill:#ffebee,color:#000\n",
        "```\n",
        "\n",
        "### **4. Subset Stability Analysis Framework**\n",
        "- **Purpose**: Cross-validation and subset consistency analysis\n",
        "- **Key Features**:\n",
        "  - CV strategy selection (standard, stratified, LOOCV)\n",
        "  - Sample size adequacy assessment\n",
        "  - Partition method selection\n",
        "  - CVSI interpretation and follow-up actions\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    A[\"📂 Subset Stability<br/>Analysis Framework\"] --> B{\"Analysis Purpose?\"}\n",
        "    \n",
        "    B -->|\"Cross-Validation<br/>Reliability\"| C[\"CV Stability Focus\"]\n",
        "    B -->|\"Sample Size<br/>Requirements\"| D[\"Sample Size Focus\"]\n",
        "    B -->|\"Data Partition<br/>Effects\"| E[\"Partition Effects Focus\"]\n",
        "    B -->|\"Comprehensive<br/>Assessment\"| F[\"Full Subset Analysis\"]\n",
        "    \n",
        "    C --> C1{\"CV Strategy?\"}\n",
        "    D --> D1{\"Current Sample<br/>Size Adequate?\"}\n",
        "    E --> E1{\"Partition Method?\"}\n",
        "    F --> F1[\"Execute All Methods\"]\n",
        "    \n",
        "    C1 -->|\"Standard K-Fold\"| C2[\"5-Fold CV<br/>Equal partitions\"]\n",
        "    C1 -->|\"Stratified K-Fold\"| C3[\"Stratified 5-Fold CV<br/>Preserve proportions\"]\n",
        "    C1 -->|\"Leave-One-Out\"| C4[\"LOOCV<br/>Maximum validation\"]\n",
        "    C1 -->|\"Time-Based\"| C5[\"Temporal CV<br/>Sequential splits\"]\n",
        "    \n",
        "    D1 -->|\"Yes (n ≥ 100)\"| D2[\"Test Multiple<br/>Subset Sizes<br/>50%, 70%, 80%\"]\n",
        "    D1 -->|\"Uncertain\"| D3[\"Power Analysis<br/>Sample Size Calculation\"]\n",
        "    D1 -->|\"No (n < 100)\"| D4[\"Bootstrap + Subset<br/>Combined Approach\"]\n",
        "    \n",
        "    E1 -->|\"Random\"| E2[\"Random Partitioning<br/>Multiple iterations\"]\n",
        "    E1 -->|\"Systematic\"| E3[\"Systematic Sampling<br/>Every kth observation\"]\n",
        "    E1 -->|\"Stratified\"| E4[\"Stratified Partitioning<br/>Maintain balance\"]\n",
        "    E1 -->|\"Clustered\"| E5[\"Cluster-based<br/>Partitioning\"]\n",
        "    \n",
        "    F1 --> G[\"Calculate CVSI\"]\n",
        "    C2 --> G\n",
        "    C3 --> G\n",
        "    C4 --> G\n",
        "    C5 --> G\n",
        "    D2 --> G\n",
        "    D3 --> G\n",
        "    D4 --> G\n",
        "    E2 --> G\n",
        "    E3 --> G\n",
        "    E4 --> G\n",
        "    E5 --> G\n",
        "    \n",
        "    G --> H[\"Subset Homogeneity<br/>Chi-square Test\"]\n",
        "    H --> I[\"TV Distance<br/>Analysis\"]\n",
        "    \n",
        "    I --> J{\"CVSI Results<br/>Assessment\"}\n",
        "    \n",
        "    J -->|\"CVSI > 0.95<br/>p > 0.05\"| K[\"✅ Excellent<br/>Subset Consistency\"]\n",
        "    J -->|\"CVSI > 0.90<br/>p > 0.05\"| L[\"✅ Good<br/>Subset Consistency\"]\n",
        "    J -->|\"CVSI > 0.80<br/>p ≤ 0.05\"| M[\"⚠️ Moderate Consistency<br/>Some subset variation\"]\n",
        "    J -->|\"CVSI ≤ 0.80<br/>p ≤ 0.05\"| N[\"🚨 Poor Consistency<br/>Significant subset effects\"]\n",
        "    \n",
        "    K --> O[\"Subset Independence<br/>Assumption Valid\"]\n",
        "    L --> O\n",
        "    M --> P[\"Investigate Subset<br/>Differences\"]\n",
        "    N --> Q[\"Data Quality<br/>Investigation Required\"]\n",
        "    \n",
        "    P --> P1{\"Difference Source?\"}\n",
        "    P1 -->|\"Sample Size Effect\"| P2[\"Increase Sample Size<br/>or Use Weighted Methods\"]\n",
        "    P1 -->|\"Systematic Bias\"| P3[\"Stratified Sampling<br/>Correction\"]\n",
        "    P1 -->|\"Random Variation\"| P4[\"Accept Moderate<br/>Uncertainty\"]\n",
        "    \n",
        "    Q --> Q1[\"Check for:<br/>• Outliers<br/>• Missing patterns<br/>• Data entry errors<br/>• Sampling bias\"]\n",
        "    \n",
        "    style A fill:#f3e5f5,color:#000\n",
        "    style K fill:#e8f5e8,color:#000\n",
        "    style L fill:#e8f5e8,color:#000\n",
        "    style M fill:#fff3e0,color:#000\n",
        "    style N fill:#ffebee,color:#000\n",
        "```\n",
        "\n",
        "### **5. Sensitivity Analysis Decision Matrix**\n",
        "- **Purpose**: Comprehensive sensitivity testing guidance\n",
        "- **Key Features**:\n",
        "  - Missing data mechanism handling (MCAR, MAR, MNAR)\n",
        "  - Sample size effect analysis\n",
        "  - Outlier robustness assessment\n",
        "  - Method comparison protocols\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    A[\"🔍 Sensitivity Analysis<br/>Decision Matrix\"] --> B{\"Primary Concern?\"}\n",
        "    \n",
        "    B -->|\"Missing Data<br/>Impact\"| C[\"Missing Data<br/>Sensitivity\"]\n",
        "    B -->|\"Sample Size<br/>Effects\"| D[\"Sample Size<br/>Sensitivity\"]\n",
        "    B -->|\"Outlier<br/>Robustness\"| E[\"Outlier<br/>Sensitivity\"]\n",
        "    B -->|\"Method<br/>Dependence\"| F[\"Method<br/>Sensitivity\"]\n",
        "    B -->|\"All Factors\"| G[\"Comprehensive<br/>Sensitivity Suite\"]\n",
        "    \n",
        "    C --> C1{\"Missing Data<br/>Pattern?\"}\n",
        "    D --> D1{\"Current Sample<br/>Characteristics?\"}\n",
        "    E --> E1{\"Outlier Definition<br/>for Categories?\"}\n",
        "    F --> F1{\"Methods to<br/>Compare?\"}\n",
        "    G --> G1[\"Execute All<br/>Sensitivity Tests\"]\n",
        "    \n",
        "    C1 -->|\"MCAR<br/>(Random)\"| C2[\"Random Imputation<br/>vs Mode vs Deletion\"]\n",
        "    C1 -->|\"MAR<br/>(Predictable)\"| C3[\"Model-based<br/>Imputation Methods\"]\n",
        "    C1 -->|\"MNAR<br/>(Systematic)\"| C4[\"Multiple Imputation<br/>Sensitivity Analysis\"]\n",
        "    C1 -->|\"Unknown\"| C5[\"Test All Missing<br/>Mechanisms\"]\n",
        "    \n",
        "    D1 -->|\"Large (n>200)<br/>Balanced\"| D2[\"Subsample Analysis<br/>50, 100, 150\"]\n",
        "    D1 -->|\"Medium (50-200)<br/>Balanced\"| D3[\"Bootstrap + Subsample<br/>Combined Analysis\"]\n",
        "    D1 -->|\"Small (n<50)\"| D4[\"Bootstrap Only<br/>No Subsampling\"]\n",
        "    D1 -->|\"Imbalanced\"| D5[\"Stratified Subsampling<br/>Maintain Proportions\"]\n",
        "    \n",
        "    E1 -->|\"Rare Categories<br/>(<5% frequency)\"| E2[\"Rare Category<br/>Influence Analysis\"]\n",
        "    E1 -->|\"Extreme Values<br/>(if ordinal)\"| E3[\"Extreme Value<br/>Robustness Test\"]\n",
        "    E1 -->|\"Data Entry<br/>Errors\"| E4[\"Error Simulation<br/>Robustness Test\"]\n",
        "    \n",
        "    F1 -->|\"Estimation<br/>Methods\"| F2[\"Compare:<br/>MLE vs Bayesian<br/>vs Robust estimators\"]\n",
        "    F1 -->|\"Inference<br/>Methods\"| F3[\"Compare:<br/>Frequentist vs Bayesian<br/>vs Nonparametric\"]\n",
        "    F1 -->|\"Modeling<br/>Approaches\"| F4[\"Compare:<br/>Different model<br/>specifications\"]\n",
        "    \n",
        "    G1 --> H[\"Calculate ISI<br/>(Imputation Sensitivity Index)\"]\n",
        "    C2 --> H\n",
        "    C3 --> H\n",
        "    C4 --> H\n",
        "    C5 --> H\n",
        "    D2 --> I[\"Calculate SSAI<br/>(Sample Size Adequacy Index)\"]\n",
        "    D3 --> I\n",
        "    D4 --> I\n",
        "    D5 --> I\n",
        "    E2 --> J[\"Calculate Influence<br/>Functions\"]\n",
        "    E3 --> J\n",
        "    E4 --> J\n",
        "    F2 --> K[\"Calculate Method<br/>Concordance\"]\n",
        "    F3 --> K\n",
        "    F4 --> K\n",
        "    \n",
        "    H --> L[\"Missing Data<br/>Robustness Assessment\"]\n",
        "    I --> M[\"Sample Size<br/>Adequacy Assessment\"]\n",
        "    J --> N[\"Outlier<br/>Robustness Assessment\"]\n",
        "    K --> O[\"Method<br/>Robustness Assessment\"]\n",
        "    \n",
        "    L --> P{\"Sensitivity Results<br/>Integration\"}\n",
        "    M --> P\n",
        "    N --> P\n",
        "    O --> P\n",
        "    \n",
        "    P -->|\"All ISI > 0.95<br/>All SSAI > 1.0<br/>Low influence<br/>High concordance\"| Q[\"✅ Highly Robust<br/>Results\"]\n",
        "    P -->|\"Most ISI > 0.90<br/>Most SSAI > 0.8<br/>Moderate influence<br/>Good concordance\"| R[\"✅ Moderately Robust<br/>Results\"]\n",
        "    P -->|\"Some ISI < 0.90<br/>Some SSAI < 0.8<br/>High influence<br/>Poor concordance\"| S[\"⚠️ Sensitivity Detected<br/>Proceed with Caution\"]\n",
        "    P -->|\"Many ISI < 0.80<br/>SSAI < 0.5<br/>Very high influence<br/>Low concordance\"| T[\"🚨 High Sensitivity<br/>Results Unreliable\"]\n",
        "    \n",
        "    Q --> U[\"Report Results<br/>with Confidence\"]\n",
        "    R --> V[\"Report Results<br/>with Caveats\"]\n",
        "    S --> W[\"Additional Analysis<br/>or Data Collection\"]\n",
        "    T --> X[\"Fundamental Issues<br/>Require Resolution\"]\n",
        "    \n",
        "    style A fill:#fff8e1,color:#000\n",
        "    style Q fill:#e8f5e8,color:#000\n",
        "    style R fill:#e8f5e8,color:#000\n",
        "    style S fill:#fff3e0,color:#000\n",
        "    style T fill:#ffebee,color:#000\n",
        "```\n",
        "\n",
        "### **6. Comprehensive Protocol Integration**\n",
        "- **Purpose**: Complete analysis protocol with quality scoring\n",
        "- **Key Features**:\n",
        "  - Condition-based analysis selection\n",
        "  - Weighted stability scoring system\n",
        "  - Quality thresholds and recommendations\n",
        "  - Final deliverable specifications\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    A[\"📊 Comprehensive Stability<br/>& Robustness Protocol\"] --> B[\"Initial Data Assessment\"]\n",
        "    \n",
        "    B --> B1[\"Sample Size: n\"]\n",
        "    B --> B2[\"Categories: k\"]  \n",
        "    B --> B3[\"Balance Ratio\"]\n",
        "    B --> B4[\"Missing Data %\"]\n",
        "    B --> B5[\"Temporal Component\"]\n",
        "    \n",
        "    B1 --> C{\"Decision Matrix<br/>Entry Point\"}\n",
        "    B2 --> C\n",
        "    B3 --> C\n",
        "    B4 --> C\n",
        "    B5 --> C\n",
        "    \n",
        "    C -->|\"n≥200, k≤5, Balanced<br/>Missing<5%, No temporal\"| D[\"🟢 Ideal Conditions<br/>Full Analysis Suite\"]\n",
        "    C -->|\"100≤n<200, k≤10<br/>Moderate imbalance<br/>Missing 5-15%\"| E[\"🟡 Standard Conditions<br/>Core Analysis + Sensitivity\"]\n",
        "    C -->|\"50≤n<100, k>10<br/>High imbalance<br/>Missing 15-30%\"| F[\"🟠 Challenging Conditions<br/>Bootstrap + Sensitivity Focus\"]\n",
        "    C -->|\"n<50, k>15<br/>Severe imbalance<br/>Missing >30%\"| G[\"🔴 Difficult Conditions<br/>Limited Analysis Options\"]\n",
        "    \n",
        "    D --> D1[\"Execute All Methods:<br/>✓ Bootstrap (n=1000)<br/>✓ Temporal Stability<br/>✓ Subset Analysis (CV)<br/>✓ Full Sensitivity Suite\"]\n",
        "    \n",
        "    E --> E1[\"Execute Core Methods:<br/>✓ Bootstrap (n=1500)<br/>✓ Subset Analysis<br/>✓ Missing Data Sensitivity<br/>✓ Sample Size Sensitivity\"]\n",
        "    \n",
        "    F --> F1[\"Execute Essential Methods:<br/>✓ Bootstrap (n=2000)<br/>✓ Missing Data Analysis<br/>✓ Method Sensitivity<br/>⚠ Limited Subset Analysis\"]\n",
        "    \n",
        "    G --> G1[\"Execute Minimal Methods:<br/>✓ Bootstrap Only (n=5000)<br/>✓ Basic Sensitivity<br/>❌ Skip Temporal<br/>❌ Skip Subset Analysis\"]\n",
        "    \n",
        "    D1 --> H[\"Quality Thresholds<br/>Assessment\"]\n",
        "    E1 --> H\n",
        "    F1 --> H\n",
        "    G1 --> H\n",
        "    \n",
        "    H --> H1{\"Overall Stability<br/>Score Calculation\"}\n",
        "    \n",
        "    H1 --> I[\"Weighted Average:<br/>• TCI × 0.25<br/>• BSI × 0.35<br/>• CVSI × 0.25<br/>• Sensitivity × 0.15\"]\n",
        "    \n",
        "    I --> J{\"Final Assessment<br/>& Recommendations\"}\n",
        "    \n",
        "    J -->|\"Score > 0.95\"| K[\"🌟 EXCELLENT<br/>Highly Reliable Results\"]\n",
        "    J -->|\"0.90 ≤ Score ≤ 0.95\"| L[\"✅ GOOD<br/>Reliable Results\"]\n",
        "    J -->|\"0.80 ≤ Score < 0.90\"| M[\"⚠️ MODERATE<br/>Acceptable with Caveats\"]\n",
        "    J -->|\"0.70 ≤ Score < 0.80\"| N[\"🚨 POOR<br/>Unreliable Results\"]\n",
        "    J -->|\"Score < 0.70\"| O[\"❌ UNACCEPTABLE<br/>Do Not Use Results\"]\n",
        "    \n",
        "    K --> K1[\"Recommendations:<br/>• Proceed with full confidence<br/>• Results are publication-ready<br/>• Use for critical decisions\"]\n",
        "    \n",
        "    L --> L1[\"Recommendations:<br/>• Proceed with confidence<br/>• Report stability metrics<br/>• Suitable for most purposes\"]\n",
        "    \n",
        "    M --> M1[\"Recommendations:<br/>• Proceed with caution<br/>• Report limitations clearly<br/>• Consider additional validation\"]\n",
        "    \n",
        "    N --> N1[\"Recommendations:<br/>• Do not proceed<br/>• Collect more data<br/>• Investigate data quality issues\"]\n",
        "    \n",
        "    O --> O1[\"Recommendations:<br/>• Stop analysis<br/>• Fundamental data problems<br/>• Redesign data collection\"]\n",
        "    \n",
        "    K1 --> P[\"Generate Comprehensive<br/>Stability Report\"]\n",
        "    L1 --> P\n",
        "    M1 --> P\n",
        "    N1 --> Q[\"Generate Problem<br/>Diagnosis Report\"]\n",
        "    O1 --> Q\n",
        "    \n",
        "    P --> R[\"📋 Final Deliverables:<br/>• Stability Dashboard<br/>• Confidence Intervals<br/>• Method Validation<br/>• Decision Support\"]\n",
        "    \n",
        "    Q --> S[\"🔧 Action Items:<br/>• Data Quality Issues<br/>• Sample Size Requirements<br/>• Collection Recommendations<br/>• Alternative Approaches\"]\n",
        "    \n",
        "    style A fill:#e8f5e8,color:#000\n",
        "    style D fill:#c8e6c9,color:#000\n",
        "    style E fill:#fff9c4,color:#000\n",
        "    style F fill:#ffcc80,color:#000\n",
        "    style G fill:#ffab91,color:#000\n",
        "    style K fill:#a5d6a7,color:#000\n",
        "    style L fill:#c5e1a5,color:#000\n",
        "    style M fill:#fff176,color:#000\n",
        "    style N fill:#ffb74d,color:#000\n",
        "    style O fill:#ff8a65,color:#000\n",
        "```\n",
        "\n",
        "### **7. Quick Reference Guide**\n",
        "- **Purpose**: Fast decision making for common scenarios\n",
        "- **Key Features**:\n",
        "  - Time estimates for each analysis type\n",
        "  - Minimum requirements specification\n",
        "  - Rapid interpretation guidelines\n",
        "  - Action-oriented recommendations\n",
        "\n",
        "\n",
        "## **🎯 How to Use These Flowcharts**\n",
        "\n",
        "### **For Beginners:**\n",
        "1. **Start with Chart 7** (Quick Reference) to identify your primary concern\n",
        "2. **Follow the specific path** to the detailed chart (2-5)\n",
        "3. **Use Chart 6** for comprehensive analysis when needed\n",
        "\n",
        "### **For Advanced Users:**\n",
        "1. **Start with Chart 1** (Master Framework) for complete assessment\n",
        "2. **Drill down** to specific charts (2-5) based on your data characteristics\n",
        "3. **Use Chart 6** for publication-quality analysis\n",
        "\n",
        "### **For Different Scenarios:**\n",
        "\n",
        "**🔍 Exploratory Analysis:**\n",
        "- Use Chart 7 → Chart 2 (Bootstrap focus)\n",
        "- Quick assessment with BSI and CI\n",
        "\n",
        "**📊 Research Publication:**\n",
        "- Use Chart 1 → Chart 6 (Full protocol)\n",
        "- Comprehensive analysis with all metrics\n",
        "\n",
        "**⚡ Quick Validation:**\n",
        "- Use Chart 7 → Specific method chart\n",
        "- Targeted analysis for specific concerns\n",
        "\n",
        "**🚨 Problem Diagnosis:**\n",
        "- Use Chart 5 (Sensitivity Analysis)\n",
        "- Investigate data quality issues\n",
        "\n",
        "### **Key Decision Points in Each Chart:**\n",
        "\n",
        "1. **Sample Size Gates**: n<50, 50-200, >200\n",
        "2. **Quality Thresholds**: BSI/TCI/CVSI >0.95, >0.90, >0.80\n",
        "3. **Balance Criteria**: Minority categories >10%, 5-10%, <5%\n",
        "4. **Missing Data Levels**: <5%, 5-15%, 15-30%, >30%\n",
        "5. **Time Requirements**: 5-10 min (basic) to 45-60 min (comprehensive)\n",
        "\n",
        "These flowcharts provide a complete decision-making framework that can guide anyone from beginner to expert through the complex landscape of categorical data stability and robustness analysis, ensuring appropriate method selection and reliable interpretation of results."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
