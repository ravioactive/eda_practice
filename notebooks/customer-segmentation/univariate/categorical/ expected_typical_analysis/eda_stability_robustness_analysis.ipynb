{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How are these analysis done? Explain test by test. Share not just the code but also the mathematical background, explaining each quantity in the formula. Mention why does it work - what part of this test behaves in what way to detect a certain result. Connect this with interpretability of the test, and explain how to interpret it with the current dataset of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Stability & Robustness Analysis for Categorical Data**\n",
        "\n",
        "## **Comprehensive Mathematical Framework with Mall Customers Dataset**\n",
        "\n",
        "This notebook provides a complete implementation of stability and robustness analysis for categorical data, with detailed mathematical explanations and practical interpretations using the Mall Customers dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency, bootstrap\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/Users/ravisharma/workdir/eda_practice/data/customer-segmentation/Mall_Customers.csv')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Categorical variable (Gender) distribution:\")\n",
        "print(df['Gender'].value_counts())\n",
        "print(f\"Proportions: {df['Gender'].value_counts(normalize=True)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **MATHEMATICAL FOUNDATIONS**\n",
        "\n",
        "### **Core Concepts for Categorical Stability Analysis**\n",
        "\n",
        "**1. Distance Metrics for Categorical Distributions:**\n",
        "- **Total Variation Distance**: TV(P,Q) = ½Σᵢ|pᵢ - qᵢ|\n",
        "- **Hellinger Distance**: H(P,Q) = √(½Σᵢ(√pᵢ - √qᵢ)²)  \n",
        "- **Jensen-Shannon Divergence**: JS(P,Q) = ½[KL(P||M) + KL(Q||M)] where M = ½(P+Q)\n",
        "\n",
        "**2. Stability Indices:**\n",
        "- **Temporal Consistency Index**: TCI = 1 - TV(π̂(t₁), π̂(t₂))\n",
        "- **Bootstrap Stability Index**: BSI = 1 - (IQR_bootstrap / median_bootstrap)\n",
        "- **Cross-Validation Stability Index**: CVSI = 1 - (1/k) Σᵢ TV(π̂ᵢ, π̂_full)\n",
        "\n",
        "**3. Statistical Tests:**\n",
        "- **Chi-Square Homogeneity**: χ² = Σᵢⱼ (Oᵢⱼ - Eᵢⱼ)²/Eᵢⱼ\n",
        "- **Multinomial Confidence Intervals**: Using Wilson score or bootstrap methods\n",
        "- **Trend Tests**: Cochran-Armitage for ordered categories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I'll provide a comprehensive explanation of Stability & Robustness Analysis for categorical data, covering both the mathematical foundations and practical implementation. Let me first examine the current dataset to provide context-specific interpretations.\n",
        "\n",
        "# **Stability & Robustness Analysis for Categorical Data**\n",
        "\n",
        "## **Mathematical Foundations & Conceptual Framework**\n",
        "\n",
        "Stability and robustness analysis for categorical data examines how consistent distributional properties remain under various perturbations. Unlike continuous data, categorical distributions are characterized by probability mass functions, requiring specialized approaches.\n",
        "\n",
        "### **Core Mathematical Concepts**\n",
        "\n",
        "**1. Categorical Distribution Representation**\n",
        "For a categorical variable with k categories, the distribution is characterized by:\n",
        "- **Probability vector**: π = (π₁, π₂, ..., πₖ) where Σπᵢ = 1\n",
        "- **Sample proportions**: p̂ = (p̂₁, p̂₂, ..., p̂ₖ) where p̂ᵢ = nᵢ/n\n",
        "\n",
        "**2. Distance Metrics for Categorical Distributions**\n",
        "- **Total Variation Distance**: TV(P,Q) = ½Σᵢ|pᵢ - qᵢ|\n",
        "- **Hellinger Distance**: H(P,Q) = √(½Σᵢ(√pᵢ - √qᵢ)²)\n",
        "- **Jensen-Shannon Divergence**: JS(P,Q) = ½[KL(P||M) + KL(Q||M)] where M = ½(P+Q)\n",
        "\n",
        "---\n",
        "\n",
        "## **1. TEMPORAL STABILITY ANALYSIS**\n",
        "\n",
        "### **Mathematical Background**\n",
        "\n",
        "**Objective**: Assess whether categorical distributions remain consistent across time periods.\n",
        "\n",
        "**Key Statistic: Temporal Consistency Index (TCI)**\n",
        "```\n",
        "TCI(t₁,t₂) = 1 - TV(π̂(t₁), π̂(t₂))\n",
        "```\n",
        "Where π̂(t) is the estimated probability vector at time t.\n",
        "\n",
        "**Why it works**: \n",
        "- Total Variation Distance measures maximum difference in probability assignment\n",
        "- TCI ∈ [0,1]: TCI = 1 indicates perfect stability, TCI = 0 indicates maximum instability\n",
        "- Sensitive to both systematic shifts and random fluctuations\n",
        "\n",
        "### **Implementation & Interpretation**\n",
        "\n",
        "Now let me continue building the comprehensive notebook with all the analysis methods. Let me add the implementation code:\n",
        "\n",
        "Let me continue with a comprehensive explanation of each analysis method. I'll provide the complete mathematical framework and implementation:\n",
        "\n",
        "## **Comprehensive Explanation of Stability & Robustness Analysis**\n",
        "\n",
        "### **1. TEMPORAL STABILITY ANALYSIS**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "The **Temporal Consistency Index (TCI)** measures how consistent categorical distributions remain across different time periods:\n",
        "\n",
        "```\n",
        "TCI(t₁,t₂) = 1 - TV(π̂(t₁), π̂(t₂))\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `TV(P,Q) = ½Σᵢ|pᵢ - qᵢ|` is the **Total Variation Distance**\n",
        "- `π̂(t)` is the estimated probability vector at time t\n",
        "- TCI ∈ [0,1]: 1 = perfect stability, 0 = maximum instability\n",
        "\n",
        "**Why it works:**\n",
        "- Total Variation Distance measures the **maximum difference** in probability assignment between two distributions\n",
        "- It's **sensitive to both systematic shifts** (trend changes) and **random fluctuations**\n",
        "- The complement (1 - TV) gives us a **stability measure** where higher values indicate more consistent distributions\n",
        "\n",
        "**Statistical Test:** Pearson's Chi-Square for temporal homogeneity tests whether distributions are the same across time periods:\n",
        "```\n",
        "χ² = Σᵢⱼ (Oᵢⱼ - Eᵢⱼ)²/Eᵢⱼ\n",
        "```\n",
        "\n",
        "**Interpretation for Mall Customers Dataset:**\n",
        "- Since the dataset lacks temporal information, we simulate time periods\n",
        "- **High TCI (>0.9)**: Gender distribution is very stable across time periods\n",
        "- **Low TCI (<0.7)**: Significant temporal variation in gender proportions\n",
        "- **Chi-square p-value < 0.05**: Reject temporal homogeneity (distributions differ significantly across time)\n",
        "\n",
        "### **2. SAMPLE STABILITY (Bootstrap Analysis)**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "Bootstrap analysis estimates the **sampling distribution** of categorical proportions by resampling with replacement.\n",
        "\n",
        "**Key Metrics:**\n",
        "\n",
        "1. **Bootstrap Confidence Intervals:**\n",
        "   ```\n",
        "   CI_α = [θ̂*(α/2), θ̂*(1-α/2)]\n",
        "   ```\n",
        "   Where θ̂* are bootstrap estimates of proportions\n",
        "\n",
        "2. **Bootstrap Stability Index (BSI):**\n",
        "   ```\n",
        "   BSI = 1 - (IQR_bootstrap / median_bootstrap)\n",
        "   ```\n",
        "   Measures relative variability of bootstrap estimates\n",
        "\n",
        "3. **Bias Estimation:**\n",
        "   ```\n",
        "   Bias = E[θ̂*] - θ̂_original\n",
        "   ```\n",
        "\n",
        "**Why it works:**\n",
        "- Bootstrap **approximates the true sampling distribution** without knowing the population\n",
        "- **Confidence intervals** show the range of plausible values for proportions\n",
        "- **BSI** quantifies how stable estimates are under sampling variation\n",
        "- **Bias estimation** reveals systematic over/under-estimation\n",
        "\n",
        "**Interpretation for Mall Customers Dataset:**\n",
        "- **Narrow CI**: Proportions are precisely estimated (large sample effect)\n",
        "- **High BSI (>0.9)**: Proportions are stable under resampling\n",
        "- **Low bias (<0.01)**: Estimates are unbiased\n",
        "- **Wide CI for minority categories**: Less precision for smaller groups\n",
        "\n",
        "### **3. SUBSET STABILITY ANALYSIS**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "Assesses whether categorical distributions remain consistent when analyzing different subsets of data.\n",
        "\n",
        "**Cross-Validation Stability Index (CVSI):**\n",
        "```\n",
        "CVSI = 1 - (1/k) Σᵢ TV(π̂ᵢ, π̂_full)\n",
        "```\n",
        "Where π̂ᵢ = proportion vector from fold i, π̂_full = full dataset proportions\n",
        "\n",
        "**Subset Homogeneity Test:**\n",
        "```\n",
        "H₀: π₁ = π₂ = ... = πₖ (all subsets have same distribution)\n",
        "χ² = Σᵢⱼ (Oᵢⱼ - Eᵢⱼ)²/Eᵢⱼ\n",
        "```\n",
        "\n",
        "**Why it works:**\n",
        "- **Cross-validation** reveals how much estimates change when different data portions are used\n",
        "- **Subset size analysis** shows the **minimum sample size** needed for stable estimates\n",
        "- **Homogeneity tests** detect whether subsets come from the same population\n",
        "\n",
        "**Interpretation for Mall Customers Dataset:**\n",
        "- **High CVSI (>0.9)**: Gender distribution is consistent across data subsets\n",
        "- **Decreasing stability with smaller subsets**: Natural effect of reduced sample size\n",
        "- **Chi-square significance**: Subsets have significantly different distributions\n",
        "\n",
        "### **4. SENSITIVITY ANALYSIS**\n",
        "\n",
        "#### **A. Impact of Missing Data Imputation**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "Tests how different missing data handling strategies affect distributional estimates.\n",
        "\n",
        "**Missing Data Mechanisms:**\n",
        "1. **MCAR (Missing Completely At Random)**: P(Missing|X,Y) = P(Missing)\n",
        "2. **MAR (Missing At Random)**: P(Missing|X,Y) = P(Missing|X)  \n",
        "3. **MNAR (Missing Not At Random)**: P(Missing|X,Y) depends on Y\n",
        "\n",
        "**Imputation Sensitivity Index (ISI):**\n",
        "```\n",
        "ISI = 1 - max_methods TV(π̂_method1, π̂_method2)\n",
        "```\n",
        "\n",
        "**Why it works:**\n",
        "- Different imputation methods make different assumptions about missing data\n",
        "- **High sensitivity** indicates results depend heavily on imputation choice\n",
        "- **Low sensitivity** suggests robust findings regardless of imputation method\n",
        "\n",
        "#### **B. Robustness to Outliers**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "For categorical data, \"outliers\" are **rare categories** or **unusual patterns**.\n",
        "\n",
        "**Influence Function for Proportions:**\n",
        "```\n",
        "IF(x; π̂) = (I(x=category) - π̂_category) / n\n",
        "```\n",
        "\n",
        "**Breakdown Point:** Minimum fraction of contamination that can make estimates arbitrarily bad.\n",
        "\n",
        "**Why it works:**\n",
        "- **Influence function** measures how much a single observation affects the estimate\n",
        "- **Breakdown point** for proportions is typically 50% (very robust)\n",
        "- **Rare categories** have higher influence per observation\n",
        "\n",
        "#### **C. Effect of Sample Size on Distribution**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "**Asymptotic Properties:**\n",
        "```\n",
        "√n(π̂ - π) → N(0, Σ)\n",
        "```\n",
        "Where Σ = diag(π) - ππᵀ (multinomial covariance)\n",
        "\n",
        "**Sample Size Adequacy Index (SSAI):**\n",
        "```\n",
        "SSAI = min_categories (n × π̂_category) / 5\n",
        "```\n",
        "Rule of thumb: each category should have ≥5 expected observations\n",
        "\n",
        "**Why it works:**\n",
        "- **Central Limit Theorem** ensures normal approximation for large samples\n",
        "- **Multinomial variance** decreases as O(1/n)\n",
        "- **Small categories** require larger samples for stable estimation\n",
        "\n",
        "**Interpretation for Mall Customers Dataset:**\n",
        "- **n=200, 2 categories**: Adequate sample size (each category has >50 observations)\n",
        "- **Balanced distribution**: Both genders well-represented\n",
        "- **High stability expected**: Large sample + balanced distribution = stable estimates\n",
        "\n",
        "---\n",
        "\n",
        "## **PRACTICAL IMPLEMENTATION GUIDE**\n",
        "\n",
        "Perfect! I've created a comprehensive analysis of **Stability & Robustness Analysis** for categorical data. Let me provide you with a complete summary of what each test does, the mathematical background, and how to interpret the results:\n",
        "\n",
        "## **COMPREHENSIVE SUMMARY: Stability & Robustness Analysis**\n",
        "\n",
        "### **🎯 WHAT THIS ANALYSIS CATEGORY DOES**\n",
        "\n",
        "Stability & Robustness Analysis examines how **consistent and reliable** categorical distributions remain under various conditions and perturbations. It answers critical questions:\n",
        "\n",
        "- **Temporal Stability**: Does the distribution change over time?\n",
        "- **Sample Stability**: How much do estimates vary due to sampling?\n",
        "- **Subset Stability**: Are results consistent across data partitions?\n",
        "- **Sensitivity**: How robust are findings to analytical choices?\n",
        "\n",
        "---\n",
        "\n",
        "### **📊 TEST-BY-TEST BREAKDOWN**\n",
        "\n",
        "#### **1. TEMPORAL STABILITY ANALYSIS**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "TCI(t₁,t₂) = 1 - TV(π̂(t₁), π̂(t₂))\n",
        "TV(P,Q) = ½Σᵢ|pᵢ - qᵢ|\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Systematic trends** in categorical proportions over time\n",
        "- **Cyclical patterns** (seasonal effects)\n",
        "- **Structural breaks** (sudden distribution changes)\n",
        "\n",
        "**Why it works:**\n",
        "- Total Variation Distance measures **maximum probability difference**\n",
        "- TCI ∈ [0,1]: Higher values = more temporal consistency\n",
        "- Chi-square test detects **significant temporal heterogeneity**\n",
        "\n",
        "**Interpretation for Mall Customers:**\n",
        "- **TCI > 0.95**: Gender distribution is highly stable across time periods\n",
        "- **TCI < 0.80**: Significant temporal variation (investigate causes)\n",
        "- **Chi-square p < 0.05**: Reject temporal homogeneity hypothesis\n",
        "\n",
        "#### **2. BOOTSTRAP STABILITY ANALYSIS**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "BSI = 1 - (IQR_bootstrap / median_bootstrap)\n",
        "CI_α = [θ̂*(α/2), θ̂*(1-α/2)]\n",
        "Bias = E[θ̂*] - θ̂_original\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Sampling variability** of proportion estimates\n",
        "- **Bias** in estimation procedures\n",
        "- **Precision** of categorical distribution estimates\n",
        "\n",
        "**Why it works:**\n",
        "- Bootstrap **approximates true sampling distribution**\n",
        "- Resampling reveals **natural uncertainty** in estimates\n",
        "- BSI quantifies **relative stability** under sampling variation\n",
        "\n",
        "**Interpretation for Mall Customers:**\n",
        "- **Narrow CI**: Large sample provides precise estimates\n",
        "- **High BSI (>0.9)**: Proportions are stable under resampling\n",
        "- **Low bias (<0.01)**: Unbiased estimation\n",
        "\n",
        "#### **3. SUBSET STABILITY ANALYSIS**\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "CVSI = 1 - (1/k) Σᵢ TV(π̂ᵢ, π̂_full)\n",
        "H₀: π₁ = π₂ = ... = πₖ (subset homogeneity)\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Consistency** across different data partitions\n",
        "- **Minimum sample size** requirements\n",
        "- **Data quality** issues (non-representative subsets)\n",
        "\n",
        "**Why it works:**\n",
        "- Cross-validation reveals **partition-dependent variation**\n",
        "- Subset size analysis shows **sample size adequacy**\n",
        "- Homogeneity tests detect **systematic subset differences**\n",
        "\n",
        "**Interpretation for Mall Customers:**\n",
        "- **High CVSI (>0.9)**: Results are consistent across data subsets\n",
        "- **Decreasing stability with smaller subsets**: Natural sample size effect\n",
        "- **Significant chi-square**: Subsets may not be representative\n",
        "\n",
        "#### **4. SENSITIVITY ANALYSIS**\n",
        "\n",
        "##### **A. Missing Data Impact**\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "ISI = 1 - max_methods TV(π̂_method1, π̂_method2)\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Robustness** to missing data handling choices\n",
        "- **MCAR vs MAR vs MNAR** mechanism effects\n",
        "- **Imputation method sensitivity**\n",
        "\n",
        "**Why it works:**\n",
        "- Different imputation methods make **different assumptions**\n",
        "- High sensitivity indicates **method-dependent results**\n",
        "- ISI quantifies **robustness** to analytical choices\n",
        "\n",
        "##### **B. Sample Size Effects**\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "SSAI = min_categories (n × π̂_category) / 5\n",
        "√n(π̂ - π) → N(0, Σ)\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Minimum sample size** for stable estimation\n",
        "- **Asymptotic behavior** of proportion estimates\n",
        "- **Category-specific** sample requirements\n",
        "\n",
        "**Why it works:**\n",
        "- **Central Limit Theorem** ensures normal approximation\n",
        "- Rule of thumb: ≥5 observations per category\n",
        "- Variance decreases as **O(1/n)**\n",
        "\n",
        "##### **C. Outlier Robustness**\n",
        "**Mathematical Foundation:**\n",
        "```\n",
        "IF(x; π̂) = (I(x=category) - π̂_category) / n\n",
        "Breakdown Point ≈ 50% for proportions\n",
        "```\n",
        "\n",
        "**What it detects:**\n",
        "- **Influence** of individual observations\n",
        "- **Rare category** effects\n",
        "- **Data contamination** robustness\n",
        "\n",
        "**Why it works:**\n",
        "- Influence function measures **per-observation impact**\n",
        "- Proportions have **high breakdown point** (very robust)\n",
        "- Rare categories have **higher influence per observation**\n",
        "\n",
        "---\n",
        "\n",
        "### **🔍 INTERPRETATION FRAMEWORK**\n",
        "\n",
        "#### **For Mall Customers Dataset (Gender Analysis):**\n",
        "\n",
        "**Expected Results:**\n",
        "- **High temporal stability** (TCI > 0.95): Gender distribution is inherently stable\n",
        "- **Excellent bootstrap stability** (BSI > 0.95): Large sample (n=200) provides precise estimates\n",
        "- **High subset consistency** (CVSI > 0.90): Balanced distribution ensures consistent subsets\n",
        "- **Low sensitivity** to missing data: Robust binary categorical variable\n",
        "- **Adequate sample size**: Both categories well-represented (>50 observations each)\n",
        "\n",
        "**Key Insights:**\n",
        "1. **Binary variables** are generally more stable than multi-category variables\n",
        "2. **Balanced distributions** show higher stability than skewed ones\n",
        "3. **Large samples** (n>100) typically show excellent bootstrap stability\n",
        "4. **Missing data impact** is minimal for well-represented categories\n",
        "\n",
        "#### **Decision Criteria:**\n",
        "\n",
        "| Metric | Excellent | High | Moderate | Low |\n",
        "|--------|-----------|------|----------|-----|\n",
        "| **TCI/CVSI/BSI** | >0.95 | >0.90 | >0.80 | ≤0.80 |\n",
        "| **CI Width** | <0.05 | <0.10 | <0.15 | ≥0.15 |\n",
        "| **Bias** | <0.001 | <0.01 | <0.05 | ≥0.05 |\n",
        "| **Sample Size** | >200 | >100 | >50 | ≤50 |\n",
        "\n",
        "#### **Practical Recommendations:**\n",
        "\n",
        "**✅ High Stability (Overall Score >0.90):**\n",
        "- Proceed with confidence in analytical results\n",
        "- Current sample size is adequate\n",
        "- Results are robust to analytical choices\n",
        "\n",
        "**⚠️ Moderate Stability (0.80-0.90):**\n",
        "- Consider increasing sample size\n",
        "- Use robust statistical methods\n",
        "- Validate with different subsets\n",
        "\n",
        "**🚨 Low Stability (<0.80):**\n",
        "- Collect more data before final analysis\n",
        "- Investigate sources of instability\n",
        "- Use bootstrap confidence intervals\n",
        "- Consider alternative approaches\n",
        "\n",
        "---\n",
        "\n",
        "### **🎯 PRACTICAL VALUE**\n",
        "\n",
        "This analysis framework provides:\n",
        "\n",
        "1. **Confidence Assessment**: How much can you trust your categorical distribution estimates?\n",
        "2. **Sample Size Planning**: What's the minimum sample needed for stable results?\n",
        "3. **Method Validation**: Are your analytical choices affecting the conclusions?\n",
        "4. **Quality Control**: Are there data quality issues affecting stability?\n",
        "5. **Robustness Testing**: How sensitive are results to various perturbations?\n",
        "\n",
        "The comprehensive implementation I've provided gives you a complete toolkit for assessing the reliability and robustness of categorical data analysis, with specific interpretations tailored to your Mall Customers dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_stability_robustness_analysis(df, cat_col, **kwargs):\n",
        "    \"\"\"\n",
        "    Complete stability and robustness analysis for categorical data\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Input dataset\n",
        "    cat_col : str\n",
        "        Categorical column name\n",
        "    **kwargs : dict\n",
        "        Additional parameters for specific analyses\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Complete analysis results\n",
        "    \"\"\"\n",
        "    \n",
        "    results = {\n",
        "        'dataset_info': {\n",
        "            'n_observations': len(df),\n",
        "            'n_categories': df[cat_col].nunique(),\n",
        "            'categories': df[cat_col].unique().tolist(),\n",
        "            'original_proportions': df[cat_col].value_counts(normalize=True).sort_index()\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # 1. Temporal Stability Analysis\n",
        "    results['temporal_stability'] = temporal_stability_analysis(\n",
        "        df, cat_col, \n",
        "        time_periods=kwargs.get('time_periods', 5),\n",
        "        random_state=kwargs.get('random_state', 42)\n",
        "    )\n",
        "    \n",
        "    # 2. Bootstrap Stability Analysis  \n",
        "    results['bootstrap_stability'] = bootstrap_stability_analysis(\n",
        "        df, cat_col,\n",
        "        n_bootstrap=kwargs.get('n_bootstrap', 1000),\n",
        "        confidence_level=kwargs.get('confidence_level', 0.95),\n",
        "        random_state=kwargs.get('random_state', 42)\n",
        "    )\n",
        "    \n",
        "    # 3. Subset Stability Analysis\n",
        "    results['subset_stability'] = subset_stability_analysis(\n",
        "        df, cat_col,\n",
        "        n_folds=kwargs.get('n_folds', 5),\n",
        "        subset_sizes=kwargs.get('subset_sizes', [0.5, 0.7, 0.8]),\n",
        "        random_state=kwargs.get('random_state', 42)\n",
        "    )\n",
        "    \n",
        "    # 4. Sensitivity Analysis\n",
        "    results['sensitivity_analysis'] = sensitivity_analysis(\n",
        "        df, cat_col,\n",
        "        missing_rates=kwargs.get('missing_rates', [0.05, 0.1, 0.2]),\n",
        "        sample_sizes=kwargs.get('sample_sizes', [50, 100, 150]),\n",
        "        random_state=kwargs.get('random_state', 42)\n",
        "    )\n",
        "    \n",
        "    return results\n",
        "\n",
        "def temporal_stability_analysis(df, cat_col, time_periods=5, random_state=42):\n",
        "    \"\"\"Temporal stability analysis implementation\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Create artificial time periods\n",
        "    df_temp = df.copy()\n",
        "    df_temp['time_period'] = np.random.randint(1, time_periods + 1, len(df_temp))\n",
        "    \n",
        "    # Calculate proportions for each time period\n",
        "    time_proportions = []\n",
        "    period_data = []\n",
        "    \n",
        "    for period in range(1, time_periods + 1):\n",
        "        period_df = df_temp[df_temp['time_period'] == period]\n",
        "        if len(period_df) > 0:\n",
        "            proportions = period_df[cat_col].value_counts(normalize=True).sort_index()\n",
        "            time_proportions.append(proportions.values)\n",
        "            period_data.append(period_df[cat_col].value_counts().sort_index())\n",
        "    \n",
        "    time_proportions = np.array(time_proportions)\n",
        "    \n",
        "    # Calculate Temporal Consistency Index (TCI)\n",
        "    def total_variation_distance(p, q):\n",
        "        return 0.5 * np.sum(np.abs(p - q))\n",
        "    \n",
        "    def temporal_consistency_index(p1, p2):\n",
        "        return 1 - total_variation_distance(p1, p2)\n",
        "    \n",
        "    # Pairwise TCI matrix\n",
        "    n_periods = len(time_proportions)\n",
        "    tci_matrix = np.zeros((n_periods, n_periods))\n",
        "    \n",
        "    for i in range(n_periods):\n",
        "        for j in range(n_periods):\n",
        "            tci_matrix[i, j] = temporal_consistency_index(time_proportions[i], time_proportions[j])\n",
        "    \n",
        "    # Average TCI (excluding diagonal)\n",
        "    mask = ~np.eye(n_periods, dtype=bool)\n",
        "    avg_tci = np.mean(tci_matrix[mask])\n",
        "    \n",
        "    # Chi-Square Test for Temporal Homogeneity\n",
        "    contingency_table = np.array([counts.values for counts in period_data])\n",
        "    chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "    \n",
        "    # Coefficient of Variation for each category\n",
        "    cv_by_category = {}\n",
        "    categories = df[cat_col].unique()\n",
        "    \n",
        "    for i, category in enumerate(categories):\n",
        "        proportions_cat = time_proportions[:, i]\n",
        "        cv = np.std(proportions_cat) / np.mean(proportions_cat) if np.mean(proportions_cat) > 0 else np.inf\n",
        "        cv_by_category[category] = cv\n",
        "    \n",
        "    return {\n",
        "        'tci_matrix': tci_matrix,\n",
        "        'average_tci': avg_tci,\n",
        "        'chi2_homogeneity': {\n",
        "            'statistic': chi2_stat,\n",
        "            'p_value': p_value,\n",
        "            'degrees_of_freedom': dof\n",
        "        },\n",
        "        'coefficient_of_variation': cv_by_category,\n",
        "        'time_proportions': time_proportions,\n",
        "        'categories': categories,\n",
        "        'contingency_table': contingency_table\n",
        "    }\n",
        "\n",
        "def bootstrap_stability_analysis(df, cat_col, n_bootstrap=1000, confidence_level=0.95, random_state=42):\n",
        "    \"\"\"Bootstrap stability analysis implementation\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Original proportions\n",
        "    original_props = df[cat_col].value_counts(normalize=True).sort_index()\n",
        "    categories = original_props.index.tolist()\n",
        "    \n",
        "    # Bootstrap sampling\n",
        "    bootstrap_proportions = []\n",
        "    bootstrap_entropies = []\n",
        "    bootstrap_gini = []\n",
        "    \n",
        "    for i in range(n_bootstrap):\n",
        "        # Resample with replacement\n",
        "        bootstrap_sample = resample(df[cat_col].values, n_samples=len(df), random_state=i)\n",
        "        \n",
        "        # Calculate proportions\n",
        "        bootstrap_props = pd.Series(bootstrap_sample).value_counts(normalize=True)\n",
        "        \n",
        "        # Ensure all categories are represented\n",
        "        props_vector = np.array([bootstrap_props.get(cat, 0) for cat in categories])\n",
        "        bootstrap_proportions.append(props_vector)\n",
        "        \n",
        "        # Calculate entropy: H = -Σ p_i * log(p_i)\n",
        "        entropy = -np.sum([p * np.log(p) if p > 0 else 0 for p in props_vector])\n",
        "        bootstrap_entropies.append(entropy)\n",
        "        \n",
        "        # Calculate Gini coefficient: G = 1 - Σ p_i²\n",
        "        gini = 1 - np.sum(props_vector**2)\n",
        "        bootstrap_gini.append(gini)\n",
        "    \n",
        "    bootstrap_proportions = np.array(bootstrap_proportions)\n",
        "    bootstrap_entropies = np.array(bootstrap_entropies)\n",
        "    bootstrap_gini = np.array(bootstrap_gini)\n",
        "    \n",
        "    # Calculate confidence intervals and stability indices\n",
        "    alpha = 1 - confidence_level\n",
        "    lower_percentile = (alpha/2) * 100\n",
        "    upper_percentile = (1 - alpha/2) * 100\n",
        "    \n",
        "    confidence_intervals = {}\n",
        "    bootstrap_stability_indices = {}\n",
        "    bias_estimates = {}\n",
        "    \n",
        "    for i, category in enumerate(categories):\n",
        "        cat_proportions = bootstrap_proportions[:, i]\n",
        "        \n",
        "        # Confidence intervals\n",
        "        ci_lower = np.percentile(cat_proportions, lower_percentile)\n",
        "        ci_upper = np.percentile(cat_proportions, upper_percentile)\n",
        "        confidence_intervals[category] = (ci_lower, ci_upper)\n",
        "        \n",
        "        # Bootstrap Stability Index (BSI)\n",
        "        median_prop = np.median(cat_proportions)\n",
        "        iqr_prop = np.percentile(cat_proportions, 75) - np.percentile(cat_proportions, 25)\n",
        "        bsi = 1 - (iqr_prop / median_prop) if median_prop > 0 else 0\n",
        "        bootstrap_stability_indices[category] = bsi\n",
        "        \n",
        "        # Bias estimation\n",
        "        bias = np.mean(cat_proportions) - original_props[category]\n",
        "        bias_estimates[category] = bias\n",
        "    \n",
        "    return {\n",
        "        'original_proportions': original_props,\n",
        "        'bootstrap_proportions': bootstrap_proportions,\n",
        "        'confidence_intervals': confidence_intervals,\n",
        "        'bootstrap_stability_indices': bootstrap_stability_indices,\n",
        "        'bias_estimates': bias_estimates,\n",
        "        'entropy_analysis': {\n",
        "            'original': -np.sum([p * np.log(p) if p > 0 else 0 for p in original_props.values]),\n",
        "            'bootstrap_values': bootstrap_entropies,\n",
        "            'confidence_interval': (np.percentile(bootstrap_entropies, lower_percentile), \n",
        "                                  np.percentile(bootstrap_entropies, upper_percentile))\n",
        "        },\n",
        "        'categories': categories\n",
        "    }\n",
        "\n",
        "# Run the comprehensive analysis\n",
        "print(\"=== COMPREHENSIVE STABILITY & ROBUSTNESS ANALYSIS ===\")\n",
        "results = comprehensive_stability_robustness_analysis(df, 'Gender')\n",
        "\n",
        "print(f\"\\nDataset Info:\")\n",
        "print(f\"- Observations: {results['dataset_info']['n_observations']}\")\n",
        "print(f\"- Categories: {results['dataset_info']['n_categories']}\")\n",
        "print(f\"- Original proportions: {dict(results['dataset_info']['original_proportions'])}\")\n",
        "\n",
        "print(f\"\\nTemporal Stability:\")\n",
        "print(f\"- Average TCI: {results['temporal_stability']['average_tci']:.4f}\")\n",
        "print(f\"- Interpretation: {'HIGH' if results['temporal_stability']['average_tci'] > 0.9 else 'MODERATE' if results['temporal_stability']['average_tci'] > 0.7 else 'LOW'} temporal stability\")\n",
        "\n",
        "print(f\"\\nBootstrap Stability:\")\n",
        "for cat in results['bootstrap_stability']['categories']:\n",
        "    bsi = results['bootstrap_stability']['bootstrap_stability_indices'][cat]\n",
        "    ci = results['bootstrap_stability']['confidence_intervals'][cat]\n",
        "    print(f\"- {cat}: BSI = {bsi:.4f}, 95% CI = [{ci[0]:.4f}, {ci[1]:.4f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def subset_stability_analysis(df, cat_col, n_folds=5, subset_sizes=[0.5, 0.7, 0.8], random_state=42):\n",
        "    \"\"\"Subset stability analysis implementation\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    # Full dataset proportions\n",
        "    full_props = df[cat_col].value_counts(normalize=True).sort_index()\n",
        "    categories = full_props.index.tolist()\n",
        "    \n",
        "    results = {\n",
        "        'full_proportions': full_props,\n",
        "        'categories': categories,\n",
        "        'cv_analysis': {},\n",
        "        'subset_size_analysis': {}\n",
        "    }\n",
        "    \n",
        "    # Cross-Validation Stability Analysis\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
        "    \n",
        "    fold_proportions = []\n",
        "    fold_tv_distances = []\n",
        "    \n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(df)):\n",
        "        fold_data = df.iloc[train_idx]\n",
        "        fold_props = fold_data[cat_col].value_counts(normalize=True)\n",
        "        \n",
        "        # Ensure all categories are represented\n",
        "        fold_props_vector = np.array([fold_props.get(cat, 0) for cat in categories])\n",
        "        fold_proportions.append(fold_props_vector)\n",
        "        \n",
        "        # Calculate Total Variation Distance from full dataset\n",
        "        tv_distance = 0.5 * np.sum(np.abs(fold_props_vector - full_props.values))\n",
        "        fold_tv_distances.append(tv_distance)\n",
        "    \n",
        "    fold_proportions = np.array(fold_proportions)\n",
        "    \n",
        "    # Cross-Validation Stability Index\n",
        "    cvsi = 1 - np.mean(fold_tv_distances)\n",
        "    \n",
        "    results['cv_analysis'] = {\n",
        "        'cvsi': cvsi,\n",
        "        'fold_proportions': fold_proportions,\n",
        "        'tv_distances': fold_tv_distances\n",
        "    }\n",
        "    \n",
        "    # Subset Size Stability Analysis\n",
        "    for subset_size in subset_sizes:\n",
        "        n_samples = int(len(df) * subset_size)\n",
        "        subset_proportions = []\n",
        "        subset_tv_distances = []\n",
        "        \n",
        "        # Generate multiple random subsets\n",
        "        for i in range(50):  # 50 random subsets for each size\n",
        "            subset_df = df.sample(n=n_samples, random_state=i)\n",
        "            subset_props = subset_df[cat_col].value_counts(normalize=True)\n",
        "            \n",
        "            subset_props_vector = np.array([subset_props.get(cat, 0) for cat in categories])\n",
        "            subset_proportions.append(subset_props_vector)\n",
        "            \n",
        "            tv_distance = 0.5 * np.sum(np.abs(subset_props_vector - full_props.values))\n",
        "            subset_tv_distances.append(tv_distance)\n",
        "        \n",
        "        subset_proportions = np.array(subset_proportions)\n",
        "        \n",
        "        # Subset Stability Index\n",
        "        ssi = 1 - np.mean(subset_tv_distances)\n",
        "        \n",
        "        results['subset_size_analysis'][subset_size] = {\n",
        "            'ssi': ssi,\n",
        "            'subset_proportions': subset_proportions,\n",
        "            'tv_distances': subset_tv_distances,\n",
        "            'mean_tv_distance': np.mean(subset_tv_distances),\n",
        "            'std_tv_distance': np.std(subset_tv_distances)\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "def sensitivity_analysis(df, cat_col, missing_rates=[0.05, 0.1, 0.2], sample_sizes=[50, 100, 150], random_state=42):\n",
        "    \"\"\"Comprehensive sensitivity analysis implementation\"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    \n",
        "    original_props = df[cat_col].value_counts(normalize=True).sort_index()\n",
        "    categories = original_props.index.tolist()\n",
        "    \n",
        "    results = {\n",
        "        'original_proportions': original_props,\n",
        "        'missing_data_sensitivity': {},\n",
        "        'sample_size_sensitivity': {},\n",
        "        'outlier_sensitivity': {}\n",
        "    }\n",
        "    \n",
        "    # 1. Missing Data Sensitivity\n",
        "    for missing_rate in missing_rates:\n",
        "        n_missing = int(len(df) * missing_rate)\n",
        "        \n",
        "        # Different imputation strategies\n",
        "        imputation_results = {}\n",
        "        \n",
        "        # Create missing data (MCAR)\n",
        "        df_missing = df.copy()\n",
        "        missing_indices = np.random.choice(len(df), n_missing, replace=False)\n",
        "        df_missing.loc[missing_indices, cat_col] = np.nan\n",
        "        \n",
        "        # Strategy 1: Drop missing values\n",
        "        df_drop = df_missing.dropna(subset=[cat_col])\n",
        "        if len(df_drop) > 0:\n",
        "            props_drop = df_drop[cat_col].value_counts(normalize=True).sort_index()\n",
        "            props_vector_drop = np.array([props_drop.get(cat, 0) for cat in categories])\n",
        "            tv_drop = 0.5 * np.sum(np.abs(props_vector_drop - original_props.values))\n",
        "            imputation_results['drop_missing'] = {\n",
        "                'proportions': props_vector_drop,\n",
        "                'tv_distance': tv_drop\n",
        "            }\n",
        "        \n",
        "        # Strategy 2: Mode imputation\n",
        "        mode_value = df[cat_col].mode()[0]\n",
        "        df_mode = df_missing.copy()\n",
        "        df_mode[cat_col].fillna(mode_value, inplace=True)\n",
        "        props_mode = df_mode[cat_col].value_counts(normalize=True).sort_index()\n",
        "        props_vector_mode = np.array([props_mode.get(cat, 0) for cat in categories])\n",
        "        tv_mode = 0.5 * np.sum(np.abs(props_vector_mode - original_props.values))\n",
        "        imputation_results['mode_imputation'] = {\n",
        "            'proportions': props_vector_mode,\n",
        "            'tv_distance': tv_mode\n",
        "        }\n",
        "        \n",
        "        # Strategy 3: Random imputation\n",
        "        df_random = df_missing.copy()\n",
        "        missing_mask = df_random[cat_col].isna()\n",
        "        random_values = np.random.choice(categories, sum(missing_mask), \n",
        "                                       p=original_props.values)\n",
        "        df_random.loc[missing_mask, cat_col] = random_values\n",
        "        props_random = df_random[cat_col].value_counts(normalize=True).sort_index()\n",
        "        props_vector_random = np.array([props_random.get(cat, 0) for cat in categories])\n",
        "        tv_random = 0.5 * np.sum(np.abs(props_vector_random - original_props.values))\n",
        "        imputation_results['random_imputation'] = {\n",
        "            'proportions': props_vector_random,\n",
        "            'tv_distance': tv_random\n",
        "        }\n",
        "        \n",
        "        # Calculate Imputation Sensitivity Index\n",
        "        tv_distances = [result['tv_distance'] for result in imputation_results.values()]\n",
        "        isi = 1 - (max(tv_distances) - min(tv_distances))\n",
        "        \n",
        "        results['missing_data_sensitivity'][missing_rate] = {\n",
        "            'imputation_results': imputation_results,\n",
        "            'imputation_sensitivity_index': isi,\n",
        "            'max_tv_difference': max(tv_distances) - min(tv_distances)\n",
        "        }\n",
        "    \n",
        "    # 2. Sample Size Sensitivity\n",
        "    for sample_size in sample_sizes:\n",
        "        if sample_size >= len(df):\n",
        "            continue\n",
        "            \n",
        "        sample_proportions = []\n",
        "        sample_tv_distances = []\n",
        "        \n",
        "        # Generate multiple samples of given size\n",
        "        for i in range(100):\n",
        "            sample_df = df.sample(n=sample_size, random_state=i)\n",
        "            sample_props = sample_df[cat_col].value_counts(normalize=True)\n",
        "            \n",
        "            sample_props_vector = np.array([sample_props.get(cat, 0) for cat in categories])\n",
        "            sample_proportions.append(sample_props_vector)\n",
        "            \n",
        "            tv_distance = 0.5 * np.sum(np.abs(sample_props_vector - original_props.values))\n",
        "            sample_tv_distances.append(tv_distance)\n",
        "        \n",
        "        sample_proportions = np.array(sample_proportions)\n",
        "        \n",
        "        # Sample Size Adequacy Index\n",
        "        min_expected = min([sample_size * original_props[cat] for cat in categories])\n",
        "        ssai = min_expected / 5  # Rule of thumb: ≥5 expected observations per category\n",
        "        \n",
        "        results['sample_size_sensitivity'][sample_size] = {\n",
        "            'sample_proportions': sample_proportions,\n",
        "            'tv_distances': sample_tv_distances,\n",
        "            'mean_tv_distance': np.mean(sample_tv_distances),\n",
        "            'std_tv_distance': np.std(sample_tv_distances),\n",
        "            'sample_size_adequacy_index': ssai,\n",
        "            'cv_by_category': {\n",
        "                cat: np.std(sample_proportions[:, i]) / np.mean(sample_proportions[:, i])\n",
        "                for i, cat in enumerate(categories)\n",
        "                if np.mean(sample_proportions[:, i]) > 0\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Complete the analysis with remaining functions\n",
        "def subset_stability_analysis_complete(df, cat_col, n_folds=5, subset_sizes=[0.5, 0.7, 0.8], random_state=42):\n",
        "    \"\"\"Complete subset stability analysis with additional metrics\"\"\"\n",
        "    base_results = subset_stability_analysis(df, cat_col, n_folds, subset_sizes, random_state)\n",
        "    \n",
        "    # Add chi-square test for fold homogeneity\n",
        "    fold_counts = []\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
        "    \n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(df)):\n",
        "        fold_data = df.iloc[train_idx]\n",
        "        fold_count = fold_data[cat_col].value_counts().sort_index()\n",
        "        fold_counts.append([fold_count.get(cat, 0) for cat in base_results['categories']])\n",
        "    \n",
        "    fold_contingency = np.array(fold_counts)\n",
        "    chi2_folds, p_val_folds, dof_folds, expected_folds = chi2_contingency(fold_contingency)\n",
        "    \n",
        "    base_results['cv_analysis']['chi2_homogeneity'] = {\n",
        "        'statistic': chi2_folds,\n",
        "        'p_value': p_val_folds,\n",
        "        'degrees_of_freedom': dof_folds\n",
        "    }\n",
        "    \n",
        "    return base_results\n",
        "\n",
        "# Update the main analysis function\n",
        "print(\"\\n=== RUNNING COMPLETE ANALYSIS ===\")\n",
        "\n",
        "# Re-run with complete functions\n",
        "results_complete = {\n",
        "    'dataset_info': {\n",
        "        'n_observations': len(df),\n",
        "        'n_categories': df['Gender'].nunique(),\n",
        "        'categories': df['Gender'].unique().tolist(),\n",
        "        'original_proportions': df['Gender'].value_counts(normalize=True).sort_index()\n",
        "    },\n",
        "    'temporal_stability': temporal_stability_analysis(df, 'Gender'),\n",
        "    'bootstrap_stability': bootstrap_stability_analysis(df, 'Gender'),\n",
        "    'subset_stability': subset_stability_analysis_complete(df, 'Gender'),\n",
        "    'sensitivity_analysis': sensitivity_analysis(df, 'Gender')\n",
        "}\n",
        "\n",
        "print(\"Analysis complete! Results stored in 'results_complete' dictionary.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## **COMPREHENSIVE RESULTS INTERPRETATION**\n",
        "\n",
        "def interpret_stability_results(results):\n",
        "    \"\"\"\n",
        "    Provide comprehensive interpretation of stability and robustness analysis results\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPREHENSIVE STABILITY & ROBUSTNESS ANALYSIS INTERPRETATION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Dataset Overview\n",
        "    print(f\"\\n📊 DATASET OVERVIEW:\")\n",
        "    print(f\"   • Sample Size: {results['dataset_info']['n_observations']}\")\n",
        "    print(f\"   • Categories: {results['dataset_info']['n_categories']} ({', '.join(results['dataset_info']['categories'])})\")\n",
        "    print(f\"   • Original Distribution: {dict(results['dataset_info']['original_proportions'])}\")\n",
        "    \n",
        "    # 1. Temporal Stability Interpretation\n",
        "    print(f\"\\n⏰ TEMPORAL STABILITY ANALYSIS:\")\n",
        "    tci = results['temporal_stability']['average_tci']\n",
        "    chi2_p = results['temporal_stability']['chi2_homogeneity']['p_value']\n",
        "    \n",
        "    print(f\"   • Temporal Consistency Index (TCI): {tci:.4f}\")\n",
        "    if tci > 0.95:\n",
        "        print(f\"     → EXCELLENT temporal stability - distributions are highly consistent across time\")\n",
        "    elif tci > 0.90:\n",
        "        print(f\"     → HIGH temporal stability - minor variations across time periods\")\n",
        "    elif tci > 0.80:\n",
        "        print(f\"     → MODERATE temporal stability - some temporal variation present\")\n",
        "    else:\n",
        "        print(f\"     → LOW temporal stability - significant temporal variation detected\")\n",
        "    \n",
        "    print(f\"   • Chi-Square Homogeneity Test: p = {chi2_p:.4f}\")\n",
        "    if chi2_p < 0.001:\n",
        "        print(f\"     → STRONG evidence against temporal homogeneity (p < 0.001)\")\n",
        "    elif chi2_p < 0.01:\n",
        "        print(f\"     → MODERATE evidence against temporal homogeneity (p < 0.01)\")\n",
        "    elif chi2_p < 0.05:\n",
        "        print(f\"     → WEAK evidence against temporal homogeneity (p < 0.05)\")\n",
        "    else:\n",
        "        print(f\"     → NO significant evidence against temporal homogeneity (p ≥ 0.05)\")\n",
        "    \n",
        "    # 2. Bootstrap Stability Interpretation\n",
        "    print(f\"\\n🔄 BOOTSTRAP STABILITY ANALYSIS:\")\n",
        "    for cat in results['bootstrap_stability']['categories']:\n",
        "        bsi = results['bootstrap_stability']['bootstrap_stability_indices'][cat]\n",
        "        ci = results['bootstrap_stability']['confidence_intervals'][cat]\n",
        "        bias = results['bootstrap_stability']['bias_estimates'][cat]\n",
        "        \n",
        "        print(f\"   • {cat}:\")\n",
        "        print(f\"     - Bootstrap Stability Index: {bsi:.4f}\")\n",
        "        if bsi > 0.95:\n",
        "            print(f\"       → EXCELLENT sampling stability\")\n",
        "        elif bsi > 0.90:\n",
        "            print(f\"       → HIGH sampling stability\")\n",
        "        elif bsi > 0.80:\n",
        "            print(f\"       → MODERATE sampling stability\")\n",
        "        else:\n",
        "            print(f\"       → LOW sampling stability\")\n",
        "            \n",
        "        print(f\"     - 95% Confidence Interval: [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
        "        ci_width = ci[1] - ci[0]\n",
        "        if ci_width < 0.05:\n",
        "            print(f\"       → NARROW CI (width: {ci_width:.4f}) - precise estimation\")\n",
        "        elif ci_width < 0.10:\n",
        "            print(f\"       → MODERATE CI (width: {ci_width:.4f}) - reasonable precision\")\n",
        "        else:\n",
        "            print(f\"       → WIDE CI (width: {ci_width:.4f}) - less precise estimation\")\n",
        "            \n",
        "        print(f\"     - Bias Estimate: {bias:.6f}\")\n",
        "        if abs(bias) < 0.001:\n",
        "            print(f\"       → NEGLIGIBLE bias - unbiased estimation\")\n",
        "        elif abs(bias) < 0.01:\n",
        "            print(f\"       → SMALL bias - minimal systematic error\")\n",
        "        else:\n",
        "            print(f\"       → NOTABLE bias - systematic {'over' if bias > 0 else 'under'}-estimation\")\n",
        "    \n",
        "    # 3. Subset Stability Interpretation\n",
        "    print(f\"\\n📂 SUBSET STABILITY ANALYSIS:\")\n",
        "    cvsi = results['subset_stability']['cv_analysis']['cvsi']\n",
        "    chi2_p_folds = results['subset_stability']['cv_analysis']['chi2_homogeneity']['p_value']\n",
        "    \n",
        "    print(f\"   • Cross-Validation Stability Index: {cvsi:.4f}\")\n",
        "    if cvsi > 0.95:\n",
        "        print(f\"     → EXCELLENT subset consistency - very stable across data partitions\")\n",
        "    elif cvsi > 0.90:\n",
        "        print(f\"     → HIGH subset consistency - stable across most partitions\")\n",
        "    elif cvsi > 0.80:\n",
        "        print(f\"     → MODERATE subset consistency - some variation across partitions\")\n",
        "    else:\n",
        "        print(f\"     → LOW subset consistency - significant variation across partitions\")\n",
        "    \n",
        "    print(f\"   • Fold Homogeneity Test: p = {chi2_p_folds:.4f}\")\n",
        "    print(f\"     → {'Significant' if chi2_p_folds < 0.05 else 'Non-significant'} differences between CV folds\")\n",
        "    \n",
        "    print(f\"   • Subset Size Analysis:\")\n",
        "    for size in [0.5, 0.7, 0.8]:\n",
        "        if size in results['subset_stability']['subset_size_analysis']:\n",
        "            ssi = results['subset_stability']['subset_size_analysis'][size]['ssi']\n",
        "            mean_tv = results['subset_stability']['subset_size_analysis'][size]['mean_tv_distance']\n",
        "            print(f\"     - {int(size*100)}% subset: SSI = {ssi:.4f}, Mean TV = {mean_tv:.4f}\")\n",
        "    \n",
        "    # 4. Sensitivity Analysis Interpretation\n",
        "    print(f\"\\n🔍 SENSITIVITY ANALYSIS:\")\n",
        "    \n",
        "    # Missing Data Sensitivity\n",
        "    print(f\"   • Missing Data Impact:\")\n",
        "    for rate in [0.05, 0.1, 0.2]:\n",
        "        if rate in results['sensitivity_analysis']['missing_data_sensitivity']:\n",
        "            isi = results['sensitivity_analysis']['missing_data_sensitivity'][rate]['imputation_sensitivity_index']\n",
        "            max_diff = results['sensitivity_analysis']['missing_data_sensitivity'][rate]['max_tv_difference']\n",
        "            print(f\"     - {int(rate*100)}% missing: ISI = {isi:.4f}, Max difference = {max_diff:.4f}\")\n",
        "            if isi > 0.95:\n",
        "                print(f\"       → ROBUST to imputation method choice\")\n",
        "            elif isi > 0.90:\n",
        "                print(f\"       → MODERATELY robust to imputation method\")\n",
        "            else:\n",
        "                print(f\"       → SENSITIVE to imputation method choice\")\n",
        "    \n",
        "    # Sample Size Sensitivity  \n",
        "    print(f\"   • Sample Size Impact:\")\n",
        "    for size in [50, 100, 150]:\n",
        "        if size in results['sensitivity_analysis']['sample_size_sensitivity']:\n",
        "            mean_tv = results['sensitivity_analysis']['sample_size_sensitivity'][size]['mean_tv_distance']\n",
        "            ssai = results['sensitivity_analysis']['sample_size_sensitivity'][size]['sample_size_adequacy_index']\n",
        "            print(f\"     - n={size}: Mean TV = {mean_tv:.4f}, SSAI = {ssai:.2f}\")\n",
        "            if ssai >= 1.0:\n",
        "                print(f\"       → ADEQUATE sample size for stable estimation\")\n",
        "            elif ssai >= 0.5:\n",
        "                print(f\"       → MARGINAL sample size - some instability expected\")\n",
        "            else:\n",
        "                print(f\"       → INADEQUATE sample size - high instability expected\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(\"OVERALL ASSESSMENT:\")\n",
        "    \n",
        "    # Overall stability score\n",
        "    stability_scores = [tci, cvsi]\n",
        "    if 'bootstrap_stability' in results:\n",
        "        avg_bsi = np.mean([results['bootstrap_stability']['bootstrap_stability_indices'][cat] \n",
        "                          for cat in results['bootstrap_stability']['categories']])\n",
        "        stability_scores.append(avg_bsi)\n",
        "    \n",
        "    overall_stability = np.mean(stability_scores)\n",
        "    \n",
        "    print(f\"   • Overall Stability Score: {overall_stability:.4f}\")\n",
        "    if overall_stability > 0.95:\n",
        "        print(f\"     → EXCELLENT overall stability - highly reliable categorical distribution\")\n",
        "    elif overall_stability > 0.90:\n",
        "        print(f\"     → HIGH overall stability - reliable for most analytical purposes\")\n",
        "    elif overall_stability > 0.80:\n",
        "        print(f\"     → MODERATE overall stability - adequate with some caution\")\n",
        "    else:\n",
        "        print(f\"     → LOW overall stability - results should be interpreted carefully\")\n",
        "    \n",
        "    # Recommendations\n",
        "    print(f\"\\n📋 RECOMMENDATIONS:\")\n",
        "    if overall_stability > 0.90:\n",
        "        print(f\"   ✅ Distribution is highly stable - proceed with confidence\")\n",
        "        print(f\"   ✅ Current sample size is adequate for reliable inference\")\n",
        "        print(f\"   ✅ Results are robust to analytical choices\")\n",
        "    elif overall_stability > 0.80:\n",
        "        print(f\"   ⚠️  Distribution shows moderate stability - consider:\")\n",
        "        print(f\"      • Increasing sample size if possible\")\n",
        "        print(f\"      • Using robust statistical methods\")\n",
        "        print(f\"      • Validating results with different subsets\")\n",
        "    else:\n",
        "        print(f\"   🚨 Distribution shows low stability - recommend:\")\n",
        "        print(f\"      • Collecting more data before final analysis\")\n",
        "        print(f\"      • Investigating sources of instability\")\n",
        "        print(f\"      • Using bootstrap confidence intervals\")\n",
        "        print(f\"      • Considering alternative analytical approaches\")\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "\n",
        "# Run the interpretation\n",
        "interpret_stability_results(results_complete)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comprehensive Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## **COMPREHENSIVE VISUALIZATIONS**\n",
        "\n",
        "def create_comprehensive_visualizations(results):\n",
        "    \"\"\"\n",
        "    Create comprehensive visualizations for all stability and robustness analyses\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set up the figure with subplots\n",
        "    fig = plt.figure(figsize=(20, 24))\n",
        "    gs = fig.add_gridspec(6, 4, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # Color palette\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, 12))\n",
        "    \n",
        "    # 1. Dataset Overview (Top row)\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    original_props = results['dataset_info']['original_proportions']\n",
        "    categories = results['dataset_info']['categories']\n",
        "    \n",
        "    bars = ax1.bar(categories, original_props.values, color=colors[:len(categories)], alpha=0.8)\n",
        "    ax1.set_title('Original Distribution', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Proportion')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, original_props.values):\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 2. Temporal Consistency Index Matrix\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    tci_matrix = results['temporal_stability']['tci_matrix']\n",
        "    im = ax2.imshow(tci_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
        "    ax2.set_title('Temporal Consistency Matrix', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Time Period')\n",
        "    ax2.set_ylabel('Time Period')\n",
        "    \n",
        "    # Add text annotations\n",
        "    for i in range(tci_matrix.shape[0]):\n",
        "        for j in range(tci_matrix.shape[1]):\n",
        "            ax2.text(j, i, f'{tci_matrix[i, j]:.3f}', ha='center', va='center',\n",
        "                    color='white' if tci_matrix[i, j] < 0.5 else 'black', fontweight='bold')\n",
        "    \n",
        "    plt.colorbar(im, ax=ax2, fraction=0.046, pad=0.04)\n",
        "    \n",
        "    # 3. Bootstrap Distributions\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    bootstrap_props = results['bootstrap_stability']['bootstrap_proportions']\n",
        "    \n",
        "    for i, cat in enumerate(categories):\n",
        "        ax3.hist(bootstrap_props[:, i], bins=30, alpha=0.6, label=cat, \n",
        "                color=colors[i], density=True)\n",
        "        # Add original proportion line\n",
        "        ax3.axvline(original_props[cat], color=colors[i], linestyle='--', \n",
        "                   linewidth=2, alpha=0.8)\n",
        "    \n",
        "    ax3.set_title('Bootstrap Distributions', fontsize=14, fontweight='bold')\n",
        "    ax3.set_xlabel('Proportion')\n",
        "    ax3.set_ylabel('Density')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Stability Indices Comparison\n",
        "    ax4 = fig.add_subplot(gs[0, 3])\n",
        "    \n",
        "    # Collect all stability indices\n",
        "    tci = results['temporal_stability']['average_tci']\n",
        "    cvsi = results['subset_stability']['cv_analysis']['cvsi']\n",
        "    avg_bsi = np.mean([results['bootstrap_stability']['bootstrap_stability_indices'][cat] \n",
        "                      for cat in categories])\n",
        "    \n",
        "    stability_metrics = ['TCI', 'CVSI', 'Avg BSI']\n",
        "    stability_values = [tci, cvsi, avg_bsi]\n",
        "    \n",
        "    bars = ax4.bar(stability_metrics, stability_values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "    ax4.set_title('Stability Indices Comparison', fontsize=14, fontweight='bold')\n",
        "    ax4.set_ylabel('Stability Index')\n",
        "    ax4.set_ylim(0, 1)\n",
        "    ax4.axhline(y=0.9, color='green', linestyle='--', alpha=0.7, label='High stability')\n",
        "    ax4.axhline(y=0.8, color='orange', linestyle='--', alpha=0.7, label='Moderate stability')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, value in zip(bars, stability_values):\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # 5. Temporal Evolution (Second row)\n",
        "    ax5 = fig.add_subplot(gs[1, :2])\n",
        "    time_proportions = results['temporal_stability']['time_proportions']\n",
        "    time_points = range(1, len(time_proportions) + 1)\n",
        "    \n",
        "    for i, cat in enumerate(categories):\n",
        "        ax5.plot(time_points, time_proportions[:, i], marker='o', linewidth=3,\n",
        "                markersize=8, label=cat, color=colors[i])\n",
        "        # Add confidence bands (simulated)\n",
        "        std_dev = np.std(time_proportions[:, i])\n",
        "        ax5.fill_between(time_points, \n",
        "                        time_proportions[:, i] - std_dev,\n",
        "                        time_proportions[:, i] + std_dev,\n",
        "                        alpha=0.2, color=colors[i])\n",
        "    \n",
        "    ax5.set_title('Temporal Evolution of Proportions', fontsize=14, fontweight='bold')\n",
        "    ax5.set_xlabel('Time Period')\n",
        "    ax5.set_ylabel('Proportion')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Cross-Validation Fold Analysis\n",
        "    ax6 = fig.add_subplot(gs[1, 2:])\n",
        "    fold_props = results['subset_stability']['cv_analysis']['fold_proportions']\n",
        "    tv_distances = results['subset_stability']['cv_analysis']['tv_distances']\n",
        "    \n",
        "    x_pos = np.arange(len(categories))\n",
        "    width = 0.15\n",
        "    \n",
        "    for i in range(len(fold_props)):\n",
        "        ax6.bar(x_pos + i*width, fold_props[i], width, alpha=0.7, \n",
        "               label=f'Fold {i+1}', color=colors[i])\n",
        "    \n",
        "    # Add full dataset proportions\n",
        "    ax6.bar(x_pos + len(fold_props)*width, original_props.values, width, \n",
        "           color='red', alpha=0.8, label='Full Dataset')\n",
        "    \n",
        "    ax6.set_title('Cross-Validation Fold Proportions', fontsize=14, fontweight='bold')\n",
        "    ax6.set_xlabel('Category')\n",
        "    ax6.set_ylabel('Proportion')\n",
        "    ax6.set_xticks(x_pos + width * len(fold_props)/2)\n",
        "    ax6.set_xticklabels(categories)\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 7. Subset Size Analysis (Third row)\n",
        "    ax7 = fig.add_subplot(gs[2, :2])\n",
        "    subset_sizes = [0.5, 0.7, 0.8]\n",
        "    ssi_values = []\n",
        "    mean_tv_values = []\n",
        "    \n",
        "    for size in subset_sizes:\n",
        "        if size in results['subset_stability']['subset_size_analysis']:\n",
        "            ssi_values.append(results['subset_stability']['subset_size_analysis'][size]['ssi'])\n",
        "            mean_tv_values.append(results['subset_stability']['subset_size_analysis'][size]['mean_tv_distance'])\n",
        "    \n",
        "    ax7_twin = ax7.twinx()\n",
        "    \n",
        "    line1 = ax7.plot(subset_sizes, ssi_values, 'bo-', linewidth=3, markersize=10, \n",
        "                     label='Subset Stability Index')\n",
        "    line2 = ax7_twin.plot(subset_sizes, mean_tv_values, 'ro-', linewidth=3, markersize=10,\n",
        "                         label='Mean TV Distance')\n",
        "    \n",
        "    ax7.set_xlabel('Subset Size')\n",
        "    ax7.set_ylabel('Subset Stability Index', color='blue')\n",
        "    ax7_twin.set_ylabel('Mean TV Distance', color='red')\n",
        "    ax7.set_title('Stability vs Subset Size', fontsize=14, fontweight='bold')\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Combine legends\n",
        "    lines = line1 + line2\n",
        "    labels = [l.get_label() for l in lines]\n",
        "    ax7.legend(lines, labels, loc='center right')\n",
        "    \n",
        "    # 8. Missing Data Sensitivity\n",
        "    ax8 = fig.add_subplot(gs[2, 2:])\n",
        "    missing_rates = [0.05, 0.1, 0.2]\n",
        "    \n",
        "    imputation_methods = ['drop_missing', 'mode_imputation', 'random_imputation']\n",
        "    method_colors = ['red', 'blue', 'green']\n",
        "    \n",
        "    x_pos = np.arange(len(missing_rates))\n",
        "    width = 0.25\n",
        "    \n",
        "    for i, method in enumerate(imputation_methods):\n",
        "        tv_values = []\n",
        "        for rate in missing_rates:\n",
        "            if rate in results['sensitivity_analysis']['missing_data_sensitivity']:\n",
        "                imputation_results = results['sensitivity_analysis']['missing_data_sensitivity'][rate]['imputation_results']\n",
        "                if method in imputation_results:\n",
        "                    tv_values.append(imputation_results[method]['tv_distance'])\n",
        "                else:\n",
        "                    tv_values.append(0)\n",
        "            else:\n",
        "                tv_values.append(0)\n",
        "        \n",
        "        ax8.bar(x_pos + i*width, tv_values, width, alpha=0.7, \n",
        "               label=method.replace('_', ' ').title(), color=method_colors[i])\n",
        "    \n",
        "    ax8.set_title('Missing Data Sensitivity', fontsize=14, fontweight='bold')\n",
        "    ax8.set_xlabel('Missing Rate')\n",
        "    ax8.set_ylabel('TV Distance from Original')\n",
        "    ax8.set_xticks(x_pos + width)\n",
        "    ax8.set_xticklabels([f'{int(rate*100)}%' for rate in missing_rates])\n",
        "    ax8.legend()\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 9. Sample Size Sensitivity (Fourth row)\n",
        "    ax9 = fig.add_subplot(gs[3, :2])\n",
        "    sample_sizes = [50, 100, 150]\n",
        "    \n",
        "    for size in sample_sizes:\n",
        "        if size in results['sensitivity_analysis']['sample_size_sensitivity']:\n",
        "            tv_distances = results['sensitivity_analysis']['sample_size_sensitivity'][size]['tv_distances']\n",
        "            ax9.hist(tv_distances, bins=20, alpha=0.6, label=f'n={size}', density=True)\n",
        "    \n",
        "    ax9.set_title('Sample Size Sensitivity - TV Distance Distributions', fontsize=14, fontweight='bold')\n",
        "    ax9.set_xlabel('TV Distance from Full Dataset')\n",
        "    ax9.set_ylabel('Density')\n",
        "    ax9.legend()\n",
        "    ax9.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 10. Bootstrap Confidence Intervals\n",
        "    ax10 = fig.add_subplot(gs[3, 2:])\n",
        "    \n",
        "    for i, cat in enumerate(categories):\n",
        "        ci = results['bootstrap_stability']['confidence_intervals'][cat]\n",
        "        original_prop = original_props[cat]\n",
        "        \n",
        "        # Plot confidence interval as error bar\n",
        "        ax10.errorbar(i, original_prop, yerr=[[original_prop - ci[0]], [ci[1] - original_prop]], \n",
        "                     fmt='o', markersize=10, capsize=10, capthick=3, \n",
        "                     color=colors[i], label=cat)\n",
        "        \n",
        "        # Add CI width annotation\n",
        "        ci_width = ci[1] - ci[0]\n",
        "        ax10.text(i, ci[1] + 0.02, f'Width: {ci_width:.3f}', \n",
        "                 ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    ax10.set_title('Bootstrap 95% Confidence Intervals', fontsize=14, fontweight='bold')\n",
        "    ax10.set_xlabel('Category')\n",
        "    ax10.set_ylabel('Proportion')\n",
        "    ax10.set_xticks(range(len(categories)))\n",
        "    ax10.set_xticklabels(categories)\n",
        "    ax10.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 11. Comprehensive Stability Heatmap (Fifth row)\n",
        "    ax11 = fig.add_subplot(gs[4, :2])\n",
        "    \n",
        "    # Create stability matrix\n",
        "    stability_data = []\n",
        "    row_labels = []\n",
        "    \n",
        "    # Temporal stability by category\n",
        "    for cat in categories:\n",
        "        cv = results['temporal_stability']['coefficient_of_variation'][cat]\n",
        "        stability_data.append([1 - cv if cv != np.inf else 0])  # Convert CV to stability\n",
        "        row_labels.append(f'Temporal-{cat}')\n",
        "    \n",
        "    # Bootstrap stability by category\n",
        "    for cat in categories:\n",
        "        bsi = results['bootstrap_stability']['bootstrap_stability_indices'][cat]\n",
        "        stability_data.append([bsi])\n",
        "        row_labels.append(f'Bootstrap-{cat}')\n",
        "    \n",
        "    # Subset stability\n",
        "    for size in subset_sizes:\n",
        "        if size in results['subset_stability']['subset_size_analysis']:\n",
        "            ssi = results['subset_stability']['subset_size_analysis'][size]['ssi']\n",
        "            stability_data.append([ssi])\n",
        "            row_labels.append(f'Subset-{int(size*100)}%')\n",
        "    \n",
        "    stability_matrix = np.array(stability_data)\n",
        "    \n",
        "    im = ax11.imshow(stability_matrix, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
        "    ax11.set_title('Comprehensive Stability Heatmap', fontsize=14, fontweight='bold')\n",
        "    ax11.set_yticks(range(len(row_labels)))\n",
        "    ax11.set_yticklabels(row_labels)\n",
        "    ax11.set_xticks([0])\n",
        "    ax11.set_xticklabels(['Stability Index'])\n",
        "    \n",
        "    # Add text annotations\n",
        "    for i in range(len(row_labels)):\n",
        "        ax11.text(0, i, f'{stability_matrix[i, 0]:.3f}', ha='center', va='center',\n",
        "                 color='white' if stability_matrix[i, 0] < 0.5 else 'black', fontweight='bold')\n",
        "    \n",
        "    plt.colorbar(im, ax=ax11, fraction=0.046, pad=0.04)\n",
        "    \n",
        "    # 12. Overall Assessment Radar Chart\n",
        "    ax12 = fig.add_subplot(gs[4, 2:], projection='polar')\n",
        "    \n",
        "    # Metrics for radar chart\n",
        "    metrics = ['Temporal\\nStability', 'Bootstrap\\nStability', 'Subset\\nStability', \n",
        "              'Missing Data\\nRobustness', 'Sample Size\\nAdequacy']\n",
        "    \n",
        "    values = [\n",
        "        tci,\n",
        "        avg_bsi,\n",
        "        cvsi,\n",
        "        np.mean([results['sensitivity_analysis']['missing_data_sensitivity'][rate]['imputation_sensitivity_index'] \n",
        "                for rate in [0.05, 0.1, 0.2] \n",
        "                if rate in results['sensitivity_analysis']['missing_data_sensitivity']]),\n",
        "        np.mean([results['sensitivity_analysis']['sample_size_sensitivity'][size]['sample_size_adequacy_index'] \n",
        "                for size in [50, 100, 150] \n",
        "                if size in results['sensitivity_analysis']['sample_size_sensitivity'] \n",
        "                and results['sensitivity_analysis']['sample_size_sensitivity'][size]['sample_size_adequacy_index'] <= 2])\n",
        "    ]\n",
        "    \n",
        "    # Normalize the last value to [0,1] range\n",
        "    values[-1] = min(values[-1], 1.0)\n",
        "    \n",
        "    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
        "    values += values[:1]  # Complete the circle\n",
        "    angles += angles[:1]\n",
        "    \n",
        "    ax12.plot(angles, values, 'o-', linewidth=3, markersize=8, color='blue', alpha=0.7)\n",
        "    ax12.fill(angles, values, alpha=0.25, color='blue')\n",
        "    ax12.set_xticks(angles[:-1])\n",
        "    ax12.set_xticklabels(metrics)\n",
        "    ax12.set_ylim(0, 1)\n",
        "    ax12.set_title('Overall Stability Assessment', fontsize=14, fontweight='bold', pad=20)\n",
        "    ax12.grid(True)\n",
        "    \n",
        "    # Add reference circles\n",
        "    ax12.axhline(y=0.9, color='green', linestyle='--', alpha=0.7)\n",
        "    ax12.axhline(y=0.8, color='orange', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # 13. Summary Statistics Table (Bottom row)\n",
        "    ax13 = fig.add_subplot(gs[5, :])\n",
        "    ax13.axis('off')\n",
        "    \n",
        "    # Create summary table\n",
        "    summary_data = [\n",
        "        ['Metric', 'Value', 'Interpretation'],\n",
        "        ['Overall Stability Score', f'{np.mean([tci, cvsi, avg_bsi]):.4f}', \n",
        "         'EXCELLENT' if np.mean([tci, cvsi, avg_bsi]) > 0.95 else \n",
        "         'HIGH' if np.mean([tci, cvsi, avg_bsi]) > 0.90 else \n",
        "         'MODERATE' if np.mean([tci, cvsi, avg_bsi]) > 0.80 else 'LOW'],\n",
        "        ['Temporal Consistency', f'{tci:.4f}', \n",
        "         'EXCELLENT' if tci > 0.95 else 'HIGH' if tci > 0.90 else 'MODERATE' if tci > 0.80 else 'LOW'],\n",
        "        ['Bootstrap Stability', f'{avg_bsi:.4f}', \n",
        "         'EXCELLENT' if avg_bsi > 0.95 else 'HIGH' if avg_bsi > 0.90 else 'MODERATE' if avg_bsi > 0.80 else 'LOW'],\n",
        "        ['Subset Consistency', f'{cvsi:.4f}', \n",
        "         'EXCELLENT' if cvsi > 0.95 else 'HIGH' if cvsi > 0.90 else 'MODERATE' if cvsi > 0.80 else 'LOW'],\n",
        "        ['Sample Size', f'{len(results[\"dataset_info\"][\"original_proportions\"])} obs, {results[\"dataset_info\"][\"n_categories\"]} categories', \n",
        "         'ADEQUATE' if len(results[\"dataset_info\"][\"original_proportions\"]) > 100 else 'MARGINAL'],\n",
        "    ]\n",
        "    \n",
        "    table = ax13.table(cellText=summary_data[1:], colLabels=summary_data[0],\n",
        "                      cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(12)\n",
        "    table.scale(1, 2)\n",
        "    \n",
        "    # Color code the interpretation column\n",
        "    for i in range(1, len(summary_data)):\n",
        "        interpretation = summary_data[i][2]\n",
        "        if 'EXCELLENT' in interpretation:\n",
        "            table[(i, 2)].set_facecolor('#90EE90')  # Light green\n",
        "        elif 'HIGH' in interpretation:\n",
        "            table[(i, 2)].set_facecolor('#87CEEB')  # Sky blue\n",
        "        elif 'MODERATE' in interpretation:\n",
        "            table[(i, 2)].set_facecolor('#FFE4B5')  # Moccasin\n",
        "        elif 'LOW' in interpretation:\n",
        "            table[(i, 2)].set_facecolor('#FFB6C1')  # Light pink\n",
        "        else:\n",
        "            table[(i, 2)].set_facecolor('#F0F0F0')  # Light gray\n",
        "    \n",
        "    plt.suptitle('Comprehensive Stability & Robustness Analysis Dashboard', \n",
        "                fontsize=20, fontweight='bold', y=0.98)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create the comprehensive visualizations\n",
        "create_comprehensive_visualizations(results_complete)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
