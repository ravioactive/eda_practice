{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a938c47c",
   "metadata": {},
   "source": [
    "I'll provide a comprehensive explanation of Fourier analysis for categorical time series, covering the mathematical foundations, practical implementation, and interpretation strategies. Let me examine your current notebook to understand the context better.\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "Perfect! I can see the Fourier analysis method in your notebook. Let me provide a comprehensive explanation of how Fourier analysis works for categorical time series analysis.\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **Fourier Analysis for Categorical Time Series: Complete Mathematical Framework**\n",
    "\n",
    "### **1. MATHEMATICAL FOUNDATIONS**\n",
    "\n",
    "#### **A. Core Concept: From Categorical to Continuous**\n",
    "\n",
    "Fourier analysis requires **continuous numerical data**, but categorical data is discrete. The key transformation:\n",
    "\n",
    "**Categorical → Proportions → Time Series → Frequency Domain**\n",
    "\n",
    "```python\n",
    "# Step 1: Categorical data over time\n",
    "# Gender: [M, F, M, M, F, F, M, F, ...]\n",
    "# Time:   [1, 2, 3, 4, 5, 6, 7, 8, ...]\n",
    "\n",
    "# Step 2: Convert to proportions per time period\n",
    "# Time Period 1: P(Male)=0.6, P(Female)=0.4\n",
    "# Time Period 2: P(Male)=0.3, P(Female)=0.7\n",
    "# Time Period 3: P(Male)=0.8, P(Female)=0.2\n",
    "```\n",
    "\n",
    "#### **B. Discrete Fourier Transform (DFT) Mathematics**\n",
    "\n",
    "For a time series of proportions **x[n]** with **N** time points:\n",
    "\n",
    "**X[k] = Σ(n=0 to N-1) x[n] × e^(-i2πkn/N)**\n",
    "\n",
    "Where:\n",
    "- **X[k]** = Complex frequency component at frequency k\n",
    "- **x[n]** = Proportion value at time n\n",
    "- **k** = Frequency index (0 to N-1)\n",
    "- **i** = Imaginary unit (√-1)\n",
    "\n",
    "#### **C. Key Frequency Domain Properties**\n",
    "\n",
    "1. **Magnitude**: **|X[k]| = √(Real[X[k]]² + Imag[X[k]]²)**\n",
    "2. **Phase**: **φ[k] = arctan(Imag[X[k]] / Real[X[k]])**\n",
    "3. **Power**: **P[k] = |X[k]|²**\n",
    "4. **Frequency**: **f[k] = k / N** (normalized frequency)\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "### **2. ENHANCED IMPLEMENTATION WITH DETAILED ANALYSIS**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import seaborn as sns\n",
    "\n",
    "def comprehensive_fourier_categorical_analysis(df, cat_col, time_col, sampling_period='D'):\n",
    "    \"\"\"\n",
    "    Complete Fourier analysis implementation for categorical time series\n",
    "    \"\"\"\n",
    "    \n",
    "    # STEP 1: Data Preparation and Validation\n",
    "    print(\"=== STEP 1: DATA PREPARATION ===\")\n",
    "    \n",
    "    # Ensure time column is datetime\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    df = df.sort_values(time_col)\n",
    "    \n",
    "    # Create regular time grid (handle missing periods)\n",
    "    full_time_range = pd.date_range(\n",
    "        start=df[time_col].min(), \n",
    "        end=df[time_col].max(), \n",
    "        freq=sampling_period\n",
    "    )\n",
    "    \n",
    "    print(f\"Original data points: {len(df)}\")\n",
    "    print(f\"Full time range: {len(full_time_range)} periods\")\n",
    "    print(f\"Categories: {df[cat_col].unique()}\")\n",
    "    \n",
    "    # STEP 2: Convert to Proportion Time Series\n",
    "    print(\"\\n=== STEP 2: PROPORTION TIME SERIES ===\")\n",
    "    \n",
    "    # Group by time periods and calculate proportions\n",
    "    time_grouped = df.groupby(df[time_col].dt.floor(sampling_period))\n",
    "    proportion_series = time_grouped[cat_col].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    \n",
    "    # Reindex to full time range (fill missing with interpolation)\n",
    "    proportion_series = proportion_series.reindex(full_time_range).interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    print(f\"Proportion series shape: {proportion_series.shape}\")\n",
    "    print(\"Sample proportions:\")\n",
    "    print(proportion_series.head())\n",
    "    \n",
    "    # STEP 3: Fourier Transform for Each Category\n",
    "    print(\"\\n=== STEP 3: FOURIER TRANSFORM ===\")\n",
    "    \n",
    "    results = {}\n",
    "    N = len(proportion_series)\n",
    "    \n",
    "    # Calculate sampling frequency (for proper frequency scaling)\n",
    "    if sampling_period == 'D':\n",
    "        fs = 1.0  # 1 sample per day\n",
    "        freq_unit = 'cycles/day'\n",
    "    elif sampling_period == 'H':\n",
    "        fs = 24.0  # 24 samples per day\n",
    "        freq_unit = 'cycles/hour'\n",
    "    elif sampling_period == 'M':\n",
    "        fs = 1/30.0  # 1 sample per month (approx)\n",
    "        freq_unit = 'cycles/month'\n",
    "    else:\n",
    "        fs = 1.0\n",
    "        freq_unit = 'cycles/period'\n",
    "    \n",
    "    for category in proportion_series.columns:\n",
    "        print(f\"\\nAnalyzing category: {category}\")\n",
    "        \n",
    "        # Get time series for this category\n",
    "        y = proportion_series[category].values\n",
    "        \n",
    "        # Remove DC component (mean) for better analysis\n",
    "        y_detrended = y - np.mean(y)\n",
    "        \n",
    "        # Apply window function to reduce spectral leakage\n",
    "        window = signal.windows.hann(N)\n",
    "        y_windowed = y_detrended * window\n",
    "        \n",
    "        # Compute FFT\n",
    "        Y = fft(y_windowed)\n",
    "        frequencies = fftfreq(N, d=1/fs)\n",
    "        \n",
    "        # Only keep positive frequencies (due to symmetry)\n",
    "        positive_freq_idx = frequencies > 0\n",
    "        frequencies_pos = frequencies[positive_freq_idx]\n",
    "        Y_pos = Y[positive_freq_idx]\n",
    "        \n",
    "        # Calculate magnitude, phase, and power\n",
    "        magnitude = np.abs(Y_pos)\n",
    "        phase = np.angle(Y_pos)\n",
    "        power = magnitude ** 2\n",
    "        \n",
    "        # Normalize power spectrum\n",
    "        power_normalized = power / np.sum(power)\n",
    "        \n",
    "        # Find dominant frequencies\n",
    "        peak_indices = signal.find_peaks(magnitude, height=np.max(magnitude)*0.1)[0]\n",
    "        dominant_frequencies = frequencies_pos[peak_indices]\n",
    "        dominant_magnitudes = magnitude[peak_indices]\n",
    "        \n",
    "        # Convert frequencies to periods (more intuitive)\n",
    "        dominant_periods = 1 / dominant_frequencies[dominant_frequencies > 0]\n",
    "        \n",
    "        results[category] = {\n",
    "            'time_series': y,\n",
    "            'detrended_series': y_detrended,\n",
    "            'frequencies': frequencies_pos,\n",
    "            'magnitude': magnitude,\n",
    "            'phase': phase,\n",
    "            'power': power,\n",
    "            'power_normalized': power_normalized,\n",
    "            'dominant_frequencies': dominant_frequencies,\n",
    "            'dominant_periods': dominant_periods,\n",
    "            'dominant_magnitudes': dominant_magnitudes,\n",
    "            'peak_indices': peak_indices,\n",
    "            'total_variance': np.var(y),\n",
    "            'explained_variance_ratio': np.sum(power_normalized[peak_indices])\n",
    "        }\n",
    "        \n",
    "        print(f\"  Dominant periods: {dominant_periods}\")\n",
    "        print(f\"  Explained variance: {results[category]['explained_variance_ratio']:.3f}\")\n",
    "    \n",
    "    return results, proportion_series\n",
    "\n",
    "def interpret_fourier_results(results, sampling_period='D'):\n",
    "    \"\"\"\n",
    "    Comprehensive interpretation of Fourier analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== FOURIER ANALYSIS INTERPRETATION ===\")\n",
    "    \n",
    "    interpretations = {}\n",
    "    \n",
    "    for category, data in results.items():\n",
    "        print(f\"\\n--- CATEGORY: {category} ---\")\n",
    "        \n",
    "        # Extract key metrics\n",
    "        dominant_periods = data['dominant_periods']\n",
    "        dominant_magnitudes = data['dominant_magnitudes']\n",
    "        explained_variance = data['explained_variance_ratio']\n",
    "        \n",
    "        # Interpretation framework\n",
    "        interpretation = {\n",
    "            'seasonality_strength': 'weak',\n",
    "            'primary_cycles': [],\n",
    "            'seasonal_patterns': [],\n",
    "            'trend_component': 'stable',\n",
    "            'noise_level': 'low'\n",
    "        }\n",
    "        \n",
    "        # 1. SEASONALITY STRENGTH ASSESSMENT\n",
    "        if explained_variance > 0.7:\n",
    "            interpretation['seasonality_strength'] = 'very_strong'\n",
    "        elif explained_variance > 0.5:\n",
    "            interpretation['seasonality_strength'] = 'strong'\n",
    "        elif explained_variance > 0.3:\n",
    "            interpretation['seasonality_strength'] = 'moderate'\n",
    "        elif explained_variance > 0.1:\n",
    "            interpretation['seasonality_strength'] = 'weak'\n",
    "        else:\n",
    "            interpretation['seasonality_strength'] = 'very_weak'\n",
    "        \n",
    "        print(f\"Seasonality Strength: {interpretation['seasonality_strength']}\")\n",
    "        print(f\"Explained Variance: {explained_variance:.3f}\")\n",
    "        \n",
    "        # 2. CYCLE IDENTIFICATION\n",
    "        if len(dominant_periods) > 0:\n",
    "            # Sort by magnitude (strongest cycles first)\n",
    "            sorted_indices = np.argsort(dominant_magnitudes)[::-1]\n",
    "            \n",
    "            for i, idx in enumerate(sorted_indices[:3]):  # Top 3 cycles\n",
    "                period = dominant_periods[idx]\n",
    "                magnitude = dominant_magnitudes[idx]\n",
    "                \n",
    "                # Interpret period length\n",
    "                if sampling_period == 'D':\n",
    "                    if 6 <= period <= 8:\n",
    "                        cycle_type = 'weekly'\n",
    "                    elif 28 <= period <= 32:\n",
    "                        cycle_type = 'monthly'\n",
    "                    elif 85 <= period <= 95:\n",
    "                        cycle_type = 'quarterly'\n",
    "                    elif 360 <= period <= 370:\n",
    "                        cycle_type = 'annual'\n",
    "                    elif period < 6:\n",
    "                        cycle_type = 'short_term'\n",
    "                    else:\n",
    "                        cycle_type = f'{period:.1f}_day'\n",
    "                elif sampling_period == 'H':\n",
    "                    if 23 <= period <= 25:\n",
    "                        cycle_type = 'daily'\n",
    "                    elif 167 <= period <= 169:\n",
    "                        cycle_type = 'weekly'\n",
    "                    else:\n",
    "                        cycle_type = f'{period:.1f}_hour'\n",
    "                else:\n",
    "                    cycle_type = f'{period:.1f}_period'\n",
    "                \n",
    "                interpretation['primary_cycles'].append({\n",
    "                    'period': period,\n",
    "                    'cycle_type': cycle_type,\n",
    "                    'magnitude': magnitude,\n",
    "                    'rank': i + 1\n",
    "                })\n",
    "                \n",
    "                print(f\"Cycle {i+1}: {cycle_type} ({period:.1f} periods, magnitude: {magnitude:.3f})\")\n",
    "        \n",
    "        # 3. TREND ANALYSIS (from DC component and low frequencies)\n",
    "        dc_component = np.mean(data['time_series'])\n",
    "        low_freq_power = np.sum(data['power'][:5]) / np.sum(data['power'])\n",
    "        \n",
    "        if low_freq_power > 0.3:\n",
    "            interpretation['trend_component'] = 'strong_trend'\n",
    "        elif low_freq_power > 0.1:\n",
    "            interpretation['trend_component'] = 'moderate_trend'\n",
    "        else:\n",
    "            interpretation['trend_component'] = 'stable'\n",
    "        \n",
    "        print(f\"Trend Component: {interpretation['trend_component']}\")\n",
    "        print(f\"Low Frequency Power: {low_freq_power:.3f}\")\n",
    "        \n",
    "        # 4. NOISE ASSESSMENT\n",
    "        high_freq_power = np.sum(data['power'][len(data['power'])//2:]) / np.sum(data['power'])\n",
    "        \n",
    "        if high_freq_power > 0.4:\n",
    "            interpretation['noise_level'] = 'high'\n",
    "        elif high_freq_power > 0.2:\n",
    "            interpretation['noise_level'] = 'moderate'\n",
    "        else:\n",
    "            interpretation['noise_level'] = 'low'\n",
    "        \n",
    "        print(f\"Noise Level: {interpretation['noise_level']}\")\n",
    "        print(f\"High Frequency Power: {high_freq_power:.3f}\")\n",
    "        \n",
    "        interpretations[category] = interpretation\n",
    "    \n",
    "    return interpretations\n",
    "```\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "### **3. COMPREHENSIVE INTERPRETATION FRAMEWORK**\n",
    "\n",
    "#### **A. Frequency Domain Insights**\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "#### **B. Advanced Interpretation Methods**\n",
    "\n",
    "```python\n",
    "def advanced_fourier_interpretation(results, time_index):\n",
    "    \"\"\"\n",
    "    Advanced interpretation techniques for Fourier analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    advanced_insights = {}\n",
    "    \n",
    "    for category, data in results.items():\n",
    "        insights = {\n",
    "            'spectral_characteristics': {},\n",
    "            'seasonal_decomposition': {},\n",
    "            'predictability_metrics': {},\n",
    "            'comparative_analysis': {}\n",
    "        }\n",
    "        \n",
    "        # 1. SPECTRAL CHARACTERISTICS\n",
    "        frequencies = data['frequencies']\n",
    "        magnitude = data['magnitude']\n",
    "        power = data['power']\n",
    "        \n",
    "        # Spectral centroid (weighted average frequency)\n",
    "        spectral_centroid = np.sum(frequencies * magnitude) / np.sum(magnitude)\n",
    "        \n",
    "        # Spectral spread (frequency distribution width)\n",
    "        spectral_spread = np.sqrt(np.sum(((frequencies - spectral_centroid) ** 2) * magnitude) / np.sum(magnitude))\n",
    "        \n",
    "        # Spectral rolloff (frequency below which 85% of energy is contained)\n",
    "        cumulative_power = np.cumsum(power)\n",
    "        rolloff_threshold = 0.85 * cumulative_power[-1]\n",
    "        rolloff_idx = np.where(cumulative_power >= rolloff_threshold)[0][0]\n",
    "        spectral_rolloff = frequencies[rolloff_idx]\n",
    "        \n",
    "        insights['spectral_characteristics'] = {\n",
    "            'centroid': spectral_centroid,\n",
    "            'spread': spectral_spread,\n",
    "            'rolloff': spectral_rolloff,\n",
    "            'bandwidth': spectral_rolloff - frequencies[0]\n",
    "        }\n",
    "        \n",
    "        # 2. SEASONAL DECOMPOSITION INSIGHTS\n",
    "        # Reconstruct signal from dominant frequencies\n",
    "        dominant_indices = data['peak_indices']\n",
    "        reconstructed_signal = np.zeros(len(data['time_series']))\n",
    "        \n",
    "        for idx in dominant_indices:\n",
    "            freq = frequencies[idx]\n",
    "            amplitude = magnitude[idx] / len(data['time_series'])\n",
    "            phase = data['phase'][idx]\n",
    "            \n",
    "            # Reconstruct sinusoidal component\n",
    "            t = np.arange(len(data['time_series']))\n",
    "            component = amplitude * np.cos(2 * np.pi * freq * t + phase)\n",
    "            reconstructed_signal += component\n",
    "        \n",
    "        # Calculate reconstruction quality\n",
    "        original_signal = data['detrended_series']\n",
    "        reconstruction_error = np.mean((original_signal - reconstructed_signal) ** 2)\n",
    "        reconstruction_r2 = 1 - reconstruction_error / np.var(original_signal)\n",
    "        \n",
    "        insights['seasonal_decomposition'] = {\n",
    "            'reconstructed_signal': reconstructed_signal,\n",
    "            'reconstruction_r2': reconstruction_r2,\n",
    "            'residual_variance': reconstruction_error,\n",
    "            'seasonal_component_strength': np.var(reconstructed_signal) / np.var(original_signal)\n",
    "        }\n",
    "        \n",
    "        # 3. PREDICTABILITY METRICS\n",
    "        # Entropy of power spectrum (measure of predictability)\n",
    "        power_normalized = power / np.sum(power)\n",
    "        spectral_entropy = -np.sum(power_normalized * np.log2(power_normalized + 1e-10))\n",
    "        max_entropy = np.log2(len(power))\n",
    "        normalized_entropy = spectral_entropy / max_entropy\n",
    "        \n",
    "        # Regularity index (concentration of power in dominant frequencies)\n",
    "        dominant_power = np.sum(power[dominant_indices])\n",
    "        total_power = np.sum(power)\n",
    "        regularity_index = dominant_power / total_power\n",
    "        \n",
    "        insights['predictability_metrics'] = {\n",
    "            'spectral_entropy': spectral_entropy,\n",
    "            'normalized_entropy': normalized_entropy,\n",
    "            'regularity_index': regularity_index,\n",
    "            'predictability_score': 1 - normalized_entropy  # Higher = more predictable\n",
    "        }\n",
    "        \n",
    "        advanced_insights[category] = insights\n",
    "    \n",
    "    return advanced_insights\n",
    "\n",
    "def business_interpretation_framework(results, category_context=None):\n",
    "    \"\"\"\n",
    "    Translate Fourier analysis results into business insights\n",
    "    \"\"\"\n",
    "    \n",
    "    business_insights = {}\n",
    "    \n",
    "    for category, data in results.items():\n",
    "        insights = {\n",
    "            'business_patterns': [],\n",
    "            'operational_implications': [],\n",
    "            'forecasting_potential': 'low',\n",
    "            'anomaly_detection_capability': 'low'\n",
    "        }\n",
    "        \n",
    "        # Analyze dominant cycles for business meaning\n",
    "        if 'primary_cycles' in data:\n",
    "            for cycle in data['primary_cycles']:\n",
    "                period = cycle['period']\n",
    "                cycle_type = cycle['cycle_type']\n",
    "                magnitude = cycle['magnitude']\n",
    "                \n",
    "                # Business pattern interpretation\n",
    "                if cycle_type == 'weekly':\n",
    "                    insights['business_patterns'].append({\n",
    "                        'pattern': 'Weekly Business Cycle',\n",
    "                        'description': f'{category} shows {magnitude:.2f} strength weekly pattern',\n",
    "                        'implication': 'Staff scheduling and inventory planning opportunities'\n",
    "                    })\n",
    "                elif cycle_type == 'monthly':\n",
    "                    insights['business_patterns'].append({\n",
    "                        'pattern': 'Monthly Consumer Cycle',\n",
    "                        'description': f'{category} follows monthly spending/behavior patterns',\n",
    "                        'implication': 'Align marketing campaigns with monthly cycles'\n",
    "                    })\n",
    "                elif cycle_type == 'quarterly':\n",
    "                    insights['business_patterns'].append({\n",
    "                        'pattern': 'Quarterly Business Seasonality',\n",
    "                        'description': f'{category} shows quarterly seasonal variation',\n",
    "                        'implication': 'Strategic planning and budget allocation considerations'\n",
    "                    })\n",
    "                elif cycle_type == 'annual':\n",
    "                    insights['business_patterns'].append({\n",
    "                        'pattern': 'Annual Seasonal Pattern',\n",
    "                        'description': f'{category} has strong yearly seasonality',\n",
    "                        'implication': 'Long-term capacity planning and seasonal hiring'\n",
    "                    })\n",
    "        \n",
    "        # Forecasting potential assessment\n",
    "        explained_variance = data.get('explained_variance_ratio', 0)\n",
    "        if explained_variance > 0.6:\n",
    "            insights['forecasting_potential'] = 'high'\n",
    "            insights['operational_implications'].append('High forecasting accuracy possible')\n",
    "        elif explained_variance > 0.3:\n",
    "            insights['forecasting_potential'] = 'medium'\n",
    "            insights['operational_implications'].append('Moderate forecasting accuracy expected')\n",
    "        else:\n",
    "            insights['forecasting_potential'] = 'low'\n",
    "            insights['operational_implications'].append('Limited forecasting reliability')\n",
    "        \n",
    "        # Anomaly detection capability\n",
    "        regularity = data.get('regularity_index', 0)\n",
    "        if regularity > 0.5:\n",
    "            insights['anomaly_detection_capability'] = 'high'\n",
    "            insights['operational_implications'].append('Strong baseline for anomaly detection')\n",
    "        elif regularity > 0.2:\n",
    "            insights['anomaly_detection_capability'] = 'medium'\n",
    "            insights['operational_implications'].append('Moderate anomaly detection possible')\n",
    "        \n",
    "        business_insights[category] = insights\n",
    "    \n",
    "    return business_insights\n",
    "```\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "### **4. PRACTICAL EXAMPLES & CASE STUDIES**\n",
    "\n",
    "#### **A. Example 1: Strong Weekly Pattern**\n",
    "\n",
    "```python\n",
    "def create_weekly_pattern_example():\n",
    "    \"\"\"\n",
    "    Generate synthetic data with strong weekly seasonality\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create 2 years of daily data\n",
    "    dates = pd.date_range('2022-01-01', '2023-12-31', freq='D')\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    # Create strong weekly pattern: More males on weekdays, more females on weekends\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = []\n",
    "    for i, date in enumerate(dates):\n",
    "        day_of_week = date.dayofweek  # 0=Monday, 6=Sunday\n",
    "        \n",
    "        # Weekly pattern: Higher male proportion on weekdays\n",
    "        if day_of_week < 5:  # Weekdays\n",
    "            male_prob = 0.7 + 0.1 * np.sin(2 * np.pi * i / 7)  # Weekly sine wave\n",
    "        else:  # Weekends\n",
    "            male_prob = 0.3 + 0.1 * np.sin(2 * np.pi * i / 7)\n",
    "        \n",
    "        # Add some noise\n",
    "        male_prob += np.random.normal(0, 0.05)\n",
    "        male_prob = np.clip(male_prob, 0.1, 0.9)\n",
    "        \n",
    "        # Generate observations for this day\n",
    "        n_obs = np.random.poisson(50)  # Average 50 observations per day\n",
    "        genders = np.random.choice(['Male', 'Female'], size=n_obs, p=[male_prob, 1-male_prob])\n",
    "        \n",
    "        for gender in genders:\n",
    "            data.append({'date': date, 'gender': gender})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Apply Fourier analysis\n",
    "    results, proportion_series = comprehensive_fourier_categorical_analysis(\n",
    "        df, 'gender', 'date', sampling_period='D'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== WEEKLY PATTERN EXAMPLE RESULTS ===\")\n",
    "    for category, result in results.items():\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        print(f\"Dominant periods: {result['dominant_periods']}\")\n",
    "        print(f\"Expected: ~7 days (weekly pattern)\")\n",
    "        print(f\"Explained variance: {result['explained_variance_ratio']:.3f}\")\n",
    "    \n",
    "    return df, results, proportion_series\n",
    "\n",
    "# Example 2: Monthly + Annual Pattern\n",
    "def create_complex_seasonal_example():\n",
    "    \"\"\"\n",
    "    Generate data with both monthly and annual seasonality\n",
    "    \"\"\"\n",
    "    \n",
    "    dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    data = []\n",
    "    for i, date in enumerate(dates):\n",
    "        # Annual pattern: More of category A in summer\n",
    "        annual_component = 0.1 * np.sin(2 * np.pi * i / 365.25)\n",
    "        \n",
    "        # Monthly pattern: Category A peaks mid-month\n",
    "        monthly_component = 0.05 * np.sin(2 * np.pi * i / 30.44)\n",
    "        \n",
    "        # Base probability + seasonal components\n",
    "        prob_A = 0.5 + annual_component + monthly_component\n",
    "        prob_A = np.clip(prob_A, 0.1, 0.9)\n",
    "        \n",
    "        # Generate observations\n",
    "        n_obs = np.random.poisson(30)\n",
    "        categories = np.random.choice(['A', 'B'], size=n_obs, p=[prob_A, 1-prob_A])\n",
    "        \n",
    "        for cat in categories:\n",
    "            data.append({'date': date, 'category': cat})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    results, proportion_series = comprehensive_fourier_categorical_analysis(\n",
    "        df, 'category', 'date', sampling_period='D'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== COMPLEX SEASONAL EXAMPLE RESULTS ===\")\n",
    "    for category, result in results.items():\n",
    "        periods = result['dominant_periods']\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        print(f\"Dominant periods: {periods}\")\n",
    "        print(\"Expected: ~30 days (monthly) and ~365 days (annual)\")\n",
    "        print(f\"Explained variance: {result['explained_variance_ratio']:.3f}\")\n",
    "    \n",
    "    return df, results, proportion_series\n",
    "\n",
    "# Example 3: Trend + Seasonality\n",
    "def create_trend_seasonal_example():\n",
    "    \"\"\"\n",
    "    Generate data with both trend and seasonal components\n",
    "    \"\"\"\n",
    "    \n",
    "    dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')\n",
    "    n_days = len(dates)\n",
    "    \n",
    "    data = []\n",
    "    for i, date in enumerate(dates):\n",
    "        # Linear trend: Category A proportion increases over time\n",
    "        trend_component = 0.1 * (i / n_days)  # 0 to 0.1 over the period\n",
    "        \n",
    "        # Seasonal component: Annual cycle\n",
    "        seasonal_component = 0.15 * np.sin(2 * np.pi * i / 365.25)\n",
    "        \n",
    "        # Combine components\n",
    "        prob_A = 0.4 + trend_component + seasonal_component\n",
    "        prob_A = np.clip(prob_A, 0.1, 0.9)\n",
    "        \n",
    "        # Generate observations\n",
    "        n_obs = np.random.poisson(40)\n",
    "        categories = np.random.choice(['A', 'B'], size=n_obs, p=[prob_A, 1-prob_A])\n",
    "        \n",
    "        for cat in categories:\n",
    "            data.append({'date': date, 'category': cat})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    results, proportion_series = comprehensive_fourier_categorical_analysis(\n",
    "        df, 'category', 'date', sampling_period='D'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== TREND + SEASONAL EXAMPLE RESULTS ===\")\n",
    "    for category, result in results.items():\n",
    "        periods = result['dominant_periods']\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        print(f\"Dominant periods: {periods}\")\n",
    "        print(\"Expected: ~365 days (annual) + low-frequency trend\")\n",
    "        print(f\"Explained variance: {result['explained_variance_ratio']:.3f}\")\n",
    "    \n",
    "    return df, results, proportion_series\n",
    "```\n",
    "\n",
    "#### **B. Real-World Application Scenarios**\n",
    "\n",
    "```python\n",
    "def retail_customer_analysis_example():\n",
    "    \"\"\"\n",
    "    Example: Retail customer gender patterns analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulate retail customer data with realistic patterns\n",
    "    dates = pd.date_range('2022-01-01', '2023-12-31', freq='H')  # Hourly data\n",
    "    \n",
    "    data = []\n",
    "    for i, date in enumerate(dates):\n",
    "        hour = date.hour\n",
    "        day_of_week = date.dayofweek\n",
    "        month = date.month\n",
    "        \n",
    "        # Daily pattern: More females during daytime shopping hours\n",
    "        if 9 <= hour <= 17:\n",
    "            female_prob = 0.65\n",
    "        elif 18 <= hour <= 21:  # Evening shopping\n",
    "            female_prob = 0.55\n",
    "        else:  # Early morning/late night\n",
    "            female_prob = 0.45\n",
    "        \n",
    "        # Weekly pattern: More families (balanced) on weekends\n",
    "        if day_of_week >= 5:  # Weekend\n",
    "            female_prob = 0.52\n",
    "        \n",
    "        # Seasonal pattern: Holiday shopping (November-December)\n",
    "        if month in [11, 12]:\n",
    "            female_prob += 0.08  # More female shoppers during holidays\n",
    "        \n",
    "        # Add noise\n",
    "        female_prob += np.random.normal(0, 0.03)\n",
    "        female_prob = np.clip(female_prob, 0.3, 0.8)\n",
    "        \n",
    "        # Generate customers for this hour\n",
    "        n_customers = max(0, np.random.poisson(20) - 10)  # Variable traffic\n",
    "        if n_customers > 0:\n",
    "            genders = np.random.choice(['Male', 'Female'], \n",
    "                                     size=n_customers, \n",
    "                                     p=[1-female_prob, female_prob])\n",
    "            \n",
    "            for gender in genders:\n",
    "                data.append({'datetime': date, 'gender': gender})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Analyze at different time scales\n",
    "    print(\"=== RETAIL CUSTOMER ANALYSIS ===\")\n",
    "    \n",
    "    # Daily patterns (hourly sampling)\n",
    "    results_hourly, _ = comprehensive_fourier_categorical_analysis(\n",
    "        df, 'gender', 'datetime', sampling_period='H'\n",
    "    )\n",
    "    \n",
    "    # Weekly patterns (daily sampling)  \n",
    "    results_daily, _ = comprehensive_fourier_categorical_analysis(\n",
    "        df, 'gender', 'datetime', sampling_period='D'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nHourly Analysis (Daily Patterns):\")\n",
    "    for category, result in results_hourly.items():\n",
    "        periods = result['dominant_periods']\n",
    "        daily_periods = [p for p in periods if 20 <= p <= 28]  # ~24 hour periods\n",
    "        if daily_periods:\n",
    "            print(f\"  {category}: Daily pattern detected at {daily_periods[0]:.1f} hours\")\n",
    "    \n",
    "    print(\"\\nDaily Analysis (Weekly/Seasonal Patterns):\")\n",
    "    for category, result in results_daily.items():\n",
    "        periods = result['dominant_periods']\n",
    "        weekly_periods = [p for p in periods if 6 <= p <= 8]  # ~7 day periods\n",
    "        seasonal_periods = [p for p in periods if 360 <= p <= 370]  # ~365 day periods\n",
    "        \n",
    "        if weekly_periods:\n",
    "            print(f\"  {category}: Weekly pattern at {weekly_periods[0]:.1f} days\")\n",
    "        if seasonal_periods:\n",
    "            print(f\"  {category}: Annual pattern at {seasonal_periods[0]:.1f} days\")\n",
    "    \n",
    "    return df, results_hourly, results_daily\n",
    "```\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "### **5. COMPREHENSIVE VISUALIZATION FRAMEWORK**\n",
    "\n",
    "```python\n",
    "def create_fourier_visualization_suite(results, proportion_series, category_name):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization suite for Fourier analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Get data for the specified category\n",
    "    data = results[category_name]\n",
    "    \n",
    "    # 1. TIME SERIES PLOT\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    plt.plot(proportion_series.index, proportion_series[category_name], 'b-', alpha=0.7, linewidth=1)\n",
    "    plt.title(f'Time Series: {category_name} Proportions', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. DETRENDED TIME SERIES\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    plt.plot(data['detrended_series'], 'g-', alpha=0.7, linewidth=1)\n",
    "    plt.title(f'Detrended Series: {category_name}', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Detrended Proportion')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. MAGNITUDE SPECTRUM\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    plt.semilogy(data['frequencies'], data['magnitude'], 'r-', linewidth=2)\n",
    "    \n",
    "    # Highlight dominant frequencies\n",
    "    for i, peak_idx in enumerate(data['peak_indices'][:5]):  # Top 5 peaks\n",
    "        plt.semilogy(data['frequencies'][peak_idx], data['magnitude'][peak_idx], \n",
    "                    'ro', markersize=8, label=f'Peak {i+1}' if i < 3 else '')\n",
    "    \n",
    "    plt.title(f'Magnitude Spectrum: {category_name}', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Frequency (cycles/period)')\n",
    "    plt.ylabel('Magnitude (log scale)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. POWER SPECTRUM\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    plt.loglog(data['frequencies'], data['power'], 'purple', linewidth=2)\n",
    "    plt.title(f'Power Spectrum: {category_name}', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Frequency (cycles/period)')\n",
    "    plt.ylabel('Power (log scale)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. PERIOD SPECTRUM (more intuitive)\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    periods = 1 / data['frequencies'][data['frequencies'] > 0]\n",
    "    power_periods = data['power'][data['frequencies'] > 0]\n",
    "    \n",
    "    # Filter reasonable periods (avoid very short or very long)\n",
    "    valid_mask = (periods >= 2) & (periods <= len(proportion_series) / 2)\n",
    "    periods_filtered = periods[valid_mask]\n",
    "    power_filtered = power_periods[valid_mask]\n",
    "    \n",
    "    plt.semilogx(periods_filtered, power_filtered, 'orange', linewidth=2)\n",
    "    plt.title(f'Period Spectrum: {category_name}', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Period (time units)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add period labels for common cycles\n",
    "    common_periods = [7, 30, 91, 365]  # Weekly, monthly, quarterly, annual\n",
    "    for period in common_periods:\n",
    "        if period <= max(periods_filtered):\n",
    "            plt.axvline(x=period, color='red', linestyle='--', alpha=0.5)\n",
    "            plt.text(period, max(power_filtered)*0.8, f'{period}d', \n",
    "                    rotation=90, verticalalignment='bottom')\n",
    "    \n",
    "    # 6. PHASE SPECTRUM\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    plt.plot(data['frequencies'], np.degrees(data['phase']), 'cyan', linewidth=1)\n",
    "    plt.title(f'Phase Spectrum: {category_name}', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Frequency (cycles/period)')\n",
    "    plt.ylabel('Phase (degrees)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. RECONSTRUCTED SIGNAL COMPARISON\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    \n",
    "    # Reconstruct signal from dominant frequencies\n",
    "    reconstructed = np.zeros(len(data['detrended_series']))\n",
    "    t = np.arange(len(data['detrended_series']))\n",
    "    \n",
    "    for peak_idx in data['peak_indices'][:3]:  # Top 3 components\n",
    "        freq = data['frequencies'][peak_idx]\n",
    "        amplitude = data['magnitude'][peak_idx] / len(data['detrended_series'])\n",
    "        phase = data['phase'][peak_idx]\n",
    "        component = amplitude * np.cos(2 * np.pi * freq * t + phase)\n",
    "        reconstructed += component\n",
    "    \n",
    "    plt.plot(data['detrended_series'], 'b-', alpha=0.7, label='Original', linewidth=1)\n",
    "    plt.plot(reconstructed, 'r--', alpha=0.8, label='Reconstructed (Top 3)', linewidth=2)\n",
    "    plt.title(f'Signal Reconstruction: {category_name}', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 8. DOMINANT CYCLES BAR CHART\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    \n",
    "    if len(data['dominant_periods']) > 0:\n",
    "        # Sort by magnitude\n",
    "        sorted_indices = np.argsort(data['dominant_magnitudes'])[::-1]\n",
    "        top_periods = data['dominant_periods'][sorted_indices][:5]\n",
    "        top_magnitudes = data['dominant_magnitudes'][sorted_indices][:5]\n",
    "        \n",
    "        bars = plt.bar(range(len(top_periods)), top_magnitudes, \n",
    "                      color=['red', 'orange', 'yellow', 'green', 'blue'][:len(top_periods)])\n",
    "        \n",
    "        # Add period labels\n",
    "        labels = [f'{period:.1f}' for period in top_periods]\n",
    "        plt.xticks(range(len(top_periods)), labels)\n",
    "        \n",
    "        plt.title(f'Top Dominant Cycles: {category_name}', fontsize=12, fontweight='bold')\n",
    "        plt.xlabel('Period (time units)')\n",
    "        plt.ylabel('Magnitude')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, mag) in enumerate(zip(bars, top_magnitudes)):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(top_magnitudes)*0.01,\n",
    "                    f'{mag:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # 9. SUMMARY STATISTICS\n",
    "    ax9 = plt.subplot(3, 3, 9)\n",
    "    ax9.axis('off')\n",
    "    \n",
    "    # Create summary text\n",
    "    summary_text = f\"\"\"\n",
    "    FOURIER ANALYSIS SUMMARY\n",
    "    Category: {category_name}\n",
    "    \n",
    "    Key Metrics:\n",
    "    • Explained Variance: {data['explained_variance_ratio']:.3f}\n",
    "    • Total Variance: {data['total_variance']:.6f}\n",
    "    • Dominant Cycles: {len(data['dominant_periods'])}\n",
    "    \n",
    "    Top 3 Periods:\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(data['dominant_periods']) > 0:\n",
    "        sorted_indices = np.argsort(data['dominant_magnitudes'])[::-1]\n",
    "        for i in range(min(3, len(sorted_indices))):\n",
    "            idx = sorted_indices[i]\n",
    "            period = data['dominant_periods'][idx]\n",
    "            magnitude = data['dominant_magnitudes'][idx]\n",
    "            summary_text += f\"    {i+1}. {period:.1f} units (mag: {magnitude:.3f})\\n\"\n",
    "    \n",
    "    # Interpretation\n",
    "    explained_var = data['explained_variance_ratio']\n",
    "    if explained_var > 0.7:\n",
    "        interpretation = \"Very Strong Seasonality\"\n",
    "    elif explained_var > 0.5:\n",
    "        interpretation = \"Strong Seasonality\"\n",
    "    elif explained_var > 0.3:\n",
    "        interpretation = \"Moderate Seasonality\"\n",
    "    elif explained_var > 0.1:\n",
    "        interpretation = \"Weak Seasonality\"\n",
    "    else:\n",
    "        interpretation = \"Very Weak/No Seasonality\"\n",
    "    \n",
    "    summary_text += f\"\\n    Interpretation: {interpretation}\"\n",
    "    \n",
    "    plt.text(0.05, 0.95, summary_text, transform=ax9.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Comprehensive Fourier Analysis: {category_name}', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_comparative_analysis_plot(results):\n",
    "    \"\"\"\n",
    "    Create comparative analysis across all categories\n",
    "    \"\"\"\n",
    "    \n",
    "    categories = list(results.keys())\n",
    "    n_categories = len(categories)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Explained Variance Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    explained_vars = [results[cat]['explained_variance_ratio'] for cat in categories]\n",
    "    bars1 = ax1.bar(categories, explained_vars, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:n_categories])\n",
    "    ax1.set_title('Seasonality Strength Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Explained Variance Ratio')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, explained_vars):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Dominant Period Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    # Get primary period for each category\n",
    "    primary_periods = []\n",
    "    for cat in categories:\n",
    "        if len(results[cat]['dominant_periods']) > 0:\n",
    "            # Get period with highest magnitude\n",
    "            max_idx = np.argmax(results[cat]['dominant_magnitudes'])\n",
    "            primary_periods.append(results[cat]['dominant_periods'][max_idx])\n",
    "        else:\n",
    "            primary_periods.append(0)\n",
    "    \n",
    "    bars2 = ax2.bar(categories, primary_periods, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:n_categories])\n",
    "    ax2.set_title('Primary Cycle Period Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Period (time units)')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars2, primary_periods):\n",
    "        if val > 0:\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(primary_periods)*0.01,\n",
    "                    f'{val:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Power Spectrum Overlay\n",
    "    ax3 = axes[1, 0]\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    for i, cat in enumerate(categories):\n",
    "        data = results[cat]\n",
    "        # Normalize power for comparison\n",
    "        normalized_power = data['power'] / np.max(data['power'])\n",
    "        ax3.loglog(data['frequencies'], normalized_power, \n",
    "                  color=colors[i % len(colors)], label=cat, linewidth=2, alpha=0.7)\n",
    "    \n",
    "    ax3.set_title('Power Spectrum Comparison', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Frequency (cycles/period)')\n",
    "    ax3.set_ylabel('Normalized Power')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Cycle Distribution Heatmap\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Create matrix of cycle strengths\n",
    "    common_periods = [2, 3, 7, 14, 30, 91, 182, 365]  # Common business cycles\n",
    "    cycle_matrix = np.zeros((len(categories), len(common_periods)))\n",
    "    \n",
    "    for i, cat in enumerate(categories):\n",
    "        data = results[cat]\n",
    "        for j, target_period in enumerate(common_periods):\n",
    "            # Find closest detected period\n",
    "            if len(data['dominant_periods']) > 0:\n",
    "                period_diffs = np.abs(data['dominant_periods'] - target_period)\n",
    "                closest_idx = np.argmin(period_diffs)\n",
    "                \n",
    "                # If close enough (within 20%), record the magnitude\n",
    "                if period_diffs[closest_idx] / target_period < 0.2:\n",
    "                    cycle_matrix[i, j] = data['dominant_magnitudes'][closest_idx]\n",
    "    \n",
    "    im = ax4.imshow(cycle_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    ax4.set_title('Cycle Strength Heatmap', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Period Type')\n",
    "    ax4.set_ylabel('Category')\n",
    "    ax4.set_xticks(range(len(common_periods)))\n",
    "    ax4.set_xticklabels([f'{p}d' for p in common_periods], rotation=45)\n",
    "    ax4.set_yticks(range(len(categories)))\n",
    "    ax4.set_yticklabels(categories)\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=ax4, label='Magnitude')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(categories)):\n",
    "        for j in range(len(common_periods)):\n",
    "            if cycle_matrix[i, j] > 0:\n",
    "                ax4.text(j, i, f'{cycle_matrix[i, j]:.2f}', \n",
    "                        ha='center', va='center', fontweight='bold', \n",
    "                        color='white' if cycle_matrix[i, j] > np.max(cycle_matrix)*0.5 else 'black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Comparative Fourier Analysis Results', fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    return fig\n",
    "```\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **6. KEY INSIGHTS FOURIER ANALYSIS PROVIDES**\n",
    "\n",
    "### **A. Seasonality Detection & Quantification**\n",
    "\n",
    "1. **Cycle Identification**: Automatically detects dominant cycles (weekly, monthly, annual)\n",
    "2. **Strength Measurement**: Quantifies how strong each seasonal pattern is\n",
    "3. **Multiple Seasonality**: Can detect overlapping seasonal patterns simultaneously\n",
    "4. **Phase Information**: Reveals when in the cycle peaks/troughs occur\n",
    "\n",
    "### **B. Trend Analysis Capabilities**\n",
    "\n",
    "1. **Low-Frequency Components**: DC and near-DC components reveal long-term trends\n",
    "2. **Trend vs Seasonality**: Separates trend from cyclical components\n",
    "3. **Trend Strength**: Quantifies how much variation is due to trend vs seasonality\n",
    "\n",
    "### **C. Predictability Assessment**\n",
    "\n",
    "1. **Signal-to-Noise Ratio**: Distinguishes predictable patterns from random noise\n",
    "2. **Forecasting Potential**: High explained variance indicates good forecasting potential\n",
    "3. **Regularity Metrics**: Measures how consistent the patterns are over time\n",
    "\n",
    "### **D. Business Intelligence**\n",
    "\n",
    "1. **Operational Planning**: Identifies cycles relevant for staffing, inventory, marketing\n",
    "2. **Anomaly Detection**: Establishes baseline patterns for detecting unusual behavior\n",
    "3. **Resource Allocation**: Informs when to scale resources up/down based on cycles\n",
    "\n",
    "## **7. MATHEMATICAL ADVANTAGES OVER CHI-SQUARED**\n",
    "\n",
    "| **Aspect** | **Chi-Squared Test** | **Fourier Analysis** |\n",
    "|------------|---------------------|---------------------|\n",
    "| **Pattern Detection** | Tests for any dependence | Identifies specific cycles |\n",
    "| **Multiple Cycles** | Cannot separate | Detects all simultaneously |\n",
    "| **Strength Quantification** | Binary (significant/not) | Continuous strength measures |\n",
    "| **Phase Information** | None | Reveals timing of cycles |\n",
    "| **Trend Detection** | Limited | Excellent via low frequencies |\n",
    "| **Noise Handling** | Sensitive to noise | Separates signal from noise |\n",
    "| **Forecasting** | Not applicable | Enables prediction |\n",
    "\n",
    "## **SUMMARY: Complete Fourier Framework**\n",
    "\n",
    "**✅ Mathematical Foundation**: DFT transforms categorical proportions to frequency domain\n",
    "\n",
    "**✅ Implementation**: Comprehensive analysis with proper preprocessing and windowing  \n",
    "\n",
    "**✅ Interpretation**: Multi-level framework from spectral metrics to business insights\n",
    "\n",
    "**✅ Examples**: Practical cases showing different seasonal patterns\n",
    "\n",
    "**✅ Visualization**: Complete suite for presenting results effectively\n",
    "\n",
    "**Key Advantage**: Fourier analysis doesn't just detect seasonality—it **quantifies, characterizes, and enables prediction** of complex temporal patterns in categorical data, making it far more powerful than simple statistical tests for understanding cyclical behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983a12e",
   "metadata": {},
   "source": [
    "I'll provide a deep mathematical explanation of why Fourier analysis is so powerful at detecting these properties. Let me break down each mathematical property and show exactly how the formulae enable these capabilities.\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **Mathematical Properties of Fourier Analysis: Deep Dive into Why It Works**\n",
    "\n",
    "### **1. PATTERN DETECTION: The Correlation Principle**\n",
    "\n",
    "#### **A. Mathematical Foundation**\n",
    "\n",
    "The DFT formula: **$X[k] = Σ(n=0 to N-1) x[n] × e^(-i2πkn/N) $**\n",
    "\n",
    "**Why this detects patterns:**\n",
    "\n",
    "The exponential term **e^(-i2πkn/N)** is a **complex sinusoid** at frequency k. The DFT essentially computes the **correlation** between your data and sinusoids of all possible frequencies.\n",
    "\n",
    "```python\n",
    "def explain_pattern_detection_mathematics():\n",
    "    \"\"\"\n",
    "    Mathematical explanation of why DFT detects patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    # The DFT is mathematically equivalent to:\n",
    "    # X[k] = Σ x[n] × [cos(2πkn/N) - i×sin(2πkn/N)]\n",
    "    \n",
    "    # This can be rewritten as two separate correlations:\n",
    "    # Real part: Σ x[n] × cos(2πkn/N)  [correlation with cosine]\n",
    "    # Imag part: Σ x[n] × sin(2πkn/N)   [correlation with sine]\n",
    "    \n",
    "    N = 100\n",
    "    n = np.arange(N)\n",
    "    \n",
    "    # Example: Data with hidden 10-period cycle\n",
    "    true_signal = np.cos(2 * np.pi * n / 10)  # 10-period cycle\n",
    "    noise = np.random.normal(0, 0.5, N)\n",
    "    data = true_signal + noise\n",
    "    \n",
    "    # Manual correlation calculation for frequency k=10 (10-period cycle)\n",
    "    k = 10\n",
    "    test_cosine = np.cos(2 * np.pi * k * n / N)\n",
    "    test_sine = np.sin(2 * np.pi * k * n / N)\n",
    "    \n",
    "    # Correlation coefficients\n",
    "    real_correlation = np.sum(data * test_cosine)\n",
    "    imag_correlation = -np.sum(data * test_sine)  # Note the minus sign in DFT\n",
    "    \n",
    "    # Magnitude (strength of correlation)\n",
    "    magnitude = np.sqrt(real_correlation**2 + imag_correlation**2)\n",
    "    \n",
    "    print(\"=== PATTERN DETECTION MATHEMATICS ===\")\n",
    "    print(f\"Data contains 10-period cycle\")\n",
    "    print(f\"Correlation with 10-period cosine: {real_correlation:.3f}\")\n",
    "    print(f\"Correlation with 10-period sine: {imag_correlation:.3f}\")\n",
    "    print(f\"Combined magnitude: {magnitude:.3f}\")\n",
    "    \n",
    "    # Compare with other frequencies\n",
    "    print(\"\\nCorrelation with other frequencies:\")\n",
    "    for test_k in [5, 15, 20]:\n",
    "        test_cos = np.cos(2 * np.pi * test_k * n / N)\n",
    "        correlation = np.sum(data * test_cos)\n",
    "        print(f\"  {test_k}-period: {correlation:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'data': data,\n",
    "        'true_frequency': 10,\n",
    "        'correlations': {\n",
    "            'real': real_correlation,\n",
    "            'imag': imag_correlation,\n",
    "            'magnitude': magnitude\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Mathematical insight: High correlation = Strong pattern match\n",
    "result = explain_pattern_detection_mathematics()\n",
    "```\n",
    "\n",
    "**Key Mathematical Insight:**\n",
    "- **High magnitude |X[k]|** means the data **strongly correlates** with a sinusoid of frequency k\n",
    "- **Low magnitude** means **weak or no correlation** with that frequency\n",
    "- This is why peaks in the magnitude spectrum indicate **dominant patterns**\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "### **2. MULTIPLE CYCLES: Orthogonality and Linear Superposition**\n",
    "\n",
    "#### **A. Mathematical Foundation: Orthogonality**\n",
    "\n",
    "**Why DFT can separate multiple cycles simultaneously:**\n",
    "\n",
    "The complex exponentials **e^(-i2πkn/N)** for different values of k are **orthogonal**:\n",
    "\n",
    "**Σ(n=0 to N-1) e^(-i2πk₁n/N) × e^(i2πk₂n/N) = N if k₁=k₂, 0 if k₁≠k₂**\n",
    "\n",
    "```python\n",
    "def demonstrate_orthogonality_principle():\n",
    "    \"\"\"\n",
    "    Mathematical demonstration of why orthogonality enables multiple cycle detection\n",
    "    \"\"\"\n",
    "    \n",
    "    N = 64\n",
    "    n = np.arange(N)\n",
    "    \n",
    "    # Create data with multiple cycles\n",
    "    cycle1 = 2 * np.cos(2 * np.pi * n / 8)    # 8-period cycle, amplitude 2\n",
    "    cycle2 = 1.5 * np.cos(2 * np.pi * n / 16)  # 16-period cycle, amplitude 1.5\n",
    "    cycle3 = 1 * np.cos(2 * np.pi * n / 32)    # 32-period cycle, amplitude 1\n",
    "    \n",
    "    # Combined signal\n",
    "    combined_signal = cycle1 + cycle2 + cycle3\n",
    "    \n",
    "    print(\"=== ORTHOGONALITY AND MULTIPLE CYCLES ===\")\n",
    "    print(\"Signal contains:\")\n",
    "    print(\"  - 8-period cycle (amplitude 2)\")\n",
    "    print(\"  - 16-period cycle (amplitude 1.5)\")  \n",
    "    print(\"  - 32-period cycle (amplitude 1)\")\n",
    "    \n",
    "    # Manual DFT calculation to show orthogonality\n",
    "    def manual_dft_component(data, k, N):\n",
    "        \"\"\"Calculate single DFT component manually\"\"\"\n",
    "        n = np.arange(N)\n",
    "        complex_exponential = np.exp(-1j * 2 * np.pi * k * n / N)\n",
    "        return np.sum(data * complex_exponential)\n",
    "    \n",
    "    # Test frequencies corresponding to our cycles\n",
    "    test_frequencies = [8, 16, 32]  # Periods we expect\n",
    "    test_k_values = [N // period for period in test_frequencies]  # Convert to k indices\n",
    "    \n",
    "    print(f\"\\nDFT Analysis (N={N}):\")\n",
    "    for period, k in zip(test_frequencies, test_k_values):\n",
    "        dft_value = manual_dft_component(combined_signal, k, N)\n",
    "        magnitude = abs(dft_value)\n",
    "        expected_magnitude = N/2  # For cosine: |X[k]| = N*A/2 where A is amplitude\n",
    "        \n",
    "        print(f\"  {period}-period cycle:\")\n",
    "        print(f\"    DFT[{k}] = {dft_value:.3f}\")\n",
    "        print(f\"    Magnitude = {magnitude:.3f}\")\n",
    "        print(f\"    Expected ≈ {expected_magnitude:.1f}\")\n",
    "    \n",
    "    # Show orthogonality in action\n",
    "    print(f\"\\nOrthogonality Verification:\")\n",
    "    \n",
    "    # Create basis functions\n",
    "    basis_8 = np.exp(-1j * 2 * np.pi * (N//8) * n / N)\n",
    "    basis_16 = np.exp(-1j * 2 * np.pi * (N//16) * n / N)\n",
    "    basis_32 = np.exp(-1j * 2 * np.pi * (N//32) * n / N)\n",
    "    \n",
    "    # Cross-correlations (should be ~0 due to orthogonality)\n",
    "    cross_8_16 = abs(np.sum(basis_8 * np.conj(basis_16)))\n",
    "    cross_8_32 = abs(np.sum(basis_8 * np.conj(basis_32)))\n",
    "    cross_16_32 = abs(np.sum(basis_16 * np.conj(basis_32)))\n",
    "    \n",
    "    print(f\"  |⟨basis_8, basis_16⟩| = {cross_8_16:.6f} (should be ~0)\")\n",
    "    print(f\"  |⟨basis_8, basis_32⟩| = {cross_8_32:.6f} (should be ~0)\")\n",
    "    print(f\"  |⟨basis_16, basis_32⟩| = {cross_16_32:.6f} (should be ~0)\")\n",
    "    \n",
    "    # Auto-correlations (should be N)\n",
    "    auto_8 = abs(np.sum(basis_8 * np.conj(basis_8)))\n",
    "    print(f\"  |⟨basis_8, basis_8⟩| = {auto_8:.1f} (should be {N})\")\n",
    "    \n",
    "    return {\n",
    "        'combined_signal': combined_signal,\n",
    "        'individual_cycles': [cycle1, cycle2, cycle3],\n",
    "        'orthogonality_verified': max(cross_8_16, cross_8_32, cross_16_32) < 1e-10\n",
    "    }\n",
    "\n",
    "result = demonstrate_orthogonality_principle()\n",
    "```\n",
    "\n",
    "#### **B. Linear Superposition Property**\n",
    "\n",
    "**Mathematical Property:** If **x[n] = s₁[n] + s₂[n] + s₃[n]**, then:\n",
    "**DFT{x[n]} = DFT{s₁[n]} + DFT{s₂[n]} + DFT{s₃[n]}**\n",
    "\n",
    "This **linearity** combined with **orthogonality** means:\n",
    "- Each cycle contributes **independently** to its corresponding frequency bin\n",
    "- No **interference** between different cycles\n",
    "- **Perfect separation** of overlapping patterns\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "### **3. STRENGTH QUANTIFICATION: Magnitude and Parseval's Theorem**\n",
    "\n",
    "#### **A. Mathematical Foundation: Magnitude as Correlation Strength**\n",
    "\n",
    "**Magnitude Formula:** **|X[k]| = √(Real[X[k]]² + Imag[X[k]]²)**\n",
    "\n",
    "**Why this quantifies strength:**\n",
    "\n",
    "```python\n",
    "def explain_strength_quantification():\n",
    "    \"\"\"\n",
    "    Mathematical explanation of how magnitude quantifies pattern strength\n",
    "    \"\"\"\n",
    "    \n",
    "    N = 100\n",
    "    n = np.arange(N)\n",
    "    \n",
    "    # Create signals with different amplitudes (strengths)\n",
    "    weak_signal = 0.5 * np.cos(2 * np.pi * n / 10)      # Amplitude 0.5\n",
    "    medium_signal = 1.0 * np.cos(2 * np.pi * n / 10)    # Amplitude 1.0  \n",
    "    strong_signal = 2.0 * np.cos(2 * np.pi * n / 10)    # Amplitude 2.0\n",
    "    \n",
    "    signals = {\n",
    "        'weak': weak_signal,\n",
    "        'medium': medium_signal, \n",
    "        'strong': strong_signal\n",
    "    }\n",
    "    \n",
    "    print(\"=== STRENGTH QUANTIFICATION MATHEMATICS ===\")\n",
    "    print(\"Signals with same frequency (10-period) but different amplitudes:\")\n",
    "    \n",
    "    for name, signal in signals.items():\n",
    "        # Calculate DFT manually for the dominant frequency\n",
    "        k = N // 10  # Frequency bin for 10-period cycle\n",
    "        \n",
    "        # DFT calculation\n",
    "        dft_value = np.sum(signal * np.exp(-1j * 2 * np.pi * k * n / N))\n",
    "        magnitude = abs(dft_value)\n",
    "        \n",
    "        # Theoretical relationship: For cosine with amplitude A\n",
    "        # |X[k]| = N * A / 2\n",
    "        theoretical_magnitude = N * np.max(signal) / 2\n",
    "        \n",
    "        print(f\"\\n{name.upper()} signal (amplitude {np.max(signal)}):\")\n",
    "        print(f\"  DFT magnitude: {magnitude:.3f}\")\n",
    "        print(f\"  Theoretical: {theoretical_magnitude:.3f}\")\n",
    "        print(f\"  Ratio: {magnitude/theoretical_magnitude:.6f}\")\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# Mathematical relationship: |X[k]| ∝ Amplitude of cycle at frequency k\n",
    "explain_strength_quantification()\n",
    "```\n",
    "\n",
    "#### **B. Parseval's Theorem: Energy Conservation**\n",
    "\n",
    "**Parseval's Theorem:** **Σ|x[n]|² = (1/N) × Σ|X[k]|²**\n",
    "\n",
    "**Why this enables strength comparison:**\n",
    "\n",
    "```python\n",
    "def demonstrate_parsevals_theorem():\n",
    "    \"\"\"\n",
    "    Show how Parseval's theorem enables relative strength measurement\n",
    "    \"\"\"\n",
    "    \n",
    "    N = 64\n",
    "    n = np.arange(N)\n",
    "    \n",
    "    # Create signal with multiple components\n",
    "    component1 = 3 * np.cos(2 * np.pi * n / 8)   # Strong 8-period\n",
    "    component2 = 1 * np.cos(2 * np.pi * n / 16)  # Weak 16-period\n",
    "    component3 = 2 * np.cos(2 * np.pi * n / 32)  # Medium 32-period\n",
    "    \n",
    "    signal = component1 + component2 + component3\n",
    "    \n",
    "    # Calculate DFT\n",
    "    X = np.fft.fft(signal)\n",
    "    frequencies = np.fft.fftfreq(N)\n",
    "    \n",
    "    # Time domain energy\n",
    "    time_energy = np.sum(np.abs(signal)**2)\n",
    "    \n",
    "    # Frequency domain energy\n",
    "    freq_energy = np.sum(np.abs(X)**2) / N\n",
    "    \n",
    "    print(\"=== PARSEVAL'S THEOREM: ENERGY CONSERVATION ===\")\n",
    "    print(f\"Time domain energy: {time_energy:.3f}\")\n",
    "    print(f\"Frequency domain energy: {freq_energy:.3f}\")\n",
    "    print(f\"Ratio (should be 1.0): {freq_energy/time_energy:.6f}\")\n",
    "    \n",
    "    # Energy distribution analysis\n",
    "    power_spectrum = np.abs(X)**2\n",
    "    total_power = np.sum(power_spectrum)\n",
    "    \n",
    "    # Find peaks corresponding to our components\n",
    "    peak_indices = [N//8, N//16, N//32]  # Expected frequency bins\n",
    "    \n",
    "    print(f\"\\nEnergy Distribution:\")\n",
    "    for i, (period, k) in enumerate(zip([8, 16, 32], peak_indices)):\n",
    "        component_power = power_spectrum[k]\n",
    "        relative_power = component_power / total_power\n",
    "        \n",
    "        print(f\"  {period}-period cycle:\")\n",
    "        print(f\"    Power: {component_power:.1f}\")\n",
    "        print(f\"    Relative: {relative_power:.3f} ({relative_power*100:.1f}%)\")\n",
    "    \n",
    "    # This shows how DFT magnitude directly relates to signal energy/strength\n",
    "    return {\n",
    "        'signal': signal,\n",
    "        'power_spectrum': power_spectrum,\n",
    "        'energy_conservation_verified': abs(freq_energy/time_energy - 1.0) < 1e-10\n",
    "    }\n",
    "\n",
    "demonstrate_parsevals_theorem()\n",
    "```\n",
    "\n",
    "**Mathematical Insight:**\n",
    "- **|X[k]|²** represents the **energy** at frequency k\n",
    "- **Relative magnitudes** show **relative importance** of different cycles\n",
    "- **Total energy is conserved** between time and frequency domains\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "### **4. PHASE ANALYSIS: Timing and Synchronization**\n",
    "\n",
    "#### **A. Mathematical Foundation: Complex Representation**\n",
    "\n",
    "**Phase Formula:** **φ[k] = arctan(Imag[X[k]] / Real[X[k]])**\n",
    "\n",
    "**Why phase reveals timing:**\n",
    "\n",
    "The DFT represents each frequency component as: **A[k] × e^(iφ[k])** where:\n",
    "- **A[k] = |X[k]|** is the amplitude (strength)\n",
    "- **φ[k]** is the phase (timing offset)\n",
    "\n",
    "```python\n",
    "def explain_phase_mathematics():\n",
    "    \"\"\"\n",
    "    Mathematical explanation of how phase reveals timing information\n",
    "    \"\"\"\n",
    "    \n",
    "    N = 100\n",
    "    n = np.arange(N)\n",
    "    \n",
    "    # Create identical cycles with different phase shifts\n",
    "    reference_cycle = np.cos(2 * np.pi * n / 20)                    # Phase = 0\n",
    "    shifted_90 = np.cos(2 * np.pi * n / 20 + np.pi/2)             # Phase = π/2 (90°)\n",
    "    shifted_180 = np.cos(2 * np.pi * n / 20 + np.pi)              # Phase = π (180°)\n",
    "    shifted_270 = np.cos(2 * np.pi * n / 20 + 3*np.pi/2)          # Phase = 3π/2 (270°)\n",
    "    \n",
    "    cycles = {\n",
    "        'reference (0°)': (reference_cycle, 0),\n",
    "        'shifted 90°': (shifted_90, np.pi/2),\n",
    "        'shifted 180°': (shifted_180, np.pi),\n",
    "        'shifted 270°': (shifted_270, 3*np.pi/2)\n",
    "    }\n",
    "    \n",
    "    print(\"=== PHASE MATHEMATICS: TIMING DETECTION ===\")\n",
    "    print(\"Identical 20-period cycles with different phase shifts:\")\n",
    "    \n",
    "    for name, (signal, expected_phase) in cycles.items():\n",
    "        # Calculate DFT for the dominant frequency\n",
    "        k = N // 20  # Frequency bin for 20-period cycle\n",
    "        \n",
    "        # Manual DFT calculation\n",
    "        dft_value = np.sum(signal * np.exp(-1j * 2 * np.pi * k * n / N))\n",
    "        \n",
    "        magnitude = abs(dft_value)\n",
    "        phase = np.angle(dft_value)\n",
    "        \n",
    "        # Convert to degrees for readability\n",
    "        phase_degrees = np.degrees(phase)\n",
    "        expected_degrees = np.degrees(expected_phase)\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  DFT value: {dft_value:.3f}\")\n",
    "        print(f\"  Magnitude: {magnitude:.3f}\")\n",
    "        print(f\"  Phase: {phase:.3f} rad ({phase_degrees:.1f}°)\")\n",
    "        print(f\"  Expected: {expected_phase:.3f} rad ({expected_degrees:.1f}°)\")\n",
    "        print(f\"  Error: {abs(phase - expected_phase):.6f} rad\")\n",
    "    \n",
    "    return cycles\n",
    "\n",
    "explain_phase_mathematics()\n",
    "```\n",
    "\n",
    "#### **B. Phase Relationships and Timing**\n",
    "\n",
    "**Mathematical Relationship:**\n",
    "For a sinusoid **A×cos(2πft + φ)**, the phase **φ** determines:\n",
    "- **When the peak occurs** in the cycle\n",
    "- **Synchronization** between different components\n",
    "- **Lead/lag relationships** between signals\n",
    "\n",
    "```python\n",
    "def demonstrate_phase_timing_relationship():\n",
    "    \"\"\"\n",
    "    Show how phase translates to actual timing in the signal\n",
    "    \"\"\"\n",
    "    \n",
    "    N = 120  # 120 time points\n",
    "    n = np.arange(N)\n",
    "    period = 24  # 24-period cycle\n",
    "    \n",
    "    # Create cycles with different phases\n",
    "    phases = [0, np.pi/4, np.pi/2, np.pi, 3*np.pi/2]\n",
    "    phase_names = ['0°', '45°', '90°', '180°', '270°']\n",
    "    \n",
    "    print(\"=== PHASE TO TIMING TRANSLATION ===\")\n",
    "    print(f\"24-period cycle, N={N} points\")\n",
    "    \n",
    "    for phase, name in zip(phases, phase_names):\n",
    "        # Create signal\n",
    "        signal = np.cos(2 * np.pi * n / period + phase)\n",
    "        \n",
    "        # Find peak location\n",
    "        peak_index = np.argmax(signal)\n",
    "        peak_time = peak_index\n",
    "        \n",
    "        # Calculate expected peak time from phase\n",
    "        # Phase shift φ corresponds to time shift of φ/(2π) * period\n",
    "        expected_peak_time = (-phase / (2 * np.pi)) * period\n",
    "        if expected_peak_time < 0:\n",
    "            expected_peak_time += period  # Wrap around\n",
    "        \n",
    "        print(f\"\\nPhase {name} ({phase:.3f} rad):\")\n",
    "        print(f\"  Peak occurs at time: {peak_time}\")\n",
    "        print(f\"  Expected from phase: {expected_peak_time:.1f}\")\n",
    "        print(f\"  Time shift: {expected_peak_time:.1f} time units\")\n",
    "        print(f\"  Fraction of cycle: {expected_peak_time/period:.3f}\")\n",
    "    \n",
    "    # Mathematical insight: Phase directly encodes timing offset\n",
    "    print(f\"\\nMathematical relationship:\")\n",
    "    print(f\"  Time shift = -φ/(2π) × Period\")\n",
    "    print(f\"  Phase φ = -2π × (time_shift/Period)\")\n",
    "\n",
    "demonstrate_phase_timing_relationship()\n",
    "```\n",
    "\n",
    "**Key Mathematical Insight:**\n",
    "- **Phase encodes timing offset** within the cycle\n",
    "- **Phase differences** reveal synchronization between patterns\n",
    "- **Negative phase** = signal **leads** the reference\n",
    "- **Positive phase** = signal **lags** the reference\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "### **5. TREND DETECTION & NOISE SEPARATION: Frequency Localization**\n",
    "\n",
    "#### **A. Mathematical Foundation: DC Component and Low Frequencies**\n",
    "\n",
    "**DC Component:** **X[0] = Σ(n=0 to N-1) x[n]** (the mean)\n",
    "\n",
    "**Low Frequencies:** **X[1], X[2], ...** represent **slow variations** (trends)\n",
    "\n",
    "**High Frequencies:** **X[N/2-1], X[N/2-2], ...** represent **rapid variations** (noise)\n",
    "\n",
    "```python\n",
    "def explain_trend_noise_separation():\n",
    "    \"\"\"\n",
    "    Mathematical explanation of trend and noise separation\n",
    "    \"\"\"\n",
    "    \n",
    "    N = 200\n",
    "    n = np.arange(N)\n",
    "    \n",
    "    # Create signal components\n",
    "    trend = 0.02 * n  # Linear trend\n",
    "    seasonal = np.cos(2 * np.pi * n / 50)  # 50-period seasonal\n",
    "    noise = 0.3 * np.random.normal(0, 1, N)  # High-frequency noise\n",
    "    \n",
    "    # Combined signal\n",
    "    signal = trend + seasonal + noise\n",
    "    \n",
    "    # Calculate DFT\n",
    "    X = np.fft.fft(signal)\n",
    "    frequencies = np.fft.fftfreq(N)\n",
    "    magnitude = np.abs(X)\n",
    "    \n",
    "    print(\"=== TREND AND NOISE SEPARATION MATHEMATICS ===\")\n",
    "    \n",
    "    # 1. DC COMPONENT (Trend baseline)\n",
    "    dc_component = X[0] / N  # Average value\n",
    "    print(f\"DC component (mean): {dc_component:.6f}\")\n",
    "    print(f\"Actual mean: {np.mean(signal):.6f}\")\n",
    "    \n",
    "    # 2. LOW FREQUENCY ANALYSIS (Trend)\n",
    "    low_freq_bins = 5  # First 5 frequency bins (excluding DC)\n",
    "    low_freq_power = np.sum(magnitude[1:low_freq_bins+1]**2)\n",
    "    total_power = np.sum(magnitude**2)\n",
    "    trend_power_ratio = low_freq_power / total_power\n",
    "    \n",
    "    print(f\"\\nLow frequency analysis (trend detection):\")\n",
    "    print(f\"  Low frequency power: {low_freq_power:.1f}\")\n",
    "    print(f\"  Total power: {total_power:.1f}\")\n",
    "    print(f\"  Trend power ratio: {trend_power_ratio:.3f}\")\n",
    "    \n",
    "    # 3. HIGH FREQUENCY ANALYSIS (Noise)\n",
    "    high_freq_threshold = N // 4  # Upper quarter of frequencies\n",
    "    high_freq_power = np.sum(magnitude[high_freq_threshold:]**2)\n",
    "    noise_power_ratio = high_freq_power / total_power\n",
    "    \n",
    "    print(f\"\\nHigh frequency analysis (noise detection):\")\n",
    "    print(f\"  High frequency power: {high_freq_power:.1f}\")\n",
    "    print(f\"  Noise power ratio: {noise_power_ratio:.3f}\")\n",
    "    \n",
    "    # 4. SIGNAL RECONSTRUCTION (Filtering)\n",
    "    # Reconstruct trend (DC + low frequencies)\n",
    "    X_trend = np.zeros_like(X)\n",
    "    X_trend[0] = X[0]  # DC\n",
    "    X_trend[1:low_freq_bins+1] = X[1:low_freq_bins+1]  # Low frequencies\n",
    "    X_trend[-low_freq_bins:] = X[-low_freq_bins:]  # Negative frequencies (symmetry)\n",
    "    \n",
    "    trend_reconstructed = np.real(np.fft.ifft(X_trend))\n",
    "    \n",
    "    # Reconstruct seasonal (mid frequencies)\n",
    "    seasonal_bin = N // 50  # 50-period cycle\n",
    "    X_seasonal = np.zeros_like(X)\n",
    "    X_seasonal[seasonal_bin] = X[seasonal_bin]\n",
    "    X_seasonal[-seasonal_bin] = X[-seasonal_bin]  # Negative frequency\n",
    "    \n",
    "    seasonal_reconstructed = np.real(np.fft.ifft(X_seasonal))\n",
    "    \n",
    "    print(f\"\\nSignal reconstruction quality:\")\n",
    "    print(f\"  Original trend correlation: {np.corrcoef(trend, trend_reconstructed)[0,1]:.3f}\")\n",
    "    print(f\"  Original seasonal correlation: {np.corrcoef(seasonal, seasonal_reconstructed)[0,1]:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'original_signal': signal,\n",
    "        'components': {'trend': trend, 'seasonal': seasonal, 'noise': noise},\n",
    "        'reconstructed': {'trend': trend_reconstructed, 'seasonal': seasonal_reconstructed},\n",
    "        'frequency_analysis': {\n",
    "            'dc_component': dc_component,\n",
    "            'trend_power_ratio': trend_power_ratio,\n",
    "            'noise_power_ratio': noise_power_ratio\n",
    "        }\n",
    "    }\n",
    "\n",
    "result = explain_trend_noise_separation()\n",
    "```\n",
    "\n",
    "#### **B. Mathematical Properties Enabling Separation**\n",
    "\n",
    "**1. Frequency Localization:**\n",
    "- **Trends** → **Low frequencies** (slow changes)\n",
    "- **Seasonality** → **Specific frequencies** (regular cycles)  \n",
    "- **Noise** → **High frequencies** (rapid, random changes)\n",
    "\n",
    "**2. Filtering Mathematics:**\n",
    "```python\n",
    "def mathematical_filtering_explanation():\n",
    "    \"\"\"\n",
    "    Show the mathematical basis for frequency-domain filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== MATHEMATICAL BASIS FOR FILTERING ===\")\n",
    "    \n",
    "    # Mathematical relationships:\n",
    "    print(\"1. FREQUENCY-TIME RELATIONSHIP:\")\n",
    "    print(\"   Low frequency ↔ Slow time variation (trends)\")\n",
    "    print(\"   High frequency ↔ Fast time variation (noise)\")\n",
    "    print(\"   f = k/N, where k is frequency bin, N is signal length\")\n",
    "    \n",
    "    print(\"\\n2. FILTERING OPERATIONS:\")\n",
    "    print(\"   Low-pass filter: Keep X[k] for k ≤ k_cutoff\")\n",
    "    print(\"   High-pass filter: Keep X[k] for k ≥ k_cutoff\") \n",
    "    print(\"   Band-pass filter: Keep X[k] for k₁ ≤ k ≤ k₂\")\n",
    "    \n",
    "    print(\"\\n3. RECONSTRUCTION FORMULA:\")\n",
    "    print(\"   x_filtered[n] = IFFT{H[k] × X[k]}\")\n",
    "    print(\"   where H[k] is the filter function\")\n",
    "    \n",
    "    # Example filter functions\n",
    "    N = 100\n",
    "    k = np.arange(N)\n",
    "    \n",
    "    # Low-pass filter (keeps trends)\n",
    "    cutoff_low = 5\n",
    "    H_lowpass = np.zeros(N)\n",
    "    H_lowpass[:cutoff_low] = 1\n",
    "    H_lowpass[-cutoff_low:] = 1  # Symmetric for real signals\n",
    "    \n",
    "    # High-pass filter (keeps noise/rapid changes)\n",
    "    H_highpass = 1 - H_lowpass\n",
    "    \n",
    "    # Band-pass filter (keeps specific frequency range)\n",
    "    H_bandpass = np.zeros(N)\n",
    "    H_bandpass[10:15] = 1  # Keep frequencies 10-14\n",
    "    H_bandpass[-15:-10] = 1  # Symmetric\n",
    "    \n",
    "    print(f\"\\n4. EXAMPLE FILTERS (N={N}):\")\n",
    "    print(f\"   Low-pass: Keeps frequencies 0-{cutoff_low} (trends)\")\n",
    "    print(f\"   High-pass: Keeps frequencies {cutoff_low}-{N//2} (noise)\")\n",
    "    print(f\"   Band-pass: Keeps frequencies 10-14 (specific cycles)\")\n",
    "    \n",
    "    return {\n",
    "        'filter_functions': {\n",
    "            'lowpass': H_lowpass,\n",
    "            'highpass': H_highpass, \n",
    "            'bandpass': H_bandpass\n",
    "        }\n",
    "    }\n",
    "\n",
    "mathematical_filtering_explanation()\n",
    "```\n",
    "\n",
    "**Key Mathematical Insight:**\n",
    "- **Frequency domain separation** enables **perfect filtering**\n",
    "- **Different signal components** occupy **different frequency ranges**\n",
    "- **Linear filtering** preserves signal structure while removing unwanted components\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "### **6. FORECASTING: Reconstruction and Extrapolation**\n",
    "\n",
    "#### **A. Mathematical Foundation: Inverse Transform and Extrapolation**\n",
    "\n",
    "**Reconstruction Formula:** **x[n] = (1/N) × Σ(k=0 to N-1) X[k] × e^(i2πkn/N)**\n",
    "\n",
    "**Forecasting Principle:** If we know **X[k]**, **A[k]**, and **φ[k]** for dominant frequencies, we can **extrapolate** beyond the observed data.\n",
    "\n",
    "```python\n",
    "def explain_forecasting_mathematics():\n",
    "    \"\"\"\n",
    "    Mathematical explanation of how Fourier analysis enables forecasting\n",
    "    \"\"\"\n",
    "    \n",
    "    N = 100\n",
    "    n = np.arange(N)\n",
    "    \n",
    "    # Create signal with known components\n",
    "    true_component1 = 2 * np.cos(2 * np.pi * n / 20)  # 20-period cycle\n",
    "    true_component2 = 1 * np.cos(2 * np.pi * n / 10 + np.pi/4)  # 10-period cycle with phase\n",
    "    noise = 0.2 * np.random.normal(0, 1, N)\n",
    "    \n",
    "    observed_signal = true_component1 + true_component2 + noise\n",
    "    \n",
    "    print(\"=== FORECASTING MATHEMATICS ===\")\n",
    "    \n",
    "    # 1. ANALYZE OBSERVED DATA\n",
    "    X = np.fft.fft(observed_signal)\n",
    "    frequencies = np.fft.fftfreq(N)\n",
    "    magnitude = np.abs(X)\n",
    "    phase = np.angle(X)\n",
    "    \n",
    "    # Find dominant frequencies\n",
    "    # Exclude DC and very high frequencies\n",
    "    search_range = slice(1, N//4)\n",
    "    dominant_indices = np.argsort(magnitude[search_range])[-3:] + 1  # Top 3 peaks\n",
    "    \n",
    "    print(\"Dominant frequency components detected:\")\n",
    "    for i, k in enumerate(dominant_indices):\n",
    "        period = N / k if k > 0 else np.inf\n",
    "        freq = frequencies[k]\n",
    "        mag = magnitude[k]\n",
    "        ph = phase[k]\n",
    "        \n",
    "        print(f\"  Component {i+1}:\")\n",
    "        print(f\"    Frequency bin k={k}, Period≈{period:.1f}\")\n",
    "        print(f\"    Magnitude: {mag:.3f}, Phase: {ph:.3f} rad\")\n",
    "    \n",
    "    # 2. MATHEMATICAL FORECASTING FORMULA\n",
    "    print(f\"\\n=== FORECASTING FORMULA ===\")\n",
    "    print(\"For each dominant component k:\")\n",
    "    print(\"  x_forecast[n] = (2/N) × |X[k]| × cos(2πkn/N + φ[k])\")\n",
    "    print(\"Combined forecast:\")\n",
    "    print(\"  x_total[n] = Σ x_forecast_k[n] + DC_component\")\n",
    "    \n",
    "    # 3. GENERATE FORECAST\n",
    "    forecast_length = 50  # Forecast 50 points into the future\n",
    "    n_extended = np.arange(N, N + forecast_length)\n",
    "    \n",
    "    # Reconstruct each dominant component\n",
    "    forecast = np.zeros(forecast_length)\n",
    "    \n",
    "    # Add DC component\n",
    "    dc_component = np.real(X[0]) / N\n",
    "    forecast += dc_component\n",
    "    \n",
    "    print(f\"\\nForecasting {forecast_length} points beyond observed data:\")\n",
    "    \n",
    "    for k in dominant_indices:\n",
    "        if k < N//2:  # Only positive frequencies\n",
    "            # Extract component parameters\n",
    "            amplitude = 2 * magnitude[k] / N  # Factor of 2 for real signals\n",
    "            frequency = k / N  # Normalized frequency\n",
    "            phase_shift = phase[k]\n",
    "            \n",
    "            # Generate forecast for this component\n",
    "            component_forecast = amplitude * np.cos(2 * np.pi * frequency * n_extended + phase_shift)\n",
    "            forecast += component_forecast\n",
    "            \n",
    "            print(f\"  Component k={k}: amplitude={amplitude:.3f}, freq={frequency:.4f}, phase={phase_shift:.3f}\")\n",
    "    \n",
    "    # 4. VALIDATION: Compare with true continuation\n",
    "    true_continuation1 = 2 * np.cos(2 * np.pi * n_extended / 20)\n",
    "    true_continuation2 = 1 * np.cos(2 * np.pi * n_extended / 10 + np.pi/4)\n",
    "    true_forecast = true_continuation1 + true_continuation2 + dc_component\n",
    "    \n",
    "    # Calculate forecast accuracy\n",
    "    forecast_error = np.mean((forecast - true_forecast)**2)\n",
    "    forecast_correlation = np.corrcoef(forecast, true_forecast)[0, 1]\n",
    "    \n",
    "    print(f\"\\nForecast validation:\")\n",
    "    print(f\"  MSE: {forecast_error:.6f}\")\n",
    "    print(f\"  Correlation with true signal: {forecast_correlation:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'observed_signal': observed_signal,\n",
    "        'forecast': forecast,\n",
    "        'true_forecast': true_forecast,\n",
    "        'dominant_components': dominant_indices,\n",
    "        'forecast_accuracy': {\n",
    "            'mse': forecast_error,\n",
    "            'correlation': forecast_correlation\n",
    "        }\n",
    "    }\n",
    "\n",
    "forecast_result = explain_forecasting_mathematics()\n",
    "```\n",
    "\n",
    "#### **B. Mathematical Properties Enabling Forecasting**\n",
    "\n",
    "**1. Periodicity Assumption:**\n",
    "If **X[k]** is significant, the corresponding sinusoid **continues indefinitely**:\n",
    "**x_future[n] = (2/N) × |X[k]| × cos(2πk(n+N)/N + φ[k])**\n",
    "\n",
    "**2. Stationarity Assumption:**\n",
    "The **frequency content remains constant** over time.\n",
    "\n",
    "**3. Superposition Principle:**\n",
    "**Total forecast = Σ(individual component forecasts)**\n",
    "\n",
    "```python\n",
    "def demonstrate_forecasting_limitations():\n",
    "    \"\"\"\n",
    "    Show mathematical limitations and assumptions of Fourier forecasting\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== MATHEMATICAL ASSUMPTIONS FOR FORECASTING ===\")\n",
    "    \n",
    "    # 1. STATIONARITY REQUIREMENT\n",
    "    print(\"1. STATIONARITY ASSUMPTION:\")\n",
    "    print(\"   Mathematical requirement: E[x(t)] = constant, Var[x(t)] = constant\")\n",
    "    print(\"   Implication: Frequency content doesn't change over time\")\n",
    "    print(\"   Violation example: Trends, changing seasonality\")\n",
    "    \n",
    "    # 2. PERIODICITY ASSUMPTION  \n",
    "    print(\"\\n2. PERIODICITY ASSUMPTION:\")\n",
    "    print(\"   Mathematical basis: sin(ωt) and cos(ωt) are periodic\")\n",
    "    print(\"   Implication: Detected cycles continue indefinitely\")\n",
    "    print(\"   Violation example: Damped oscillations, one-time events\")\n",
    "    \n",
    "    # 3. LINEARITY ASSUMPTION\n",
    "    print(\"\\n3. LINEARITY ASSUMPTION:\")\n",
    "    print(\"   Mathematical basis: DFT is linear transformation\")\n",
    "    print(\"   Implication: Components combine additively\")\n",
    "    print(\"   Violation example: Nonlinear interactions, threshold effects\")\n",
    "    \n",
    "    # Example: Non-stationary signal (changing frequency)\n",
    "    N = 200\n",
    "    n = np.arange(N)\n",
    "    \n",
    "    # Frequency changes over time (chirp signal)\n",
    "    instantaneous_freq = 0.05 + 0.02 * n / N  # Frequency increases\n",
    "    phase = np.cumsum(2 * np.pi * instantaneous_freq)\n",
    "    chirp_signal = np.cos(phase)\n",
    "    \n",
    "    # Try to forecast this with standard Fourier method\n",
    "    X = np.fft.fft(chirp_signal)\n",
    "    magnitude = np.abs(X)\n",
    "    \n",
    "    # This will fail because the signal is non-stationary\n",
    "    dominant_k = np.argmax(magnitude[1:N//2]) + 1\n",
    "    \n",
    "    print(f\"\\n=== NON-STATIONARY EXAMPLE ===\")\n",
    "    print(f\"Chirp signal with changing frequency\")\n",
    "    print(f\"Dominant frequency bin detected: {dominant_k}\")\n",
    "    print(f\"But actual frequency changes from {0.05:.3f} to {0.05+0.02:.3f}\")\n",
    "    print(f\"Standard Fourier forecast will be inaccurate!\")\n",
    "    \n",
    "    # 4. MATHEMATICAL SOLUTIONS FOR LIMITATIONS\n",
    "    print(f\"\\n=== MATHEMATICAL EXTENSIONS ===\")\n",
    "    print(\"1. Short-Time Fourier Transform (STFT):\")\n",
    "    print(\"   X(m,k) = Σ x[n]w[n-m]e^(-i2πkn/N)\")\n",
    "    print(\"   Captures time-varying frequency content\")\n",
    "    \n",
    "    print(\"\\n2. Wavelet Transform:\")\n",
    "    print(\"   W(a,b) = ∫ x(t)ψ*((t-b)/a)dt\")\n",
    "    print(\"   Better time-frequency localization\")\n",
    "    \n",
    "    print(\"\\n3. Adaptive Filtering:\")\n",
    "    print(\"   Update frequency estimates as new data arrives\")\n",
    "    print(\"   Handle non-stationary behavior\")\n",
    "    \n",
    "    return {\n",
    "        'chirp_signal': chirp_signal,\n",
    "        'limitation_demonstrated': True\n",
    "    }\n",
    "\n",
    "demonstrate_forecasting_limitations()\n",
    "```\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **MATHEMATICAL SUMMARY: Why Fourier Analysis Works**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"DFT Formula<br/>X[k] = Σ x[n]e^(-i2πkn/N)\"] --> B[\"Complex Exponentials<br/>e^(-i2πkn/N)\"]\n",
    "    \n",
    "    B --> C[\"Orthogonality Property<br/>⟨e^(ik₁), e^(ik₂)⟩ = 0 if k₁≠k₂\"]\n",
    "    B --> D[\"Correlation Computation<br/>Inner product with basis functions\"]\n",
    "    \n",
    "    C --> E[\"Multiple Cycle Separation<br/>Independent frequency bins\"]\n",
    "    D --> F[\"Pattern Detection<br/>High correlation = Strong pattern\"]\n",
    "    \n",
    "    A --> G[\"Magnitude |X[k]|<br/>= √(Real² + Imag²)\"]\n",
    "    A --> H[\"Phase φ[k]<br/>= arctan(Imag/Real)\"]\n",
    "    \n",
    "    G --> I[\"Strength Quantification<br/>Energy at each frequency\"]\n",
    "    H --> J[\"Timing Information<br/>Phase offset within cycle\"]\n",
    "    \n",
    "    A --> K[\"Frequency Localization<br/>Low k = Trends, High k = Noise\"]\n",
    "    K --> L[\"Filtering Capability<br/>Separate signal components\"]\n",
    "    \n",
    "    A --> M[\"Inverse Transform<br/>x[n] = (1/N)Σ X[k]e^(i2πkn/N)\"]\n",
    "    M --> N[\"Reconstruction & Forecasting<br/>Extrapolate periodic components\"]\n",
    "    \n",
    "    style A fill:#e1f5fe,color:#000\n",
    "    style E fill:#c8e6c9,color:#000\n",
    "    style F fill:#c8e6c9,color:#000\n",
    "    style I fill:#fff3e0,color:#000\n",
    "    style J fill:#fff3e0,color:#000\n",
    "    style L fill:#f3e5f5,color:#000\n",
    "    style N fill:#ffebee,color:#000\n",
    "```\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "### **CORE MATHEMATICAL PRINCIPLES**\n",
    "\n",
    "| **Property** | **Mathematical Foundation** | **Why It Works** |\n",
    "|--------------|----------------------------|------------------|\n",
    "| **Pattern Detection** | **Correlation**: X[k] = ⟨x, e^(ikω)⟩ | High correlation with sinusoid = Strong pattern |\n",
    "| **Multiple Cycles** | **Orthogonality**: ⟨e^(ik₁), e^(ik₂)⟩ = δ_{k₁,k₂} | Different frequencies don't interfere |\n",
    "| **Strength Quantification** | **Parseval's Theorem**: Σ\\|x[n]\\|² = (1/N)Σ\\|X[k]\\|² | Energy conservation enables comparison |\n",
    "| **Phase/Timing** | **Complex Representation**: X[k] = \\|X[k]\\|e^(iφ[k]) | Phase encodes timing offset |\n",
    "| **Trend Detection** | **Frequency Localization**: Low k ↔ Slow variation | Trends = Low frequency components |\n",
    "| **Noise Separation** | **Frequency Localization**: High k ↔ Fast variation | Noise = High frequency components |\n",
    "| **Forecasting** | **Periodicity**: sin(ωt) repeats indefinitely | Sinusoidal components extrapolate naturally |\n",
    "\n",
    "### **KEY MATHEMATICAL INSIGHTS**\n",
    "\n",
    "1. **Orthogonal Basis Decomposition**: DFT decomposes signals into orthogonal sinusoidal components\n",
    "2. **Energy Conservation**: Total signal energy is preserved in frequency domain\n",
    "3. **Frequency-Time Duality**: Slow time variations ↔ Low frequencies, Fast variations ↔ High frequencies  \n",
    "4. **Complex Representation**: Magnitude + Phase captures both strength and timing\n",
    "5. **Linear Superposition**: Multiple patterns combine additively\n",
    "6. **Inverse Transform**: Perfect reconstruction enables forecasting\n",
    "\n",
    "**The mathematical elegance of Fourier analysis lies in its ability to transform complex temporal patterns into simple, interpretable frequency components, each with clear physical meaning and predictive power.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
