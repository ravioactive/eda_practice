{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3203539c",
   "metadata": {},
   "source": [
    "I'll dive deep into Advanced Pattern Recognition for categorical data, examining the decision-making criteria and analyzing your specific dataset. Let me first examine your current distribution analysis notebook and the dataset structure.\n",
    "\n",
    "[2 tools called]\n",
    "\n",
    "Now let me analyze the dataset structure and create a comprehensive framework for Advanced Pattern Recognition in categorical data analysis.\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **Advanced Pattern Recognition Analysis Framework**\n",
    "\n",
    "Based on my analysis of your Mall_Customers dataset, let me provide a comprehensive framework for temporal and spatial pattern recognition in categorical data.\n",
    "\n",
    "### **Dataset Analysis - Mall_Customers**\n",
    "\n",
    "**Available Variables:**\n",
    "- `CustomerID` (identifier - not categorical for analysis)\n",
    "- `Gender` (categorical: Male/Female)\n",
    "- `Age` (numerical - can be categorized)\n",
    "- `Annual Income (k$)` (numerical - can be categorized)\n",
    "- `Spending Score (1-100)` (numerical - can be categorized)\n",
    "\n",
    "**Key Observations:**\n",
    "- **No explicit temporal dimension** (no timestamps/dates)\n",
    "- **No spatial/geographic dimension** (no location data)\n",
    "- **Cross-sectional data** (snapshot at one point in time)\n",
    "- **Demographic and behavioral variables** available for pattern analysis\n",
    "\n",
    "[2 tools called]\n",
    "\n",
    "## **1. TEMPORAL PATTERN ANALYSIS FRAMEWORK**\n",
    "\n",
    "### **A. Seasonal Analysis Methods**\n",
    "\n",
    "**1. Classical Seasonal Decomposition**\n",
    "```python\n",
    "# For categorical time series\n",
    "def seasonal_categorical_analysis(df, cat_col, time_col, period=12):\n",
    "    \"\"\"\n",
    "    Analyze seasonal patterns in categorical data\n",
    "    \"\"\"\n",
    "    # Create time-based contingency table\n",
    "    df['period'] = df[time_col].dt.month  # or quarter, day_of_week\n",
    "    contingency = pd.crosstab(df['period'], df[cat_col])\n",
    "    \n",
    "    # Chi-square test for seasonal independence\n",
    "    chi2, p_value, dof, expected = stats.chi2_contingency(contingency)\n",
    "    \n",
    "    # Seasonal concentration index\n",
    "    seasonal_entropy = []\n",
    "    for period in contingency.index:\n",
    "        period_dist = contingency.loc[period] / contingency.loc[period].sum()\n",
    "        entropy = -sum(period_dist * np.log2(period_dist + 1e-10))\n",
    "        seasonal_entropy.append(entropy)\n",
    "    \n",
    "    return {\n",
    "        'contingency_table': contingency,\n",
    "        'chi2_stat': chi2,\n",
    "        'p_value': p_value,\n",
    "        'seasonal_entropy': seasonal_entropy,\n",
    "        'seasonal_variation': np.std(seasonal_entropy)\n",
    "    }\n",
    "```\n",
    "\n",
    "**2. Fourier Analysis for Categorical Cycles**\n",
    "```python\n",
    "def categorical_fourier_analysis(df, cat_col, time_col):\n",
    "    \"\"\"\n",
    "    Detect cyclical patterns using Fourier transform on categorical proportions\n",
    "    \"\"\"\n",
    "    # Convert categorical to proportions over time\n",
    "    time_series = df.groupby(time_col)[cat_col].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    \n",
    "    # Apply FFT to each category's time series\n",
    "    fft_results = {}\n",
    "    for category in time_series.columns:\n",
    "        fft = np.fft.fft(time_series[category].values)\n",
    "        frequencies = np.fft.fftfreq(len(fft))\n",
    "        fft_results[category] = {\n",
    "            'fft': fft,\n",
    "            'frequencies': frequencies,\n",
    "            'dominant_freq': frequencies[np.argmax(np.abs(fft[1:]))+1]\n",
    "        }\n",
    "    \n",
    "    return fft_results\n",
    "```\n",
    "\n",
    "**3. Autocorrelation Analysis**\n",
    "```python\n",
    "def categorical_autocorrelation(df, cat_col, time_col, max_lag=12):\n",
    "    \"\"\"\n",
    "    Measure temporal autocorrelation in categorical sequences\n",
    "    \"\"\"\n",
    "    # Convert to numerical encoding\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    df['encoded'] = le.fit_transform(df[cat_col])\n",
    "    \n",
    "    # Calculate autocorrelation\n",
    "    autocorr = [df['encoded'].autocorr(lag=lag) for lag in range(1, max_lag+1)]\n",
    "    \n",
    "    # Ljung-Box test for randomness\n",
    "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "    lb_stat, lb_pvalue = acorr_ljungbox(df['encoded'], lags=max_lag, return_df=False)\n",
    "    \n",
    "    return {\n",
    "        'autocorrelations': autocorr,\n",
    "        'ljung_box_stat': lb_stat,\n",
    "        'ljung_box_pvalue': lb_pvalue\n",
    "    }\n",
    "```\n",
    "\n",
    "### **B. Trend Analysis Methods**\n",
    "\n",
    "**1. Cochran-Armitage Trend Test**\n",
    "```python\n",
    "def cochran_armitage_trend(df, cat_col, time_col, target_category):\n",
    "    \"\"\"\n",
    "    Test for linear trend in categorical proportions over time\n",
    "    \"\"\"\n",
    "    # Create time-ordered contingency table\n",
    "    time_groups = pd.cut(df[time_col], bins=10)  # or use natural time periods\n",
    "    contingency = pd.crosstab(time_groups, df[cat_col] == target_category)\n",
    "    \n",
    "    # Cochran-Armitage calculation\n",
    "    n = contingency.sum(axis=1).values\n",
    "    r = contingency.iloc[:, 1].values  # successes (target category)\n",
    "    x = np.arange(len(n))  # time scores\n",
    "    \n",
    "    # Calculate test statistic\n",
    "    N = n.sum()\n",
    "    R = r.sum()\n",
    "    x_bar = (x * n).sum() / N\n",
    "    \n",
    "    numerator = (x * r).sum() - R * x_bar\n",
    "    denominator = np.sqrt(R * (N - R) * ((x**2 * n).sum() - N * x_bar**2) / (N - 1))\n",
    "    \n",
    "    z_stat = numerator / denominator\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "    \n",
    "    return {'z_statistic': z_stat, 'p_value': p_value}\n",
    "```\n",
    "\n",
    "**2. Mann-Kendall Trend Test (Modified for Categorical)**\n",
    "```python\n",
    "def mann_kendall_categorical(df, cat_col, time_col):\n",
    "    \"\"\"\n",
    "    Modified Mann-Kendall test for categorical trend detection\n",
    "    \"\"\"\n",
    "    # Calculate category proportions over time periods\n",
    "    time_periods = pd.cut(df[time_col], bins=20)\n",
    "    proportions = df.groupby(time_periods)[cat_col].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    \n",
    "    results = {}\n",
    "    for category in proportions.columns:\n",
    "        series = proportions[category].dropna().values\n",
    "        n = len(series)\n",
    "        \n",
    "        # Calculate S statistic\n",
    "        S = 0\n",
    "        for i in range(n-1):\n",
    "            for j in range(i+1, n):\n",
    "                S += np.sign(series[j] - series[i])\n",
    "        \n",
    "        # Calculate variance\n",
    "        var_S = n * (n-1) * (2*n+5) / 18\n",
    "        \n",
    "        # Calculate Z statistic\n",
    "        if S > 0:\n",
    "            Z = (S - 1) / np.sqrt(var_S)\n",
    "        elif S < 0:\n",
    "            Z = (S + 1) / np.sqrt(var_S)\n",
    "        else:\n",
    "            Z = 0\n",
    "        \n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(Z)))\n",
    "        \n",
    "        results[category] = {\n",
    "            'S_statistic': S,\n",
    "            'Z_statistic': Z,\n",
    "            'p_value': p_value,\n",
    "            'trend': 'increasing' if Z > 0 else 'decreasing' if Z < 0 else 'no trend'\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "```\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **2. SPATIAL/GEOGRAPHIC PATTERN ANALYSIS FRAMEWORK**\n",
    "\n",
    "### **A. Spatial Distribution Analysis**\n",
    "\n",
    "**1. Moran's I for Categorical Spatial Autocorrelation**\n",
    "```python\n",
    "def morans_i_categorical(df, cat_col, lat_col, lon_col, distance_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Calculate Moran's I for categorical spatial autocorrelation\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import haversine_distances\n",
    "    \n",
    "    # Create spatial weights matrix\n",
    "    coords = df[[lat_col, lon_col]].values\n",
    "    distances = haversine_distances(np.radians(coords)) * 6371  # Earth radius in km\n",
    "    \n",
    "    # Binary spatial weights (1 if within threshold, 0 otherwise)\n",
    "    W = (distances <= distance_threshold).astype(int)\n",
    "    np.fill_diagonal(W, 0)  # No self-neighbors\n",
    "    \n",
    "    # Convert categorical to numerical for each category\n",
    "    results = {}\n",
    "    for category in df[cat_col].unique():\n",
    "        y = (df[cat_col] == category).astype(int).values\n",
    "        n = len(y)\n",
    "        y_mean = y.mean()\n",
    "        \n",
    "        # Calculate Moran's I\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        W_sum = W.sum()\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if W[i, j] > 0:\n",
    "                    numerator += W[i, j] * (y[i] - y_mean) * (y[j] - y_mean)\n",
    "            denominator += (y[i] - y_mean) ** 2\n",
    "        \n",
    "        if W_sum > 0 and denominator > 0:\n",
    "            morans_i = (n / W_sum) * (numerator / denominator)\n",
    "        else:\n",
    "            morans_i = 0\n",
    "        \n",
    "        # Expected value and variance (under null hypothesis)\n",
    "        expected_i = -1 / (n - 1)\n",
    "        var_i = (n**2 - 3*n + 3) * W_sum - n * (W**2).sum() + 3 * (W_sum**2)\n",
    "        var_i = var_i / ((n - 1) * (n - 2) * (n - 3) * (W_sum**2))\n",
    "        \n",
    "        # Z-score and p-value\n",
    "        z_score = (morans_i - expected_i) / np.sqrt(var_i) if var_i > 0 else 0\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
    "        \n",
    "        results[category] = {\n",
    "            'morans_i': morans_i,\n",
    "            'expected_i': expected_i,\n",
    "            'z_score': z_score,\n",
    "            'p_value': p_value,\n",
    "            'interpretation': 'clustered' if morans_i > expected_i else 'dispersed'\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "```\n",
    "\n",
    "**2. Getis-Ord G* Statistic for Hot/Cold Spots**\n",
    "```python\n",
    "def getis_ord_g_star(df, cat_col, lat_col, lon_col, distance_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Calculate Getis-Ord G* statistic to identify hot/cold spots\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import haversine_distances\n",
    "    \n",
    "    coords = df[[lat_col, lon_col]].values\n",
    "    distances = haversine_distances(np.radians(coords)) * 6371\n",
    "    \n",
    "    results = {}\n",
    "    for category in df[cat_col].unique():\n",
    "        y = (df[cat_col] == category).astype(int).values\n",
    "        n = len(y)\n",
    "        y_mean = y.mean()\n",
    "        y_std = y.std()\n",
    "        \n",
    "        g_star_values = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            # Define neighborhood\n",
    "            neighbors = distances[i] <= distance_threshold\n",
    "            neighbors[i] = True  # Include focal point\n",
    "            \n",
    "            # Calculate G*\n",
    "            w_sum = neighbors.sum()\n",
    "            if w_sum > 1:\n",
    "                numerator = (y[neighbors].sum() - y_mean * w_sum)\n",
    "                denominator = y_std * np.sqrt((n * w_sum - w_sum**2) / (n - 1))\n",
    "                g_star = numerator / denominator if denominator > 0 else 0\n",
    "            else:\n",
    "                g_star = 0\n",
    "            \n",
    "            g_star_values.append(g_star)\n",
    "        \n",
    "        # Identify significant hot/cold spots\n",
    "        critical_value = 1.96  # 95% confidence\n",
    "        hot_spots = np.array(g_star_values) > critical_value\n",
    "        cold_spots = np.array(g_star_values) < -critical_value\n",
    "        \n",
    "        results[category] = {\n",
    "            'g_star_values': g_star_values,\n",
    "            'hot_spots': hot_spots,\n",
    "            'cold_spots': cold_spots,\n",
    "            'n_hot_spots': hot_spots.sum(),\n",
    "            'n_cold_spots': cold_spots.sum()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "```\n",
    "\n",
    "**3. Ripley's K Function for Spatial Clustering**\n",
    "```python\n",
    "def ripleys_k_categorical(df, cat_col, lat_col, lon_col, max_distance=5.0, n_steps=20):\n",
    "    \"\"\"\n",
    "    Calculate Ripley's K function for categorical point patterns\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import haversine_distances\n",
    "    \n",
    "    results = {}\n",
    "    for category in df[cat_col].unique():\n",
    "        # Extract points for this category\n",
    "        category_df = df[df[cat_col] == category]\n",
    "        if len(category_df) < 2:\n",
    "            continue\n",
    "            \n",
    "        coords = category_df[[lat_col, lon_col]].values\n",
    "        n_points = len(coords)\n",
    "        \n",
    "        # Calculate distances between all pairs\n",
    "        distances = haversine_distances(np.radians(coords)) * 6371\n",
    "        \n",
    "        # Calculate K function at different distance thresholds\n",
    "        distance_steps = np.linspace(0, max_distance, n_steps)\n",
    "        k_values = []\n",
    "        \n",
    "        for d in distance_steps:\n",
    "            if d == 0:\n",
    "                k_values.append(0)\n",
    "                continue\n",
    "                \n",
    "            # Count pairs within distance d\n",
    "            pair_count = (distances <= d).sum() - n_points  # Exclude diagonal\n",
    "            \n",
    "            # Estimate study area (simplified as bounding box area)\n",
    "            lat_range = coords[:, 0].max() - coords[:, 0].min()\n",
    "            lon_range = coords[:, 1].max() - coords[:, 1].min()\n",
    "            area = lat_range * lon_range * (111.32 ** 2)  # Approximate kmÂ²\n",
    "            \n",
    "            # Calculate K(d)\n",
    "            k_d = (area * pair_count) / (n_points * (n_points - 1)) if area > 0 else 0\n",
    "            k_values.append(k_d)\n",
    "        \n",
    "        # Expected K under complete spatial randomness: K(d) = Ï€ * dÂ²\n",
    "        expected_k = np.pi * (distance_steps ** 2)\n",
    "        \n",
    "        # L function: L(d) = sqrt(K(d)/Ï€) - d\n",
    "        l_values = np.sqrt(np.array(k_values) / np.pi) - distance_steps\n",
    "        expected_l = np.zeros_like(distance_steps)\n",
    "        \n",
    "        results[category] = {\n",
    "            'distances': distance_steps,\n",
    "            'k_values': k_values,\n",
    "            'expected_k': expected_k,\n",
    "            'l_values': l_values,\n",
    "            'expected_l': expected_l,\n",
    "            'clustering_evidence': np.mean(l_values[1:]) > 0  # Positive L suggests clustering\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "```\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **3. DECISION CRITERIA MATRIX FOR TEST SELECTION**\n",
    "\n",
    "### **A. Data Property Assessment Framework**\n",
    "\n",
    "```python\n",
    "def assess_data_properties(df, cat_col, time_col=None, lat_col=None, lon_col=None):\n",
    "    \"\"\"\n",
    "    Assess data properties to determine appropriate pattern recognition tests\n",
    "    \"\"\"\n",
    "    properties = {\n",
    "        # Basic properties\n",
    "        'n_observations': len(df),\n",
    "        'n_categories': df[cat_col].nunique(),\n",
    "        'has_temporal_dimension': time_col is not None,\n",
    "        'has_spatial_dimension': lat_col is not None and lon_col is not None,\n",
    "        \n",
    "        # Temporal properties\n",
    "        'temporal_span': None,\n",
    "        'temporal_frequency': None,\n",
    "        'temporal_regularity': None,\n",
    "        \n",
    "        # Spatial properties\n",
    "        'spatial_extent': None,\n",
    "        'spatial_density': None,\n",
    "        'spatial_distribution': None,\n",
    "        \n",
    "        # Distribution properties\n",
    "        'balance_ratio': None,\n",
    "        'entropy': None,\n",
    "        'missing_data_pct': df[cat_col].isnull().mean()\n",
    "    }\n",
    "    \n",
    "    # Assess temporal properties\n",
    "    if time_col:\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "        properties['temporal_span'] = (df[time_col].max() - df[time_col].min()).days\n",
    "        \n",
    "        # Assess frequency\n",
    "        time_diffs = df[time_col].sort_values().diff().dropna()\n",
    "        properties['temporal_frequency'] = time_diffs.median().total_seconds() / 3600  # hours\n",
    "        properties['temporal_regularity'] = time_diffs.std().total_seconds() / time_diffs.mean().total_seconds()\n",
    "    \n",
    "    # Assess spatial properties\n",
    "    if lat_col and lon_col:\n",
    "        from sklearn.metrics.pairwise import haversine_distances\n",
    "        coords = df[[lat_col, lon_col]].dropna().values\n",
    "        if len(coords) > 1:\n",
    "            distances = haversine_distances(np.radians(coords)) * 6371\n",
    "            properties['spatial_extent'] = distances.max()\n",
    "            properties['spatial_density'] = len(coords) / (distances.max() ** 2) if distances.max() > 0 else 0\n",
    "    \n",
    "    # Assess distribution properties\n",
    "    value_counts = df[cat_col].value_counts()\n",
    "    properties['balance_ratio'] = value_counts.max() / value_counts.min() if len(value_counts) > 1 else 1\n",
    "    \n",
    "    proportions = value_counts / len(df)\n",
    "    properties['entropy'] = -sum(proportions * np.log2(proportions))\n",
    "    \n",
    "    return properties\n",
    "```\n",
    "\n",
    "### **B. Test Selection Decision Matrix**\n",
    "\n",
    "```python\n",
    "def select_appropriate_tests(properties):\n",
    "    \"\"\"\n",
    "    Select appropriate pattern recognition tests based on data properties\n",
    "    \"\"\"\n",
    "    recommendations = {\n",
    "        'temporal_tests': [],\n",
    "        'spatial_tests': [],\n",
    "        'general_tests': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    # TEMPORAL TEST SELECTION\n",
    "    if properties['has_temporal_dimension']:\n",
    "        # Sample size considerations\n",
    "        if properties['n_observations'] < 30:\n",
    "            recommendations['warnings'].append(\"Small sample size may affect temporal analysis reliability\")\n",
    "        \n",
    "        # Temporal span considerations\n",
    "        if properties['temporal_span'] and properties['temporal_span'] > 365:\n",
    "            recommendations['temporal_tests'].extend([\n",
    "                'seasonal_decomposition',\n",
    "                'fourier_analysis',\n",
    "                'cochran_armitage_trend'\n",
    "            ])\n",
    "        elif properties['temporal_span'] and properties['temporal_span'] > 30:\n",
    "            recommendations['temporal_tests'].extend([\n",
    "                'mann_kendall_trend',\n",
    "                'autocorrelation_analysis'\n",
    "            ])\n",
    "        else:\n",
    "            recommendations['warnings'].append(\"Short temporal span limits seasonal analysis\")\n",
    "        \n",
    "        # Frequency considerations\n",
    "        if properties['temporal_frequency'] and properties['temporal_frequency'] < 24:  # Sub-daily\n",
    "            recommendations['temporal_tests'].append('high_frequency_analysis')\n",
    "        elif properties['temporal_frequency'] and properties['temporal_frequency'] < 168:  # Sub-weekly\n",
    "            recommendations['temporal_tests'].append('daily_pattern_analysis')\n",
    "        \n",
    "        # Regularity considerations\n",
    "        if properties['temporal_regularity'] and properties['temporal_regularity'] > 0.5:\n",
    "            recommendations['warnings'].append(\"Irregular temporal sampling may affect time series analysis\")\n",
    "    \n",
    "    # SPATIAL TEST SELECTION\n",
    "    if properties['has_spatial_dimension']:\n",
    "        # Sample size considerations\n",
    "        if properties['n_observations'] < 20:\n",
    "            recommendations['warnings'].append(\"Small sample size may affect spatial analysis reliability\")\n",
    "        \n",
    "        # Spatial extent considerations\n",
    "        if properties['spatial_extent'] and properties['spatial_extent'] > 100:  # > 100 km\n",
    "            recommendations['spatial_tests'].extend([\n",
    "                'morans_i_autocorrelation',\n",
    "                'getis_ord_hotspots',\n",
    "                'ripleys_k_clustering'\n",
    "            ])\n",
    "        elif properties['spatial_extent'] and properties['spatial_extent'] > 10:  # 10-100 km\n",
    "            recommendations['spatial_tests'].extend([\n",
    "                'morans_i_autocorrelation',\n",
    "                'local_clustering_analysis'\n",
    "            ])\n",
    "        else:\n",
    "            recommendations['warnings'].append(\"Limited spatial extent may reduce spatial pattern detection\")\n",
    "        \n",
    "        # Density considerations\n",
    "        if properties['spatial_density'] and properties['spatial_density'] < 0.1:  # Low density\n",
    "            recommendations['warnings'].append(\"Low spatial density may affect clustering analysis\")\n",
    "    \n",
    "    # GENERAL DISTRIBUTION TESTS\n",
    "    # Balance considerations\n",
    "    if properties['balance_ratio'] > 10:\n",
    "        recommendations['general_tests'].extend([\n",
    "            'imbalance_analysis',\n",
    "            'minority_class_analysis'\n",
    "        ])\n",
    "    elif properties['balance_ratio'] > 2:\n",
    "        recommendations['general_tests'].append('moderate_imbalance_analysis')\n",
    "    \n",
    "    # Entropy considerations\n",
    "    max_entropy = np.log2(properties['n_categories'])\n",
    "    normalized_entropy = properties['entropy'] / max_entropy if max_entropy > 0 else 0\n",
    "    \n",
    "    if normalized_entropy < 0.5:\n",
    "        recommendations['general_tests'].append('concentration_analysis')\n",
    "    elif normalized_entropy > 0.9:\n",
    "        recommendations['general_tests'].append('uniformity_analysis')\n",
    "    \n",
    "    # Missing data considerations\n",
    "    if properties['missing_data_pct'] > 0.1:\n",
    "        recommendations['warnings'].append(\"High missing data percentage may bias pattern analysis\")\n",
    "    \n",
    "    return recommendations\n",
    "```\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **4. MALL_CUSTOMERS DATASET SPECIFIC ANALYSIS & RECOMMENDATIONS**\n",
    "\n",
    "### **A. Dataset Property Assessment**\n",
    "\n",
    "```python\n",
    "# Simulated analysis based on Mall_Customers dataset structure\n",
    "mall_customers_properties = {\n",
    "    'n_observations': 200,\n",
    "    'n_categories': 2,  # Gender: Male/Female\n",
    "    'has_temporal_dimension': False,  # No timestamp data\n",
    "    'has_spatial_dimension': False,   # No location data\n",
    "    'balance_ratio': 1.12,  # Relatively balanced gender distribution\n",
    "    'entropy': 0.99,  # High entropy (close to uniform)\n",
    "    'missing_data_pct': 0.0  # No missing data in Gender column\n",
    "}\n",
    "```\n",
    "\n",
    "### **B. Recommended Analysis Approach for Mall_Customers**\n",
    "\n",
    "**âŒ NOT APPLICABLE:**\n",
    "- **Temporal Pattern Analysis** - No time dimension available\n",
    "- **Spatial Pattern Analysis** - No geographic coordinates available\n",
    "\n",
    "**âœ… APPLICABLE ALTERNATIVES:**\n",
    "\n",
    "#### **1. Demographic Pattern Analysis**\n",
    "Since we lack temporal/spatial dimensions, focus on **cross-sectional demographic patterns**:\n",
    "\n",
    "```python\n",
    "def demographic_pattern_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyze demographic patterns in Mall_Customers dataset\n",
    "    \"\"\"\n",
    "    # Create age groups for pattern analysis\n",
    "    df['Age_Group'] = pd.cut(df['Age'], \n",
    "                            bins=[0, 25, 35, 50, 65, 100], \n",
    "                            labels=['Young', 'Young_Adult', 'Middle_Age', 'Senior', 'Elderly'])\n",
    "    \n",
    "    # Create income categories\n",
    "    df['Income_Category'] = pd.cut(df['Annual Income (k$)'], \n",
    "                                  bins=[0, 30, 60, 90, 150], \n",
    "                                  labels=['Low', 'Medium', 'High', 'Very_High'])\n",
    "    \n",
    "    # Create spending categories\n",
    "    df['Spending_Category'] = pd.cut(df['Spending Score (1-100)'], \n",
    "                                    bins=[0, 25, 50, 75, 100], \n",
    "                                    labels=['Low_Spender', 'Medium_Spender', 'High_Spender', 'Very_High_Spender'])\n",
    "    \n",
    "    # Analyze gender patterns across these dimensions\n",
    "    patterns = {}\n",
    "    \n",
    "    # Age-Gender patterns\n",
    "    age_gender = pd.crosstab(df['Age_Group'], df['Gender'], normalize='columns')\n",
    "    patterns['age_gender_distribution'] = age_gender\n",
    "    \n",
    "    # Income-Gender patterns\n",
    "    income_gender = pd.crosstab(df['Income_Category'], df['Gender'], normalize='columns')\n",
    "    patterns['income_gender_distribution'] = income_gender\n",
    "    \n",
    "    # Spending-Gender patterns\n",
    "    spending_gender = pd.crosstab(df['Spending_Category'], df['Gender'], normalize='columns')\n",
    "    patterns['spending_gender_distribution'] = spending_gender\n",
    "    \n",
    "    return patterns\n",
    "```\n",
    "\n",
    "#### **2. Behavioral Segmentation Pattern Analysis**\n",
    "```python\n",
    "def behavioral_segmentation_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyze behavioral patterns using customer segmentation approach\n",
    "    \"\"\"\n",
    "    # Create customer segments based on income and spending\n",
    "    df['Customer_Segment'] = 'Unknown'\n",
    "    \n",
    "    # Define segments\n",
    "    high_income_high_spending = (df['Annual Income (k$)'] > 60) & (df['Spending Score (1-100)'] > 60)\n",
    "    high_income_low_spending = (df['Annual Income (k$)'] > 60) & (df['Spending Score (1-100)'] <= 40)\n",
    "    low_income_high_spending = (df['Annual Income (k$)'] <= 40) & (df['Spending Score (1-100)'] > 60)\n",
    "    low_income_low_spending = (df['Annual Income (k$)'] <= 40) & (df['Spending Score (1-100)'] <= 40)\n",
    "    \n",
    "    df.loc[high_income_high_spending, 'Customer_Segment'] = 'Premium'\n",
    "    df.loc[high_income_low_spending, 'Customer_Segment'] = 'Conservative'\n",
    "    df.loc[low_income_high_spending, 'Customer_Segment'] = 'Aspirational'\n",
    "    df.loc[low_income_low_spending, 'Customer_Segment'] = 'Budget_Conscious'\n",
    "    \n",
    "    # Analyze gender distribution across segments\n",
    "    segment_gender = pd.crosstab(df['Customer_Segment'], df['Gender'], normalize='index')\n",
    "    \n",
    "    # Chi-square test for independence\n",
    "    chi2, p_value, dof, expected = stats.chi2_contingency(\n",
    "        pd.crosstab(df['Customer_Segment'], df['Gender'])\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'segment_distribution': segment_gender,\n",
    "        'chi2_statistic': chi2,\n",
    "        'p_value': p_value,\n",
    "        'independence_test': 'Reject H0' if p_value < 0.05 else 'Fail to reject H0'\n",
    "    }\n",
    "```\n",
    "\n",
    "#### **3. Simulated Temporal Analysis**\n",
    "```python\n",
    "def simulated_temporal_analysis(df):\n",
    "    \"\"\"\n",
    "    Create simulated temporal patterns for demonstration\n",
    "    \"\"\"\n",
    "    # Simulate customer acquisition dates based on CustomerID\n",
    "    # (Assuming lower IDs = earlier customers)\n",
    "    df['Simulated_Date'] = pd.date_range('2020-01-01', periods=len(df), freq='D')\n",
    "    \n",
    "    # Add seasonal component based on customer characteristics\n",
    "    df['Month'] = df['Simulated_Date'].dt.month\n",
    "    df['Quarter'] = df['Simulated_Date'].dt.quarter\n",
    "    \n",
    "    # Analyze seasonal gender patterns (simulated)\n",
    "    seasonal_patterns = pd.crosstab(df['Quarter'], df['Gender'], normalize='columns')\n",
    "    \n",
    "    # Test for seasonal independence\n",
    "    chi2, p_value, dof, expected = stats.chi2_contingency(\n",
    "        pd.crosstab(df['Quarter'], df['Gender'])\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'seasonal_distribution': seasonal_patterns,\n",
    "        'seasonal_chi2': chi2,\n",
    "        'seasonal_p_value': p_value\n",
    "    }\n",
    "```\n",
    "\n",
    "#### **4. Simulated Spatial Analysis**\n",
    "```python\n",
    "def simulated_spatial_analysis(df):\n",
    "    \"\"\"\n",
    "    Create simulated spatial patterns for demonstration\n",
    "    \"\"\"\n",
    "    # Simulate geographic coordinates based on customer characteristics\n",
    "    # Higher income customers might cluster in certain areas\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create income-based spatial clustering\n",
    "    base_lat, base_lon = 40.7128, -74.0060  # NYC coordinates\n",
    "    \n",
    "    df['Simulated_Lat'] = base_lat + np.random.normal(0, 0.1, len(df))\n",
    "    df['Simulated_Lon'] = base_lon + np.random.normal(0, 0.1, len(df))\n",
    "    \n",
    "    # Add income-based spatial bias\n",
    "    high_income_mask = df['Annual Income (k$)'] > 70\n",
    "    df.loc[high_income_mask, 'Simulated_Lat'] += 0.05  # Shift north\n",
    "    df.loc[high_income_mask, 'Simulated_Lon'] += 0.05  # Shift east\n",
    "    \n",
    "    # Analyze spatial distribution by gender\n",
    "    male_coords = df[df['Gender'] == 'Male'][['Simulated_Lat', 'Simulated_Lon']].values\n",
    "    female_coords = df[df['Gender'] == 'Female'][['Simulated_Lat', 'Simulated_Lon']].values\n",
    "    \n",
    "    # Calculate spatial centroids\n",
    "    male_centroid = male_coords.mean(axis=0)\n",
    "    female_centroid = female_coords.mean(axis=0)\n",
    "    \n",
    "    # Calculate spatial separation\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    centroid_distance = euclidean_distances([male_centroid], [female_centroid])[0][0]\n",
    "    \n",
    "    return {\n",
    "        'male_centroid': male_centroid,\n",
    "        'female_centroid': female_centroid,\n",
    "        'spatial_separation': centroid_distance,\n",
    "        'coordinates': df[['Simulated_Lat', 'Simulated_Lon']]\n",
    "    }\n",
    "```\n",
    "\n",
    "### **C. Final Recommendations for Mall_Customers Dataset**\n",
    "\n",
    "**ðŸŽ¯ PRIMARY ANALYSIS FOCUS:**\n",
    "\n",
    "1. **Cross-Sectional Demographic Analysis**\n",
    "   - Gender distribution across age groups\n",
    "   - Gender patterns in income categories  \n",
    "   - Gender differences in spending behavior\n",
    "\n",
    "2. **Behavioral Segmentation Analysis**\n",
    "   - Customer segment identification\n",
    "   - Gender representation in each segment\n",
    "   - Statistical independence testing\n",
    "\n",
    "3. **Correlation Pattern Analysis**\n",
    "   - Gender vs Age correlation patterns\n",
    "   - Gender vs Income relationship analysis\n",
    "   - Gender vs Spending Score associations\n",
    "\n",
    "**âš ï¸ LIMITATIONS & CONSTRAINTS:**\n",
    "\n",
    "- **No temporal dimension**: Cannot perform true seasonal/trend analysis\n",
    "- **No spatial dimension**: Cannot perform geographic clustering analysis\n",
    "- **Limited categorical variables**: Only Gender is truly categorical\n",
    "- **Cross-sectional data**: Snapshot analysis only\n",
    "\n",
    "**ðŸ”„ ALTERNATIVE APPROACHES:**\n",
    "\n",
    "- **Simulate temporal/spatial dimensions** for methodology demonstration\n",
    "- **Create derived categorical variables** from numerical ones\n",
    "- **Focus on multivariate categorical relationships**\n",
    "- **Use customer segmentation as proxy for pattern analysis**\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **SUMMARY: Advanced Pattern Recognition Decision Framework**\n",
    "\n",
    "### **âœ… Complete Framework Delivered**\n",
    "\n",
    "**1. TEMPORAL PATTERN ANALYSIS**\n",
    "- Seasonal decomposition methods\n",
    "- Fourier analysis for cyclical patterns  \n",
    "- Autocorrelation analysis\n",
    "- Cochran-Armitage trend tests\n",
    "- Mann-Kendall trend detection\n",
    "\n",
    "**2. SPATIAL PATTERN ANALYSIS**\n",
    "- Moran's I spatial autocorrelation\n",
    "- Getis-Ord G* hot/cold spot detection\n",
    "- Ripley's K clustering analysis\n",
    "- Spatial distribution assessment\n",
    "\n",
    "**3. DECISION CRITERIA MATRIX**\n",
    "- Data property assessment framework\n",
    "- Test selection based on sample size, temporal span, spatial extent\n",
    "- Balance and entropy considerations\n",
    "- Missing data impact evaluation\n",
    "\n",
    "**4. MALL_CUSTOMERS SPECIFIC ANALYSIS**\n",
    "- **Conclusion**: Limited to cross-sectional demographic analysis due to lack of temporal/spatial dimensions\n",
    "- **Recommended approach**: Focus on behavioral segmentation and demographic pattern analysis\n",
    "- **Alternative methods**: Simulate temporal/spatial dimensions for methodology demonstration\n",
    "\n",
    "### **ðŸŽ¯ Key Decision Criteria Summary**\n",
    "\n",
    "| **Data Property** | **Threshold** | **Recommended Tests** |\n",
    "|-------------------|---------------|----------------------|\n",
    "| **Temporal Span** | >365 days | Seasonal decomposition, Fourier analysis |\n",
    "| **Temporal Span** | 30-365 days | Trend analysis, Autocorrelation |\n",
    "| **Spatial Extent** | >100 km | Full spatial analysis suite |\n",
    "| **Spatial Extent** | 10-100 km | Local clustering analysis |\n",
    "| **Sample Size** | <30 | Use with caution, bootstrap if possible |\n",
    "| **Balance Ratio** | >10:1 | Imbalance-specific analysis |\n",
    "| **Missing Data** | >10% | Address before pattern analysis |\n",
    "\n",
    "This framework provides a systematic approach to selecting appropriate pattern recognition techniques based on your data's specific characteristics and limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e5663d",
   "metadata": {},
   "source": [
    "#chi-squared test for seasonal dependence\n",
    "\n",
    "In detail, covering its mathematical foundations, mechanics, and interpretation. Let me first examine your advanced pattern analysis notebook to see the current context.\n",
    "\n",
    "Perfect! I can see you're working with advanced pattern analysis. Let me provide a comprehensive explanation of the chi-squared test for seasonal dependence.\n",
    "\n",
    "## **Chi-Squared Test for Seasonal Dependence: Complete Mathematical Foundation**\n",
    "\n",
    "### **1. CONCEPTUAL FRAMEWORK**\n",
    "\n",
    "The chi-squared test for seasonal dependence tests whether categorical variable distributions are **independent** of time periods (seasons). \n",
    "\n",
    "**Null Hypothesis (Hâ‚€):** The categorical variable distribution is independent of seasonal periods\n",
    "**Alternative Hypothesis (Hâ‚):** The categorical variable distribution depends on seasonal periods\n",
    "\n",
    "### **2. MATHEMATICAL FOUNDATIONS**\n",
    "\n",
    "#### **A. Contingency Table Construction**\n",
    "\n",
    "Given categorical data over time, we create a **contingency table**:\n",
    "\n",
    "```plaintext\n",
    "                Categories\n",
    "Time Periods    Catâ‚  Catâ‚‚  Catâ‚ƒ  ...  Catâ‚–   Total\n",
    "Periodâ‚         Oâ‚â‚   Oâ‚â‚‚   Oâ‚â‚ƒ   ...  Oâ‚â‚–    Râ‚\n",
    "Periodâ‚‚         Oâ‚‚â‚   Oâ‚‚â‚‚   Oâ‚‚â‚ƒ   ...  Oâ‚‚â‚–    Râ‚‚\n",
    "Periodâ‚ƒ         Oâ‚ƒâ‚   Oâ‚ƒâ‚‚   Oâ‚ƒâ‚ƒ   ...  Oâ‚ƒâ‚–    Râ‚ƒ\n",
    "...             ...   ...   ...   ...  ...    ...\n",
    "Periodâ‚˜        Oâ‚˜â‚   Oâ‚˜â‚‚   Oâ‚˜â‚ƒ   ...  Oâ‚˜â‚–    Râ‚˜\n",
    "Total           Câ‚    Câ‚‚    Câ‚ƒ    ...  Câ‚–     N\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Oáµ¢â±¼** = Observed frequency of category j in period i\n",
    "- **Ráµ¢** = Row total for period i\n",
    "- **Câ±¼** = Column total for category j  \n",
    "- **N** = Grand total\n",
    "\n",
    "#### **B. Expected Frequencies Under Independence**\n",
    "\n",
    "If categories are independent of seasons, expected frequency for cell (i,j):\n",
    "\n",
    "**Eáµ¢â±¼ = (Ráµ¢ Ã— Câ±¼) / N**\n",
    "\n",
    "This represents what we'd expect if the distribution were uniform across seasons.\n",
    "\n",
    "#### **C. Chi-Squared Statistic**\n",
    "\n",
    "**Ï‡Â² = Î£áµ¢ Î£â±¼ [(Oáµ¢â±¼ - Eáµ¢â±¼)Â² / Eáµ¢â±¼]**\n",
    "\n",
    "Where the summation is over all m periods and k categories.\n",
    "\n",
    "#### **D. Degrees of Freedom**\n",
    "\n",
    "**df = (m - 1) Ã— (k - 1)**\n",
    "\n",
    "Where:\n",
    "- m = number of time periods\n",
    "- k = number of categories\n",
    "\n",
    "### **3. STEP-BY-STEP MATHEMATICAL PROCESS**\n",
    "\n",
    "Let me demonstrate with a concrete example:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"Temporal Categorical Data\"] --> B[\"Create Contingency Table<br/>Periods Ã— Categories\"]\n",
    "    B --> C[\"Calculate Expected Frequencies<br/>E_ij = (R_i Ã— C_j) / N\"]\n",
    "    C --> D[\"Compute Chi-Square Statistic<br/>Ï‡Â² = Î£[(O_ij - E_ij)Â² / E_ij]\"]\n",
    "    D --> E[\"Determine Degrees of Freedom<br/>df = (m-1) Ã— (k-1)\"]\n",
    "    E --> F[\"Compare with Critical Value<br/>or Calculate p-value\"]\n",
    "    F --> G{\"p < Î±?\"}\n",
    "    G -->|Yes| H[\"Reject Hâ‚€<br/>Seasonal Dependence Exists\"]\n",
    "    G -->|No| I[\"Fail to Reject Hâ‚€<br/>No Seasonal Dependence\"]\n",
    "    \n",
    "    style A fill:#ef6c00\n",
    "    style H fill:#2e7d32\n",
    "    style I fill:#1565c0\n",
    "```\n",
    "\n",
    "### **4. DETAILED MATHEMATICAL EXAMPLE**\n",
    "\n",
    "```python\n",
    "def chi_squared_seasonal_detailed_example():\n",
    "    \"\"\"\n",
    "    Detailed mathematical walkthrough of chi-squared seasonal test\n",
    "    \"\"\"\n",
    "    # Example: Gender distribution across quarters\n",
    "    # Observed data\n",
    "    observed = np.array([\n",
    "        [25, 35],  # Q1: Male, Female\n",
    "        [30, 40],  # Q2: Male, Female  \n",
    "        [45, 25],  # Q3: Male, Female\n",
    "        [20, 50]   # Q4: Male, Female\n",
    "    ])\n",
    "    \n",
    "    print(\"=== STEP 1: CONTINGENCY TABLE ===\")\n",
    "    print(\"Observed frequencies:\")\n",
    "    print(\"        Male  Female  Total\")\n",
    "    for i, (male, female) in enumerate(observed):\n",
    "        print(f\"Q{i+1}:     {male:2d}    {female:2d}    {male+female:3d}\")\n",
    "    \n",
    "    # Calculate totals\n",
    "    row_totals = observed.sum(axis=1)  # [60, 70, 70, 70]\n",
    "    col_totals = observed.sum(axis=0)  # [120, 150]\n",
    "    grand_total = observed.sum()       # 270\n",
    "    \n",
    "    print(f\"Total: {col_totals[0]:3d}   {col_totals[1]:3d}    {grand_total:3d}\")\n",
    "    \n",
    "    print(\"\\n=== STEP 2: EXPECTED FREQUENCIES ===\")\n",
    "    expected = np.outer(row_totals, col_totals) / grand_total\n",
    "    print(\"Expected frequencies (if independent):\")\n",
    "    print(\"        Male  Female\")\n",
    "    for i, (exp_male, exp_female) in enumerate(expected):\n",
    "        print(f\"Q{i+1}:  {exp_male:5.1f}  {exp_female:5.1f}\")\n",
    "    \n",
    "    print(\"\\n=== STEP 3: CHI-SQUARE CALCULATION ===\")\n",
    "    chi_square_components = (observed - expected)**2 / expected\n",
    "    print(\"(Observed - Expected)Â² / Expected:\")\n",
    "    print(\"        Male  Female\")\n",
    "    total_chi2 = 0\n",
    "    for i, (chi_male, chi_female) in enumerate(chi_square_components):\n",
    "        print(f\"Q{i+1}:  {chi_male:5.3f}  {chi_female:5.3f}\")\n",
    "        total_chi2 += chi_male + chi_female\n",
    "    \n",
    "    print(f\"\\nÏ‡Â² = {total_chi2:.3f}\")\n",
    "    \n",
    "    print(\"\\n=== STEP 4: DEGREES OF FREEDOM ===\")\n",
    "    df = (observed.shape[0] - 1) * (observed.shape[1] - 1)\n",
    "    print(f\"df = (4-1) Ã— (2-1) = {df}\")\n",
    "    \n",
    "    print(\"\\n=== STEP 5: STATISTICAL INFERENCE ===\")\n",
    "    from scipy.stats import chi2\n",
    "    p_value = 1 - chi2.cdf(total_chi2, df)\n",
    "    critical_value_05 = chi2.ppf(0.95, df)\n",
    "    \n",
    "    print(f\"p-value = {p_value:.6f}\")\n",
    "    print(f\"Critical value (Î±=0.05) = {critical_value_05:.3f}\")\n",
    "    print(f\"Decision: {'Reject Hâ‚€' if p_value < 0.05 else 'Fail to reject Hâ‚€'}\")\n",
    "    \n",
    "    return {\n",
    "        'observed': observed,\n",
    "        'expected': expected,\n",
    "        'chi_square': total_chi2,\n",
    "        'p_value': p_value,\n",
    "        'degrees_freedom': df\n",
    "    }\n",
    "\n",
    "# Run the example\n",
    "result = chi_squared_seasonal_detailed_example()\n",
    "```\n",
    "\n",
    "### **5. HOW IT DETECTS SEASONALITY**\n",
    "\n",
    "#### **A. Deviation Measurement**\n",
    "\n",
    "The test detects seasonality by measuring **deviations from expected uniform distribution**:\n",
    "\n",
    "1. **Large deviations** â†’ High Ï‡Â² â†’ Evidence of seasonal pattern\n",
    "2. **Small deviations** â†’ Low Ï‡Â² â†’ No seasonal pattern\n",
    "\n",
    "#### **B. Pattern Recognition Mechanism**\n",
    "\n",
    "```python\n",
    "def seasonality_detection_mechanism():\n",
    "    \"\"\"\n",
    "    Explain how chi-squared detects different seasonal patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pattern 1: No seasonality (uniform distribution)\n",
    "    uniform_pattern = np.array([\n",
    "        [25, 25],  # Q1: Equal distribution\n",
    "        [25, 25],  # Q2: Equal distribution  \n",
    "        [25, 25],  # Q3: Equal distribution\n",
    "        [25, 25]   # Q4: Equal distribution\n",
    "    ])\n",
    "    # Ï‡Â² â‰ˆ 0 (no deviation from expected)\n",
    "    \n",
    "    # Pattern 2: Strong seasonality\n",
    "    seasonal_pattern = np.array([\n",
    "        [10, 40],  # Q1: Female-dominated\n",
    "        [40, 10],  # Q2: Male-dominated\n",
    "        [10, 40],  # Q3: Female-dominated  \n",
    "        [40, 10]   # Q4: Male-dominated\n",
    "    ])\n",
    "    # Ï‡Â² >> 0 (large deviation from expected)\n",
    "    \n",
    "    # Pattern 3: Moderate seasonality\n",
    "    moderate_pattern = np.array([\n",
    "        [20, 30],  # Q1: Slight female preference\n",
    "        [30, 20],  # Q2: Slight male preference\n",
    "        [25, 25],  # Q3: Balanced\n",
    "        [25, 25]   # Q4: Balanced\n",
    "    ])\n",
    "    # Ï‡Â² > 0 but moderate (some deviation)\n",
    "    \n",
    "    return {\n",
    "        'uniform': uniform_pattern,\n",
    "        'strong_seasonal': seasonal_pattern, \n",
    "        'moderate_seasonal': moderate_pattern\n",
    "    }\n",
    "```\n",
    "\n",
    "### **6. INTERPRETATION FRAMEWORK**\n",
    "\n",
    "#### **A. Statistical Significance Levels**\n",
    "\n",
    "| **p-value** | **Interpretation** | **Seasonal Evidence** |\n",
    "|-------------|-------------------|----------------------|\n",
    "| p < 0.001   | Highly significant | Very strong seasonality |\n",
    "| 0.001 â‰¤ p < 0.01 | Significant | Strong seasonality |\n",
    "| 0.01 â‰¤ p < 0.05 | Moderately significant | Moderate seasonality |\n",
    "| p â‰¥ 0.05 | Not significant | No seasonal pattern |\n",
    "\n",
    "#### **B. Effect Size Measures**\n",
    "\n",
    "**CramÃ©r's V** (effect size for chi-squared):\n",
    "**V = âˆš(Ï‡Â² / (N Ã— min(m-1, k-1)))**\n",
    "\n",
    "| **CramÃ©r's V** | **Effect Size** | **Practical Significance** |\n",
    "|----------------|-----------------|---------------------------|\n",
    "| 0.1 - 0.3      | Small          | Weak seasonal pattern |\n",
    "| 0.3 - 0.5      | Medium         | Moderate seasonal pattern |\n",
    "| > 0.5          | Large          | Strong seasonal pattern |\n",
    "\n",
    "### **7. ASSUMPTIONS & LIMITATIONS**\n",
    "\n",
    "#### **A. Key Assumptions**\n",
    "\n",
    "1. **Independence**: Observations are independent\n",
    "2. **Expected frequencies**: All expected frequencies â‰¥ 5\n",
    "3. **Random sampling**: Data represents random sample\n",
    "4. **Categorical data**: Variables are truly categorical\n",
    "\n",
    "#### **B. Limitation Handling**\n",
    "\n",
    "```python\n",
    "def check_chi_square_assumptions(contingency_table):\n",
    "    \"\"\"\n",
    "    Check assumptions for chi-squared test validity\n",
    "    \"\"\"\n",
    "    expected = stats.contingency.expected_freq(contingency_table)\n",
    "    \n",
    "    assumptions = {\n",
    "        'min_expected_freq': expected.min(),\n",
    "        'cells_below_5': (expected < 5).sum(),\n",
    "        'total_cells': expected.size,\n",
    "        'percent_below_5': (expected < 5).sum() / expected.size * 100\n",
    "    }\n",
    "    \n",
    "    # Yates correction for 2x2 tables\n",
    "    if contingency_table.shape == (2, 2):\n",
    "        assumptions['recommend_yates'] = True\n",
    "    \n",
    "    # Fisher's exact test recommendation\n",
    "    if assumptions['min_expected_freq'] < 5:\n",
    "        assumptions['recommend_fisher'] = True\n",
    "    \n",
    "    return assumptions\n",
    "```\n",
    "\n",
    "### **8. ADVANCED INTERPRETATIONS**\n",
    "\n",
    "#### **A. Residual Analysis**\n",
    "\n",
    "**Standardized residuals** help identify which cells contribute most to seasonality:\n",
    "\n",
    "**ráµ¢â±¼ = (Oáµ¢â±¼ - Eáµ¢â±¼) / âˆšEáµ¢â±¼**\n",
    "\n",
    "- **|ráµ¢â±¼| > 2**: Significant contribution to seasonal pattern\n",
    "- **|ráµ¢â±¼| > 3**: Highly significant contribution\n",
    "\n",
    "#### **B. Post-hoc Analysis**\n",
    "\n",
    "After detecting seasonality, analyze **which periods and categories** drive the pattern:\n",
    "\n",
    "```python\n",
    "def post_hoc_seasonal_analysis(observed, expected):\n",
    "    \"\"\"\n",
    "    Detailed analysis of seasonal patterns after significant chi-squared test\n",
    "    \"\"\"\n",
    "    # Calculate standardized residuals\n",
    "    residuals = (observed - expected) / np.sqrt(expected)\n",
    "    \n",
    "    # Identify significant cells\n",
    "    significant_cells = np.abs(residuals) > 2\n",
    "    \n",
    "    # Calculate contribution to chi-squared\n",
    "    contributions = (observed - expected)**2 / expected\n",
    "    contribution_percent = contributions / contributions.sum() * 100\n",
    "    \n",
    "    return {\n",
    "        'residuals': residuals,\n",
    "        'significant_cells': significant_cells,\n",
    "        'contributions': contributions,\n",
    "        'contribution_percent': contribution_percent\n",
    "    }\n",
    "```\n",
    "\n",
    "### **9. PRACTICAL IMPLEMENTATION**\n",
    "\n",
    "```python\n",
    "def comprehensive_seasonal_chi_square(df, cat_col, time_col, period_type='month'):\n",
    "    \"\"\"\n",
    "    Complete implementation of seasonal chi-squared analysis\n",
    "    \"\"\"\n",
    "    # Create time periods\n",
    "    if period_type == 'month':\n",
    "        df['period'] = df[time_col].dt.month\n",
    "    elif period_type == 'quarter':\n",
    "        df['period'] = df[time_col].dt.quarter\n",
    "    elif period_type == 'day_of_week':\n",
    "        df['period'] = df[time_col].dt.dayofweek\n",
    "    \n",
    "    # Create contingency table\n",
    "    contingency = pd.crosstab(df['period'], df[cat_col])\n",
    "    \n",
    "    # Perform chi-squared test\n",
    "    chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency)\n",
    "    \n",
    "    # Calculate effect size (CramÃ©r's V)\n",
    "    n = contingency.sum().sum()\n",
    "    cramers_v = np.sqrt(chi2_stat / (n * min(contingency.shape) - 1))\n",
    "    \n",
    "    # Check assumptions\n",
    "    min_expected = expected.min()\n",
    "    cells_below_5 = (expected < 5).sum()\n",
    "    \n",
    "    # Post-hoc analysis if significant\n",
    "    residuals = None\n",
    "    if p_value < 0.05:\n",
    "        residuals = (contingency - expected) / np.sqrt(expected)\n",
    "    \n",
    "    return {\n",
    "        'contingency_table': contingency,\n",
    "        'expected_frequencies': expected,\n",
    "        'chi2_statistic': chi2_stat,\n",
    "        'p_value': p_value,\n",
    "        'degrees_freedom': dof,\n",
    "        'cramers_v': cramers_v,\n",
    "        'min_expected_freq': min_expected,\n",
    "        'cells_below_5': cells_below_5,\n",
    "        'standardized_residuals': residuals,\n",
    "        'interpretation': {\n",
    "            'significant': p_value < 0.05,\n",
    "            'effect_size': 'small' if cramers_v < 0.3 else 'medium' if cramers_v < 0.5 else 'large',\n",
    "            'assumptions_met': min_expected >= 5 and cells_below_5 == 0\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "The chi-squared test detects seasonality by comparing observed categorical distributions across time periods with what we'd expect under independence. Large deviations indicate seasonal patterns, while small deviations suggest uniform distribution across seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f0b60",
   "metadata": {},
   "source": [
    "# **What Are Dominant Patterns in Data?**\n",
    "\n",
    "**Dominant patterns** are the **strongest, most recurring structures** in your data that explain the majority of variation and provide the most predictive power.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Raw Data<br/>(Complex, Noisy)\"] --> B[\"Pattern Detection<br/>Analysis\"]\n",
    "    \n",
    "    B --> C[\"Dominant Patterns<br/>(Strong Signals)\"]\n",
    "    B --> D[\"Secondary Patterns<br/>(Weaker Signals)\"]\n",
    "    B --> E[\"Noise<br/>(Random Variation)\"]\n",
    "    \n",
    "    C --> F[\"High Explanatory Power<br/>60-80% of variance\"]\n",
    "    D --> G[\"Moderate Explanatory Power<br/>10-20% of variance\"]\n",
    "    E --> H[\"Low Explanatory Power<br/>5-15% of variance\"]\n",
    "    \n",
    "    F --> I[\"Business Insights<br/>Actionable Patterns\"]\n",
    "    G --> J[\"Supporting Evidence<br/>Confirming Trends\"]\n",
    "    H --> K[\"Background Variation<br/>Usually Ignored\"]\n",
    "    \n",
    "    style A fill:#ffcdd2,color:#000\n",
    "    style C fill:#c8e6c9,color:#000\n",
    "    style D fill:#fff3e0,color:#000\n",
    "    style E fill:#f5f5f5,color:#000\n",
    "    style I fill:#e1f5fe,color:#000\n",
    "```\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **Types of Dominant Patterns**\n",
    "\n",
    "| **Pattern Type** | **Description** | **Example** | **Strength Indicator** |\n",
    "|------------------|-----------------|-------------|------------------------|\n",
    "| **Seasonal** | Regular, cyclical variations | Weekly sales peaks | High amplitude, consistent frequency |\n",
    "| **Trend** | Long-term directional change | Growing customer base | Sustained slope, low noise |\n",
    "| **Structural** | Fundamental relationships | Price-demand correlation | High correlation coefficient |\n",
    "| **Behavioral** | Recurring human patterns | Rush hour traffic | Predictable timing, high frequency |\n",
    "| **Cyclical** | Irregular but recurring | Economic cycles | Variable period, consistent shape |\n",
    "\n",
    "## **How to Identify Dominant Patterns**\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"Data Analysis\"] --> B{\"Pattern Strength\"}\n",
    "    \n",
    "    B -->|High Magnitude| C[\"Dominant Pattern<br/>âœ“ Strong Signal<br/>âœ“ High Variance Explained<br/>âœ“ Consistent Occurrence\"]\n",
    "    \n",
    "    B -->|Medium Magnitude| D[\"Secondary Pattern<br/>â€¢ Moderate Signal<br/>â€¢ Some Variance Explained<br/>â€¢ Occasional Occurrence\"]\n",
    "    \n",
    "    B -->|Low Magnitude| E[\"Weak Pattern/Noise<br/>â€¢ Weak Signal<br/>â€¢ Little Variance Explained<br/>â€¢ Irregular Occurrence\"]\n",
    "    \n",
    "    C --> F[\"Business Action<br/>ðŸŽ¯ Strategic Planning<br/>ðŸ“Š Forecasting<br/>ðŸ’¡ Decision Making\"]\n",
    "    \n",
    "    D --> G[\"Supporting Analysis<br/>ðŸ“ˆ Trend Confirmation<br/>ðŸ” Detailed Investigation\"]\n",
    "    \n",
    "    E --> H[\"Usually Ignored<br/>ðŸ—‘ï¸ Filter Out<br/>ðŸ“‰ Background Noise\"]\n",
    "    \n",
    "    style C fill:#4caf50,color:#000\n",
    "    style D fill:#ff9800,color:#000\n",
    "    style E fill:#9e9e9e,color:#000\n",
    "    style F fill:#2196f3,color:#000\n",
    "```\n",
    "\n",
    "## **Strength Measurement Framework**\n",
    "\n",
    "### **Quantitative Indicators**\n",
    "\n",
    "| **Metric** | **Strong Pattern** | **Weak Pattern** | **Interpretation** |\n",
    "|------------|-------------------|------------------|-------------------|\n",
    "| **Variance Explained** | >50% | <20% | How much variation the pattern accounts for |\n",
    "| **Signal-to-Noise Ratio** | >3:1 | <1.5:1 | Pattern strength vs random variation |\n",
    "| **Consistency** | >80% occurrence | <40% occurrence | How reliably the pattern appears |\n",
    "| **Amplitude** | High relative to mean | Low relative to mean | Size of pattern effect |\n",
    "| **Statistical Significance** | p < 0.001 | p > 0.05 | Confidence in pattern existence |\n",
    "\n",
    "## **Pattern Hierarchy**\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"Pattern Strength Pyramid\"\n",
    "        A[\"ðŸ† DOMINANT PATTERNS<br/>1-3 patterns<br/>Explain 60-80% of variation<br/>Drive business decisions\"]\n",
    "        B[\"ðŸ¥ˆ SECONDARY PATTERNS<br/>3-5 patterns<br/>Explain 15-25% of variation<br/>Support analysis\"]\n",
    "        C[\"ðŸ¥‰ MINOR PATTERNS<br/>5-10 patterns<br/>Explain 5-15% of variation<br/>Detailed insights\"]\n",
    "        D[\"ðŸ“Š NOISE/RESIDUAL<br/>Everything else<br/>Explain <10% of variation<br/>Usually ignored\"]\n",
    "    end\n",
    "    \n",
    "    A --> E[\"Strategic Impact<br/>â€¢ Resource allocation<br/>â€¢ Long-term planning<br/>â€¢ Core business metrics\"]\n",
    "    B --> F[\"Tactical Impact<br/>â€¢ Operational adjustments<br/>â€¢ Process improvements<br/>â€¢ Supporting evidence\"]\n",
    "    C --> G[\"Analytical Impact<br/>â€¢ Deep-dive analysis<br/>â€¢ Niche insights<br/>â€¢ Research questions\"]\n",
    "    D --> H[\"Minimal Impact<br/>â€¢ Background variation<br/>â€¢ Measurement error<br/>â€¢ Random fluctuation\"]\n",
    "    \n",
    "    style A fill:#4caf50,color:#000\n",
    "    style B fill:#ff9800,color:#000\n",
    "    style C fill:#2196f3,color:#000\n",
    "    style D fill:#9e9e9e,color:#000\n",
    "```\n",
    "\n",
    "## **Real-World Examples**\n",
    "\n",
    "### **Retail Business Context**\n",
    "\n",
    "| **Data Type** | **Dominant Pattern** | **Strength** | **Business Impact** |\n",
    "|---------------|---------------------|--------------|-------------------|\n",
    "| **Sales Data** | Weekend peaks (2x weekday sales) | Very High | Staffing, inventory planning |\n",
    "| **Customer Traffic** | Holiday seasonality (3x normal) | Very High | Marketing budget allocation |\n",
    "| **Product Returns** | Post-holiday spike (5x increase) | High | Return policy, logistics |\n",
    "| **Website Visits** | Lunch-time dip (40% decrease) | Medium | Server capacity planning |\n",
    "| **Social Media** | Viral content spikes (random) | Low | Reactive content strategy |\n",
    "\n",
    "## **Why Dominant Patterns Matter**\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((Dominant Patterns))\n",
    "    Predictability\n",
    "      Forecasting Accuracy\n",
    "      Risk Assessment\n",
    "      Planning Confidence\n",
    "    Resource Optimization\n",
    "      Efficient Allocation\n",
    "      Cost Reduction\n",
    "      Performance Improvement\n",
    "    Strategic Insights\n",
    "      Market Understanding\n",
    "      Customer Behavior\n",
    "      Competitive Advantage\n",
    "    Decision Making\n",
    "      Data-Driven Choices\n",
    "      Priority Setting\n",
    "      Investment Focus\n",
    "```\n",
    "\n",
    "## **Key Takeaways**\n",
    "\n",
    "**ðŸŽ¯ Focus Principle**: Dominant patterns represent the **80/20 rule** - a few strong patterns explain most of the variation in your data.\n",
    "\n",
    "**ðŸ“Š Identification**: Look for patterns with **high magnitude**, **consistent occurrence**, and **significant variance explanation**.\n",
    "\n",
    "**ðŸ’¼ Business Value**: Dominant patterns drive **strategic decisions**, while weaker patterns provide **supporting insights**.\n",
    "\n",
    "**âš¡ Efficiency**: By focusing on dominant patterns first, you maximize analytical impact with minimal effort.\n",
    "\n",
    "**ðŸ” Analysis Strategy**: Start with dominant patterns for major insights, then drill down to secondary patterns for detailed understanding.\n",
    "\n",
    "The key is to **distinguish signal from noise** - dominant patterns are your strongest signals that deserve the most attention and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe1d56",
   "metadata": {},
   "source": [
    "# Recap of methods to determine Dominant patterns\n",
    "\n",
    "I'll examine your project notebooks to identify all the pattern analysis techniques you've implemented and provide a concise summary.\n",
    "\n",
    "[5 tools called]\n",
    "\n",
    "Based on my examination of your project notebooks, here's a comprehensive summary of all the dominant pattern identification techniques implemented in your categorical data analysis project:\n",
    "\n",
    "## **Dominant Pattern Identification Techniques in Your Project**\n",
    "\n",
    "### **ðŸ“Š 1. DESCRIPTIVE STATISTICS & FREQUENCY ANALYSIS**\n",
    "- **Basic frequency analysis** (`value_counts()`, proportions)\n",
    "- **Mode detection** (most frequent category)\n",
    "- **Dominance ratio** (majority vs minority categories)\n",
    "- **Uniformity assessment** (distribution evenness)\n",
    "- **Category concentration analysis**\n",
    "- **Cross-tabulations** and contingency tables\n",
    "\n",
    "### **ðŸ“ˆ 2. DISTRIBUTION ANALYSIS**\n",
    "- **Balance assessment** (class imbalance detection)\n",
    "- **Imbalance ratios** (majority:minority proportions)\n",
    "- **Chi-square goodness of fit** tests\n",
    "- **Uniformity vs skewed distribution detection**\n",
    "- **Minority representation analysis**\n",
    "\n",
    "### **ðŸ”¢ 3. ENTROPY & INFORMATION ANALYSIS**\n",
    "- **Shannon entropy** (information content measurement)\n",
    "- **Normalized entropy** (predictability assessment)\n",
    "- **Information diversity metrics**\n",
    "- **Entropy-based pattern strength quantification**\n",
    "- **Category complexity measures**\n",
    "\n",
    "### **ðŸ“Š 4. STATISTICAL MEASURES**\n",
    "- **Herfindahl-Hirschman Index** (concentration measurement)\n",
    "- **Simpson's diversity index**\n",
    "- **Evenness scores** (distribution uniformity)\n",
    "- **Effective number of categories**\n",
    "- **Participation ratios**\n",
    "\n",
    "### **ðŸŒŠ 5. ADVANCED PATTERN RECOGNITION**\n",
    "\n",
    "#### **A. Temporal Pattern Analysis**\n",
    "- **Seasonal decomposition** (chi-square seasonal independence)\n",
    "- **Fourier analysis** (cyclical pattern detection)\n",
    "- **Autocorrelation analysis** (temporal dependencies)\n",
    "- **Cochran-Armitage trend tests** (linear trend detection)\n",
    "- **Mann-Kendall trend analysis** (non-parametric trend testing)\n",
    "\n",
    "#### **B. Spatial Pattern Analysis**\n",
    "- **Moran's I** (spatial autocorrelation)\n",
    "- **Getis-Ord G*** (hot/cold spot detection)\n",
    "- **Ripley's K function** (spatial clustering)\n",
    "\n",
    "### **ðŸŽ¯ 6. PATTERN STRENGTH QUANTIFICATION**\n",
    "\n",
    "| **Technique** | **Purpose** | **Strength Indicator** |\n",
    "|---------------|-------------|------------------------|\n",
    "| **Magnitude Spectrum** | Cycle strength measurement | High peaks = dominant patterns |\n",
    "| **Variance Explained** | Pattern importance | >50% = dominant, <20% = weak |\n",
    "| **Power Spectrum** | Energy distribution | Concentrated power = strong patterns |\n",
    "| **Phase Analysis** | Timing detection | Consistent phase = regular patterns |\n",
    "| **Signal-to-Noise Ratio** | Pattern clarity | >3:1 = strong, <1.5:1 = weak |\n",
    "\n",
    "### **ðŸ” 7. BUSINESS INTELLIGENCE PATTERNS**\n",
    "- **Demographic segmentation** analysis\n",
    "- **Behavioral pattern** identification\n",
    "- **Customer segment** distribution analysis\n",
    "- **Cross-category relationship** patterns\n",
    "- **Predictability scoring** for forecasting potential\n",
    "\n",
    "### **ðŸ“‹ 8. PATTERN VALIDATION & TESTING**\n",
    "- **Statistical significance** testing (p-values)\n",
    "- **Effect size** measurements (CramÃ©r's V)\n",
    "- **Assumption checking** (expected frequencies, independence)\n",
    "- **Residual analysis** (pattern contribution assessment)\n",
    "- **Cross-validation** approaches\n",
    "\n",
    "## **ðŸŽ¯ Pattern Hierarchy Framework**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Raw Categorical Data\"] --> B[\"Pattern Detection Suite\"]\n",
    "    \n",
    "    B --> C[\"TIER 1: DOMINANT PATTERNS<br/>ðŸ† Explained Variance >50%<br/>ðŸ“Š High Statistical Significance<br/>ðŸŽ¯ Business Critical\"]\n",
    "    \n",
    "    B --> D[\"TIER 2: SECONDARY PATTERNS<br/>ðŸ¥ˆ Explained Variance 20-50%<br/>ðŸ“ˆ Moderate Significance<br/>ðŸ’¡ Supporting Insights\"]\n",
    "    \n",
    "    B --> E[\"TIER 3: WEAK PATTERNS<br/>ðŸ¥‰ Explained Variance <20%<br/>ðŸ“‰ Low Significance<br/>ðŸ” Research Interest\"]\n",
    "    \n",
    "    C --> F[\"Strategic Actions<br/>â€¢ Resource Allocation<br/>â€¢ Forecasting Models<br/>â€¢ Core Decisions\"]\n",
    "    \n",
    "    D --> G[\"Tactical Actions<br/>â€¢ Process Optimization<br/>â€¢ Detailed Analysis<br/>â€¢ Trend Confirmation\"]\n",
    "    \n",
    "    E --> H[\"Investigative Actions<br/>â€¢ Deep Dive Research<br/>â€¢ Hypothesis Generation<br/>â€¢ Future Monitoring\"]\n",
    "    \n",
    "    style C fill:#4caf50,color:#000\n",
    "    style D fill:#ff9800,color:#000\n",
    "    style E fill:#2196f3,color:#000\n",
    "```\n",
    "\n",
    "## **ðŸš€ Key Strengths of Your Analysis Suite**\n",
    "\n",
    "**âœ… Comprehensive Coverage**: From basic frequency analysis to advanced Fourier transforms\n",
    "**âœ… Multi-Dimensional**: Temporal, spatial, and cross-sectional pattern detection\n",
    "**âœ… Statistical Rigor**: Proper significance testing and effect size measurement\n",
    "**âœ… Business Focus**: Practical interpretation frameworks for decision-making\n",
    "**âœ… Scalable Framework**: Applicable to various categorical data types\n",
    "\n",
    "Your project implements a **complete pattern identification ecosystem** that can detect, quantify, and prioritize patterns across multiple dimensions, making it highly effective for discovering actionable insights in categorical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eebb77",
   "metadata": {},
   "source": [
    "# Explained Variance\n",
    "\n",
    "## **What is Explained Variance?**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Total Data Variation<br/>100%\"] --> B[\"Explained Variance<br/>(Pattern/Signal)\"]\n",
    "    A --> C[\"Unexplained Variance<br/>(Noise/Random)\"]\n",
    "    \n",
    "    B --> D[\"Predictable<br/>âœ“ Systematic<br/>âœ“ Meaningful<br/>âœ“ Actionable\"]\n",
    "    \n",
    "    C --> E[\"Unpredictable<br/>â€¢ Random fluctuation<br/>â€¢ Measurement error<br/>â€¢ Unknown factors\"]\n",
    "    \n",
    "    B --> F[\"High Explained Variance<br/>>70%<br/>Strong Pattern\"]\n",
    "    B --> G[\"Medium Explained Variance<br/>30-70%<br/>Moderate Pattern\"]\n",
    "    B --> H[\"Low Explained Variance<br/><30%<br/>Weak Pattern\"]\n",
    "    \n",
    "    style B fill:#4caf50,color:#000\n",
    "    style C fill:#ff5722,color:#000\n",
    "    style F fill:#2e7d32,color:#000\n",
    "    style G fill:#f57c00,color:#000\n",
    "    style H fill:#d32f2f,color:#000\n",
    "```\n",
    "\n",
    "**Explained variance** is a fundamental statistical concept that measures **how much of the total variation in your data is accounted for by a specific pattern, model, or component**.\n",
    "\n",
    "## **Core Concept**\n",
    "\n",
    "**Think of explained variance as answering the question:**\n",
    "*\"If I know this pattern exists, how much better can I predict what will happen compared to just guessing randomly?\"*\n",
    "\n",
    "### **Mathematical Foundation**\n",
    "\n",
    "**Explained Variance Ratio = (Variance of Pattern) / (Total Variance)**\n",
    "\n",
    "| **Component** | **Description** | **Example** |\n",
    "|---------------|-----------------|-------------|\n",
    "| **Total Variance** | All variation in your data | Customer visits vary from 50-200 per day |\n",
    "| **Explained Variance** | Variation due to known patterns | 80% of variation follows weekly cycle |\n",
    "| **Unexplained Variance** | Remaining random variation | 20% is unpredictable daily fluctuation |\n",
    "\n",
    "## **Intuitive Examples**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    subgraph \"High Explained Variance (90%)\"\n",
    "        A1[\"Monday: 100 customers<br/>Tuesday: 120 customers<br/>Wednesday: 110 customers<br/>Thursday: 130 customers<br/>Friday: 200 customers<br/>Saturday: 250 customers<br/>Sunday: 180 customers\"] --> A2[\"Pattern: Clear weekly cycle<br/>Prediction accuracy: Very high<br/>Business value: Excellent\"]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Low Explained Variance (20%)\"\n",
    "        B1[\"Day 1: 150 customers<br/>Day 2: 95 customers<br/>Day 3: 180 customers<br/>Day 4: 110 customers<br/>Day 5: 200 customers<br/>Day 6: 85 customers<br/>Day 7: 160 customers\"] --> B2[\"Pattern: Mostly random<br/>Prediction accuracy: Poor<br/>Business value: Limited\"]\n",
    "    end\n",
    "    \n",
    "    style A2 fill:#c8e6c9,color:#000\n",
    "    style B2 fill:#ffcdd2,color:#000\n",
    "```\n",
    "## **Interpretation Framework**\n",
    "\n",
    "### **Explained Variance Levels**\n",
    "\n",
    "| **Range** | **Interpretation** | **Pattern Strength** | **Business Implication** |\n",
    "|-----------|-------------------|---------------------|--------------------------|\n",
    "| **80-100%** | Extremely high | Very dominant pattern | Highly predictable, strong business rules |\n",
    "| **60-80%** | High | Strong pattern | Good forecasting potential |\n",
    "| **40-60%** | Moderate | Noticeable pattern | Some predictive value |\n",
    "| **20-40%** | Low | Weak pattern | Limited practical use |\n",
    "| **0-20%** | Very low | Barely detectable | Mostly noise |\n",
    "\n",
    "## **Why Explained Variance Matters**\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((Explained Variance))\n",
    "    Pattern Strength\n",
    "      Dominant vs Weak\n",
    "      Signal vs Noise\n",
    "      Reliability Assessment\n",
    "    Predictive Power\n",
    "      Forecasting Accuracy\n",
    "      Model Performance\n",
    "      Confidence Levels\n",
    "    Business Value\n",
    "      Resource Planning\n",
    "      Strategic Decisions\n",
    "      Risk Assessment\n",
    "    Research Priority\n",
    "      Focus Areas\n",
    "      Investigation Depth\n",
    "      Analysis Investment\n",
    "```\n",
    "\n",
    "## **Real-World Applications**\n",
    "\n",
    "### **Retail Example**\n",
    "- **Total variance**: Daily sales range from $1,000 to $10,000\n",
    "- **Seasonal pattern explains 70%**: Christmas season, back-to-school, etc.\n",
    "- **Day-of-week pattern explains 15%**: Weekend vs weekday differences  \n",
    "- **Unexplained variance 15%**: Random daily fluctuations\n",
    "\n",
    "### **Website Traffic Example**\n",
    "- **Total variance**: Daily visitors range from 500 to 5,000\n",
    "- **Marketing campaigns explain 60%**: Planned promotional activities\n",
    "- **Weekly cycle explains 25%**: Consistent weekday patterns\n",
    "- **Unexplained variance 15%**: Viral content, external events\n",
    "\n",
    "## **Key Insights**\n",
    "\n",
    "### **ðŸŽ¯ Pattern Prioritization**\n",
    "**High explained variance = High priority patterns**\n",
    "- Focus analytical resources on patterns that explain the most variation\n",
    "- These patterns provide the best return on investment for business decisions\n",
    "\n",
    "### **ðŸ“Š Model Quality Assessment**\n",
    "**Explained variance indicates how well your analysis captures reality**\n",
    "- High explained variance = Good model fit\n",
    "- Low explained variance = Missing important factors or mostly random data\n",
    "\n",
    "### **ðŸ”® Forecasting Reliability**\n",
    "**Higher explained variance = More reliable predictions**\n",
    "- 80% explained variance â†’ Confident forecasts\n",
    "- 30% explained variance â†’ Uncertain forecasts\n",
    "\n",
    "### **ðŸ’¼ Business Decision Confidence**\n",
    "**Explained variance directly relates to decision-making confidence**\n",
    "- High explained variance â†’ Make strategic investments\n",
    "- Low explained variance â†’ Proceed with caution, gather more data\n",
    "\n",
    "## **Common Misconceptions**\n",
    "\n",
    "| **Misconception** | **Reality** |\n",
    "|-------------------|-------------|\n",
    "| \"100% explained variance is always best\" | May indicate overfitting or unrealistic assumptions |\n",
    "| \"Low explained variance means bad analysis\" | Some phenomena are inherently unpredictable |\n",
    "| \"Explained variance equals causation\" | Correlation doesn't imply causation |\n",
    "| \"One pattern should explain everything\" | Multiple patterns often combine to explain total variance |\n",
    "\n",
    "## **Practical Takeaway**\n",
    "\n",
    "**Explained variance is your \"confidence meter\" for patterns in data.**\n",
    "\n",
    "- **High explained variance** â†’ Trust the pattern, make decisions based on it\n",
    "- **Low explained variance** â†’ Be cautious, look for additional factors or accept uncertainty\n",
    "\n",
    "It's the difference between having a reliable roadmap (high explained variance) versus navigating in fog (low explained variance)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
