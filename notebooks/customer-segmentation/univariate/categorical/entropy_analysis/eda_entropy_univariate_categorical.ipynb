{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d6a08f",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## 🔬 Entropy & Information Theory Analysis - Categorical Data Deep Dive\n",
    "\n",
    "### 1. Setup & Data Foundation\n",
    "- [Project Configuration & Environment](#project-configuration--environment)\n",
    "- [Data Loading & Validation](#data-loading--validation)\n",
    "- [Feature Type Classification](#feature-type-classification)\n",
    "- [Missing Data Assessment](#missing-data-assessment)\n",
    "- [Analysis Results Structure](#analysis-results-structure)\n",
    "\n",
    "### 2. Comprehensive Descriptive Statistics Framework\n",
    "- [2.1 Complete Frequency Analysis Guide](#complete-frequency-analysis-guide)\n",
    "  - Basic counts and proportions methodology\n",
    "  - Advanced frequency analysis techniques\n",
    "  - Distribution quality metrics and assessment\n",
    "  - Implementation examples and best practices\n",
    "- [2.2 Statistical Summary Metrics](#statistical-summary-metrics)\n",
    "  - Range and concentration measures\n",
    "  - Evenness and richness calculations\n",
    "  - Effective categories computation\n",
    "  - Cross-tabulation analysis methods\n",
    "- [2.3 Business-Specific Applications](#business-specific-applications)\n",
    "  - Market share analysis frameworks\n",
    "  - Customer segment sizing strategies\n",
    "  - Quality assessment indicators\n",
    "  - Practical business applications\n",
    "\n",
    "### 3. Information Theory & Entropy Deep Dive\n",
    "- [3.1 Shannon Entropy (Information Entropy)](#shannon-entropy)\n",
    "  - **Mathematical Foundation & Theory**\n",
    "    - Formula breakdown: H(X) = -∑ p(x) * log₂(p(x))\n",
    "    - Parameter explanation and significance\n",
    "    - Information content and \"surprise\" concepts\n",
    "  - **Implementation & Parameter Selection**\n",
    "    - Base selection (bits, nats, dits) guidelines\n",
    "    - SciPy entropy documentation and usage\n",
    "    - Practical implementation examples\n",
    "  - **Interpretation Framework**\n",
    "    - Range analysis (0 to log₂(n))\n",
    "    - Perfect order vs perfect disorder\n",
    "    - Business implications for customer data\n",
    "\n",
    "- [3.2 Rényi Entropy (Generalized Entropy)](#rényi-entropy)\n",
    "  - **Mathematical Foundation**\n",
    "    - Formula: H_α(X) = (1/(1-α)) * log(∑ p(x)^α)\n",
    "    - Order parameter (α) significance and effects\n",
    "    - Special cases and their applications\n",
    "  - **Parameter Selection Strategy**\n",
    "    - α = 0 (Hartley entropy) applications\n",
    "    - α = 1 (Shannon entropy limit case)\n",
    "    - α = 2 (Collision entropy) use cases\n",
    "    - α → ∞ (Min-entropy) scenarios\n",
    "  - **Comprehensive Results Interpretation**\n",
    "    - α < 1: Emphasizing rare events\n",
    "    - α > 1: Emphasizing common events\n",
    "    - Business interpretation matrix\n",
    "    - Customer segmentation implications\n",
    "\n",
    "- [3.3 Gini Impurity Analysis](#gini-impurity)\n",
    "  - **Decision Tree Perspective**\n",
    "    - Formula: Gini = 1 - ∑ p(x)²\n",
    "    - Purity vs impurity concepts\n",
    "    - Classification uncertainty measurement\n",
    "  - **Business Applications**\n",
    "    - Customer behavior predictability\n",
    "    - Market segmentation effectiveness\n",
    "    - A/B testing implications\n",
    "\n",
    "- [3.4 Simpson's Diversity Index Suite](#simpsons-diversity-index)\n",
    "  - **Three Variants Analysis**\n",
    "    - Simpson's Index (D): Dominance probability\n",
    "    - Simpson's Diversity (1-D): Different category probability\n",
    "    - Simpson's Reciprocal (1/D): Effective number of categories\n",
    "  - **Ecological vs Business Interpretation**\n",
    "    - Customer base diversity assessment\n",
    "    - Market representation analysis\n",
    "    - Statistical power implications\n",
    "\n",
    "### 4. Algorithmic Information Theory Applications\n",
    "- [4.1 Kolmogorov Complexity Approximation](#kolmogorov-complexity)\n",
    "  - **Theoretical Foundation**\n",
    "    - Compression-based approximation methods\n",
    "    - Randomness and pattern detection principles\n",
    "    - Data structure overhead analysis\n",
    "  - **Practical Implementation**\n",
    "    - Compression ratio interpretation (0.4966 analysis)\n",
    "    - Business quality insights\n",
    "    - Comparison with different data types\n",
    "  - **Limitations and Considerations**\n",
    "    - Method dependencies and caveats\n",
    "    - Sample size effects\n",
    "    - Format-dependent results\n",
    "\n",
    "- [4.2 Normalized Compression Distance (NCD)](#normalized-compression-distance)\n",
    "  - **Mathematical Framework**\n",
    "    - Formula: NCD(x,y) = (C(xy) - min(C(x), C(y))) / max(C(x), C(y))\n",
    "    - String-based vs pickle-based methods\n",
    "    - Cross-variable similarity measurement\n",
    "  - **Implementation Approaches**\n",
    "    - Robust pandas Series handling\n",
    "    - Error prevention strategies\n",
    "    - Method comparison and validation\n",
    "  - **Results Interpretation**\n",
    "    - Independence detection (NCD > 1.0)\n",
    "    - Similarity assessment frameworks\n",
    "    - Business relationship implications\n",
    "\n",
    "### 5. Cross-Variable Dependency Analysis\n",
    "- [5.1 Mutual Information Analysis](#mutual-information-analysis)\n",
    "  - **Information-Theoretic Foundation**\n",
    "    - Formula: I(X;Y) = ∑∑ p(x,y) * log(p(x,y) / (p(x) * p(y)))\n",
    "    - Dependency measurement principles\n",
    "    - Non-linear relationship detection\n",
    "  - **Implementation Considerations**\n",
    "    - Discrete vs continuous variable handling\n",
    "    - Binning effects and limitations\n",
    "    - Method sensitivity analysis\n",
    "  - **Business Interpretation Framework**\n",
    "    - Perfect independence (MI = 0.0000) implications\n",
    "    - Customer demographic insights\n",
    "    - Marketing strategy applications\n",
    "\n",
    "- [5.2 Cross-Variable Validation Methods](#cross-variable-validation)\n",
    "  - Alternative correlation approaches\n",
    "  - Statistical test confirmations\n",
    "  - Visual inspection techniques\n",
    "  - Different MI estimator comparisons\n",
    "\n",
    "### 6. Advanced Parameter Selection & Method Guidelines\n",
    "- [6.1 Entropy Parameter Optimization](#entropy-parameter-optimization)\n",
    "  - **Sample Size Considerations**\n",
    "    - Small samples (n < 100): Bias correction needs\n",
    "    - Large samples: Standard estimator performance\n",
    "    - Optimal threshold identification\n",
    "  - **Category Count Impact**\n",
    "    - Few categories (< 10): Method selection\n",
    "    - Many categories (> 50): Rényi considerations\n",
    "    - Effective category calculations\n",
    "  - **Distribution Shape Effects**\n",
    "    - Uniform-like: Shannon entropy optimization\n",
    "    - Heavy-tailed: Rényi entropy advantages\n",
    "    - Sparse distributions: Min-entropy applications\n",
    "\n",
    "- [6.2 Binning Strategies for Numerical Data](#binning-strategies)\n",
    "  - **Comprehensive Binning Methods**\n",
    "    - Sturges' Rule: k = 1 + log₂(n)\n",
    "    - Scott's Rule: h = 3.5σ/n^(1/3)\n",
    "    - Freedman-Diaconis: h = 2×IQR/n^(1/3)\n",
    "  - **Method Selection Framework**\n",
    "    - Data distribution assessment\n",
    "    - Outlier impact analysis\n",
    "    - Validation criteria and checks\n",
    "  - **Decision Flowchart Implementation**\n",
    "    - Systematic method selection\n",
    "    - Validation frameworks\n",
    "    - Business context considerations\n",
    "\n",
    "- [6.3 Compression Algorithm Selection](#compression-algorithm-selection)\n",
    "  - zlib vs bz2 vs lzma comparison\n",
    "  - Data type optimization strategies\n",
    "  - Performance vs accuracy trade-offs\n",
    "\n",
    "### 7. Comprehensive Interpretation Framework\n",
    "- [7.1 Scale Interpretation Guidelines](#scale-interpretation)\n",
    "  - **Entropy Value Ranges**\n",
    "    - Shannon entropy: 0 to log₂(n) interpretation\n",
    "    - Gini impurity: 0 to (1-1/n) analysis\n",
    "    - Simpson's indices: Multiple scale understanding\n",
    "  - **Business Threshold Identification**\n",
    "    - Excellent balance indicators (>98% of maximum)\n",
    "    - Moderate imbalance detection\n",
    "    - Severe bias warning signs\n",
    "  - **Cross-Method Validation**\n",
    "    - Consistency checking across measures\n",
    "    - Conflicting results resolution\n",
    "    - Method-specific strengths utilization\n",
    "\n",
    "- [7.2 Method Comparison Matrix](#method-comparison-matrix)\n",
    "  - **Shannon vs Rényi vs Gini Analysis**\n",
    "    - Computational efficiency comparison\n",
    "    - Sensitivity analysis results\n",
    "    - Use case recommendations\n",
    "  - **Strengths and Limitations Framework**\n",
    "    - Method-specific advantages\n",
    "    - Known limitations and caveats\n",
    "    - Complementary usage strategies\n",
    "\n",
    "- [7.3 Data Type Impact Analysis](#data-type-impact)\n",
    "  - **Categorical vs Numerical Considerations**\n",
    "    - Direct application scenarios\n",
    "    - Preprocessing requirements\n",
    "    - Interpretation differences\n",
    "  - **Mixed Data Type Handling**\n",
    "    - Cross-type comparison strategies\n",
    "    - Standardization approaches\n",
    "    - Domain knowledge integration\n",
    "\n",
    "### 8. Practical Applications & Business Case Studies\n",
    "- [8.1 Customer Segmentation Applications](#customer-segmentation-applications)\n",
    "  - **Gender Distribution Analysis**\n",
    "    - Near-perfect balance interpretation (entropy ≈ 0.99)\n",
    "    - Business implications and strategies\n",
    "    - Marketing approach recommendations\n",
    "  - **Market Balance Assessment**\n",
    "    - Demographic bias detection methods\n",
    "    - Sampling quality evaluation\n",
    "    - Representativeness validation\n",
    "\n",
    "- [8.2 Quality Assessment Framework](#quality-assessment-framework)\n",
    "  - **Data Completeness Evaluation**\n",
    "    - Missing value impact analysis\n",
    "    - Collection consistency validation\n",
    "    - Bias detection methodologies\n",
    "  - **Statistical Power Calculations**\n",
    "    - A/B testing sample size requirements\n",
    "    - Comparative analysis capabilities\n",
    "    - Minimum detectable effect sizing\n",
    "\n",
    "- [8.3 Decision Support Systems](#decision-support-systems)\n",
    "  - **Marketing Strategy Implications**\n",
    "    - Age-agnostic gender targeting\n",
    "    - Universal product appeal assessment\n",
    "    - Campaign flexibility analysis\n",
    "  - **Business Strategy Applications**\n",
    "    - Demographic independence advantages\n",
    "    - Market expansion considerations\n",
    "    - Risk assessment frameworks\n",
    "\n",
    "### 9. Advanced Method Selection Framework\n",
    "- [9.1 Purpose-Driven Selection Guide](#purpose-driven-selection)\n",
    "  - **Exploration vs Optimization**\n",
    "    - Shannon entropy for exploration\n",
    "    - Gini impurity for ML optimization\n",
    "    - Simpson's diversity for communication\n",
    "  - **Context-Specific Recommendations**\n",
    "    - Academic vs business contexts\n",
    "    - Technical vs non-technical audiences\n",
    "    - Research vs operational applications\n",
    "\n",
    "- [9.2 Systematic Selection Flowcharts](#systematic-selection-flowcharts)\n",
    "  - **Data Characteristic Assessment**\n",
    "    - Normality testing integration\n",
    "    - Outlier detection workflows\n",
    "    - Skewness impact evaluation\n",
    "  - **Method Validation Frameworks**\n",
    "    - Result consistency checking\n",
    "    - Business sense validation\n",
    "    - Statistical assumption verification\n",
    "\n",
    "### 10. Advanced Resources & Documentation\n",
    "- [10.1 Mathematical Foundations](#mathematical-foundations)\n",
    "  - **Information Theory Resources**\n",
    "    - Shannon's original papers\n",
    "    - Cover & Thomas textbook references\n",
    "    - MIT OpenCourseWare materials\n",
    "  - **Algorithmic Information Theory**\n",
    "    - Kolmogorov complexity introductions\n",
    "    - Compression theory foundations\n",
    "    - Universal similarity metrics\n",
    "\n",
    "- [10.2 Implementation Resources](#implementation-resources)\n",
    "  - **SciPy Documentation Links**\n",
    "    - entropy function specifications\n",
    "    - Statistical test implementations\n",
    "    - Parameter optimization guides\n",
    "  - **Scikit-learn Integration**\n",
    "    - Mutual information implementations\n",
    "    - Feature selection applications\n",
    "    - Cross-validation strategies\n",
    "\n",
    "- [10.3 Research Papers & Advanced Reading](#research-papers)\n",
    "  - **Entropy Estimation Surveys**\n",
    "    - Bias correction methodologies\n",
    "    - Finite sample considerations\n",
    "    - Robust estimation techniques\n",
    "  - **Information Theory in Machine Learning**\n",
    "    - Feature selection applications\n",
    "    - Model selection criteria\n",
    "    - Regularization connections\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Purpose**: This notebook provides an exhaustive exploration of entropy, information theory, and statistical diversity measures for categorical data analysis, with comprehensive mathematical foundations, practical implementations, and business applications.\n",
    "\n",
    "**Key Variable Analyzed**: Gender (binary categorical) - demonstrating near-perfect balance across all measures\n",
    "\n",
    "**Statistical Measures Covered**: Shannon Entropy, Rényi Entropy (multiple α), Gini Impurity, Simpson's Diversity Suite, Kolmogorov Complexity, Mutual Information, NCD\n",
    "\n",
    "**Business Applications**: Customer segmentation, market analysis, demographic assessment, data quality evaluation, A/B testing design, marketing strategy optimization\n",
    "\n",
    "**Advanced Features**: Parameter selection frameworks, method comparison matrices, systematic selection flowcharts, comprehensive interpretation guidelines\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c3f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project configuration:\n",
      "SLUG = customer-segmentation\n",
      "DATA_DIR = /Users/ravisharma/workdir/eda_practice/data/customer-segmentation\n",
      "DATASET_KEY = vjchoudhary7/customer-segmentation-tutorial-in-python\n",
      "FIG_DIR = /Users/ravisharma/workdir/eda_practice/figures/customer-segmentation\n",
      "REP_DIR = /Users/ravisharma/workdir/eda_practice/reports/customer-segmentation\n",
      "NOTEBOOK_DIR = /Users/ravisharma/workdir/eda_practice/notebooks/customer-segmentation\n",
      "Vars not found in globals: []\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "\n",
    "print(\"Project configuration:\")\n",
    "print(f\"SLUG = {SLUG}\")\n",
    "print(f\"DATA_DIR = {DATA_DIR}\")\n",
    "print(f\"DATASET_KEY = {DATASET_KEY}\")\n",
    "print(f\"FIG_DIR = {FIG_DIR}\")\n",
    "print(f\"REP_DIR = {REP_DIR}\")\n",
    "print(f\"NOTEBOOK_DIR = {NOTEBOOK_DIR}\")\n",
    "\n",
    "missing_vars = [var for var in ['SLUG', 'DATA_DIR', 'FIG_DIR', 'REP_DIR', 'NOTEBOOK_DIR', 'DATASET_KEY'] if var not in globals()]\n",
    "print(f\"Vars not found in globals: {missing_vars}\")\n",
    "\n",
    "# Set default values if variables are not found in store or are empty\n",
    "if not SLUG:  # Check if empty string\n",
    "    print(f\"{SLUG=} is empty, initializing everything explicitly\")\n",
    "    SLUG = 'customer-segmentation'\n",
    "    DATASET_KEY = 'vjchoudhary7/customer-segmentation-tutorial-in-python'\n",
    "    GIT_ROOT = Path.cwd().parent.parent\n",
    "    DATA_DIR = GIT_ROOT / 'data' / SLUG\n",
    "    FIG_DIR = GIT_ROOT / 'figures' / SLUG\n",
    "    REP_DIR = GIT_ROOT / 'reports' / SLUG\n",
    "    NOTEBOOK_DIR = GIT_ROOT / 'notebooks' / SLUG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7982eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c1302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV /Users/ravisharma/workdir/eda_practice/data/customer-segmentation/Mall_Customers.csv loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
       "0           1    Male   19                  15                      39\n",
       "1           2    Male   21                  15                      81\n",
       "2           3  Female   20                  16                       6\n",
       "3           4  Female   23                  16                      77\n",
       "4           5  Female   31                  17                      40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading data\n",
    "\n",
    "base_df = pd.DataFrame()\n",
    "\n",
    "CSV_PATH = Path(DATA_DIR) / \"Mall_Customers.csv\"\n",
    "if not CSV_PATH.exists:\n",
    "    print(f\"CSV {CSV_PATH} does not exist. base_df will remain empty.\")\n",
    "else:\n",
    "    base_df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"CSV {CSV_PATH} loaded successfully.\")\n",
    "\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11936672",
   "metadata": {},
   "source": [
    "# **Complete Section: Descriptive Statistics & Frequency Analysis**\n",
    "\n",
    "## **1. Descriptive Statistics & Frequency Analysis**\n",
    "\n",
    "### **Basic Counts:**\n",
    "- **`value_counts()`** - frequency of each category\n",
    "- **`value_counts(normalize=True)`** - proportions/percentages  \n",
    "- **`nunique()`** - number of unique categories\n",
    "- **`unique()`** - list of unique values\n",
    "- **Missing value analysis** (`isnull().sum()`)\n",
    "\n",
    "### **Advanced Frequency Analysis:**\n",
    "- **Mode** - most frequent category (`mode()[0]`)\n",
    "- **Frequency tables** with cumulative counts\n",
    "- **Cross-tabulations** with other categorical variables (`pd.crosstab()`)\n",
    "- **Contingency tables** for relationship analysis\n",
    "\n",
    "### **Distribution Metrics:**\n",
    "- **Uniformity assessment** - how evenly distributed are categories\n",
    "- **Dominance ratio** - proportion of most frequent category\n",
    "- **Minority representation** - smallest category proportion\n",
    "- **Category concentration** - distribution spread analysis\n",
    "\n",
    "### **Implementation Examples:**\n",
    "\n",
    "```python\n",
    "# Basic frequency analysis\n",
    "print(\"=== Basic Frequency Analysis ===\")\n",
    "print(f\"Unique values: {df['Gender'].unique()}\")\n",
    "print(f\"Number of unique values: {df['Gender'].nunique()}\")\n",
    "print(f\"Missing values: {df['Gender'].isnull().sum()}\")\n",
    "\n",
    "# Value counts (absolute and relative)\n",
    "print(\"\\n=== Value Counts ===\")\n",
    "counts = df['Gender'].value_counts()\n",
    "proportions = df['Gender'].value_counts(normalize=True)\n",
    "print(\"Absolute counts:\")\n",
    "print(counts)\n",
    "print(\"\\nProportions:\")\n",
    "print(proportions)\n",
    "\n",
    "# Mode and distribution metrics\n",
    "print(f\"\\nMode: {df['Gender'].mode()[0]}\")\n",
    "print(f\"Most frequent count: {counts.iloc[0]}\")\n",
    "print(f\"Dominance ratio: {proportions.iloc[0]:.4f}\")\n",
    "print(f\"Minority representation: {proportions.iloc[-1]:.4f}\")\n",
    "\n",
    "# Cumulative frequency analysis\n",
    "cumulative = counts.cumsum()\n",
    "cumulative_pct = (cumulative / len(df)) * 100\n",
    "print(\"\\nCumulative Analysis:\")\n",
    "for cat, cum_count, cum_pct in zip(counts.index, cumulative, cumulative_pct):\n",
    "    print(f\"{cat}: {cum_count} ({cum_pct:.1f}%)\")\n",
    "\n",
    "# Advanced metrics\n",
    "hhi = sum(proportions ** 2)  # Herfindahl-Hirschman Index\n",
    "evenness = 1 - hhi  # Evenness measure\n",
    "print(f\"\\nAdvanced Metrics:\")\n",
    "print(f\"Herfindahl-Hirschman Index: {hhi:.4f}\")\n",
    "print(f\"Evenness Score: {evenness:.4f}\")\n",
    "print(f\"Effective Number of Categories: {1/hhi:.2f}\")\n",
    "```\n",
    "\n",
    "### **Statistical Summary Metrics:**\n",
    "- **Range**: Number of categories (categorical \"spread\")\n",
    "- **Concentration**: Herfindahl-Hirschman Index for market concentration\n",
    "- **Evenness**: How equally distributed categories are (1 - HHI)\n",
    "- **Richness**: Total number of distinct categories present\n",
    "- **Effective Categories**: 1/HHI - equivalent number of equally-sized categories\n",
    "\n",
    "### **Cross-Tabulation Analysis:**\n",
    "- **Bivariate frequency tables** between categorical variables\n",
    "- **Joint distributions** and marginal totals\n",
    "- **Conditional probabilities** P(A|B) calculations\n",
    "- **Independence testing** via chi-square tests\n",
    "\n",
    "### **Business-Specific Metrics:**\n",
    "- **Market share analysis** - category dominance in business context\n",
    "- **Customer segment sizes** - practical implications of category sizes\n",
    "- **Actionable thresholds** - minimum viable segment sizes\n",
    "- **Representativeness assessment** - sample vs population comparison\n",
    "\n",
    "### **Quality Assessment Indicators:**\n",
    "- **Completeness rate**: (Total - Missing) / Total\n",
    "- **Category balance**: Standard deviation of proportions\n",
    "- **Outlier categories**: Categories with unusually high/low frequencies\n",
    "- **Data consistency**: Check for similar categories with different spellings\n",
    "\n",
    "### **Practical Business Applications:**\n",
    "- **Segment sizing** for marketing budget allocation\n",
    "- **A/B testing** sample size requirements per category\n",
    "- **Statistical power** calculations for category comparisons\n",
    "- **Minimum detectable effect** sizes for each category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7301be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>38.850000</td>\n",
       "      <td>60.560000</td>\n",
       "      <td>50.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57.879185</td>\n",
       "      <td>13.969007</td>\n",
       "      <td>26.264721</td>\n",
       "      <td>25.823522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.750000</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>34.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>150.250000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CustomerID         Age  Annual Income (k$)  Spending Score (1-100)\n",
       "count  200.000000  200.000000          200.000000              200.000000\n",
       "mean   100.500000   38.850000           60.560000               50.200000\n",
       "std     57.879185   13.969007           26.264721               25.823522\n",
       "min      1.000000   18.000000           15.000000                1.000000\n",
       "25%     50.750000   28.750000           41.500000               34.750000\n",
       "50%    100.500000   36.000000           61.500000               50.000000\n",
       "75%    150.250000   49.000000           78.000000               73.000000\n",
       "max    200.000000   70.000000          137.000000               99.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ba7d208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical_features=['CustomerID', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
      "categorical_features=['Gender']\n",
      "datetime_features=[]\n",
      "id_column='CustomerID'\n",
      "Dropping id columns...\n",
      "numerical_features=['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n"
     ]
    }
   ],
   "source": [
    "base_df.dtypes\n",
    "numerical_features = base_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = base_df.select_dtypes(include=['object']).columns.tolist()\n",
    "datetime_features = base_df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "id_column = 'CustomerID'\n",
    "print(f\"{numerical_features=}\")\n",
    "print(f\"{categorical_features=}\")\n",
    "print(f\"{datetime_features=}\")\n",
    "print(f\"{id_column=}\")\n",
    "print(\"Dropping id columns...\")\n",
    "numerical_features.remove(id_column)\n",
    "print(f\"{numerical_features=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41f50ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing_data=CustomerID                0\n",
      "Gender                    0\n",
      "Age                       0\n",
      "Annual Income (k$)        0\n",
      "Spending Score (1-100)    0\n",
      "dtype: int64\n",
      "missing_data_pct=CustomerID                0.0\n",
      "Gender                    0.0\n",
      "Age                       0.0\n",
      "Annual Income (k$)        0.0\n",
      "Spending Score (1-100)    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_data = base_df.isnull().sum() # a dataframe\n",
    "missing_data_pct = missing_data / len(base_df) * 100 # a dataframe with operation done column wise\n",
    "print(f\"{missing_data=}\")\n",
    "print(f\"{missing_data_pct=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f81cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b00c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results['dataset'] = {\n",
    "    \"missing_data\": missing_data.to_dict(),\n",
    "    \"missing_data_pct\": missing_data_pct.to_dict(),\n",
    "    \"numerical_features\": numerical_features,\n",
    "    \"categorical_features\": categorical_features,\n",
    "    \"datetime_features\": datetime_features,\n",
    "    \"shape\": base_df.shape\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd88e3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gender']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce11ea6",
   "metadata": {},
   "source": [
    "## Categorical Column Analysis\n",
    "\n",
    "1. Frequency Analysis\n",
    "    1. value counts\n",
    "    2. uniques\n",
    "    3. cross tabulations\n",
    "2. Statistical Tests\n",
    "    1. chi square goodness of fit\n",
    "    2. entropy / measure of randomness\n",
    "3. Visualizations\n",
    "    1. bar plots\n",
    "    2. pie charts\n",
    "    3. donut charts\n",
    "\n",
    "----\n",
    "\n",
    "## LLM Response\n",
    "Based on the notebook structure I can see and best practices for categorical EDA, here are the comprehensive analysis types that should be done for categorical features like `base_df['Gender']`:\n",
    "\n",
    "## **1. Descriptive Statistics & Frequency Analysis**\n",
    "\n",
    "**Basic Counts:** \n",
    "\n",
    "## **2. Statistical Measures**\n",
    "\n",
    "**Diversity & Randomness:**\n",
    "- **Entropy** - measures randomness/information content\n",
    "- **Gini coefficient** - measures inequality in distribution\n",
    "- **Simpson's diversity index** - probability two random samples are different\n",
    "- **Shannon diversity index** - accounts for both richness and evenness\n",
    "\n",
    "**Statistical Tests:**\n",
    "- **Chi-square goodness of fit** - test if distribution matches expected\n",
    "- **Chi-square test of independence** - relationship with other categorical variables\n",
    "- **Cramér's V** - measure of association strength (0-1 scale)\n",
    "\n",
    "## **3. Visualizations**\n",
    "\n",
    "**Basic Plots:**\n",
    "- **Bar plots** - frequency counts (vertical/horizontal)\n",
    "- **Pie charts** - proportional representation\n",
    "- **Donut charts** - pie chart with center removed\n",
    "\n",
    "**Advanced Visualizations:**\n",
    "- **Stacked bar charts** - comparing with other categorical variables\n",
    "- **Grouped bar charts** - side-by-side comparisons\n",
    "- **Heatmaps** - for cross-tabulations\n",
    "- **Waffle charts** - alternative to pie charts\n",
    "- **Treemaps** - hierarchical proportional representation\n",
    "\n",
    "## **4. Distribution Analysis**\n",
    "\n",
    "**Balance Assessment:**\n",
    "- Check for class imbalance\n",
    "- Identify dominant/minority categories\n",
    "- Calculate imbalance ratios\n",
    "\n",
    "**Pattern Recognition:**\n",
    "- Seasonal patterns (if applicable)\n",
    "- Temporal trends (if time-based)\n",
    "- Geographic patterns (if location-based)\n",
    "\n",
    "## **5. Relationship Analysis**\n",
    "\n",
    "**With Other Categorical Variables:**\n",
    "- Cross-tabulations\n",
    "- Contingency tables\n",
    "- Association rules\n",
    "- Correspondence analysis\n",
    "\n",
    "**With Numerical Variables:**\n",
    "- Group statistics (mean, median by category)\n",
    "- Box plots by category\n",
    "- Violin plots by category\n",
    "- ANOVA F-test for group differences\n",
    "\n",
    "## **6. Data Quality Checks**\n",
    "\n",
    "**Consistency:**\n",
    "- Case sensitivity issues\n",
    "- Spelling variations\n",
    "- Encoding problems\n",
    "- Standardization needs\n",
    "\n",
    "**Completeness:**\n",
    "- Missing value patterns\n",
    "- \"Unknown\" or \"Other\" categories\n",
    "- Data collection biases\n",
    "\n",
    "## **For Gender Specifically:**\n",
    "\n",
    "**Expected Categories:**\n",
    "- Male/Female (binary)\n",
    "- Male/Female/Other/Prefer not to say (inclusive)\n",
    "- Check for data entry inconsistencies\n",
    "\n",
    "**Typical Analysis:**\n",
    "- Gender distribution (should this be balanced?)\n",
    "- Gender vs spending patterns\n",
    "- Gender vs age groups\n",
    "- Gender vs income levels\n",
    "\n",
    "**Business Questions:**\n",
    "- Is there gender bias in the dataset?\n",
    "- Do spending patterns differ by gender?\n",
    "- Are there age-gender interaction effects?\n",
    "- How does gender relate to customer segments?\n",
    "\n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5751be",
   "metadata": {},
   "source": [
    "## **1. Descriptive Statistics & Frequency Analysis**\n",
    "\n",
    "**Basic Counts:** \n",
    "\n",
    "## **2. Statistical Measures**\n",
    "\n",
    "**Diversity & Randomness:**\n",
    "- **Entropy** - measures randomness/information content\n",
    "- **Gini coefficient** - measures inequality in distribution\n",
    "- **Simpson's diversity index** - probability two random samples are different\n",
    "- **Shannon diversity index** - accounts for both richness and evenness\n",
    "\n",
    "**Statistical Tests:**\n",
    "- **Chi-square goodness of fit** - test if distribution matches expected\n",
    "- **Chi-square test of independence** - relationship with other categorical variables\n",
    "- **Cramér's V** - measure of association strength (0-1 scale)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01b9ef",
   "metadata": {},
   "source": [
    "# Entropy, Randomness & Information Content Analysis\n",
    "\n",
    "## **1. Shannon Entropy (Information Entropy)**\n",
    "\n",
    "### **Mathematical Foundation**\n",
    "Shannon entropy measures the average information content in a dataset:\n",
    "\n",
    "```plaintext\n",
    "H(X) = -∑ p(x) * log₂(p(x))\n",
    "```\n",
    "Here’s what each part means:\n",
    "\n",
    "- H(X): This is the **entropy** of the random variable X. It's the final value that quantifies the average uncertainty of the system in bits.\n",
    "\n",
    "- X: This represents **a random variable**, which is just a way of describing a system with a set of possible outcomes. For example, a coin flip has the possible outcomes {Heads, Tails}.\n",
    "\n",
    "- x: This represents **a single, specific outcome from the set of possible outcomes in X**. For example, 'Heads' is a specific outcome of a coin flip.\n",
    "\n",
    "- p(x): This is the **probability of the specific outcome 'x' occurring**. For a fair coin, the probability of 'Heads' is 0.5, so p(Heads) = 0.5.\n",
    "\n",
    "- Σ (Sigma): This is the summation symbol. It means you perform the calculation that follows for every possible outcome 'x' and then add all the results together.\n",
    "\n",
    "- log₂: This is the base-2 logarithm. In the context of information theory, the logarithm is used to measure information content. The term **`-log₂(p(x))` is often called the \"surprise\" or \"information content\" of a single outcome**.\n",
    "\n",
    "    - An event with a low probability (like winning the lottery) has **a high degree of \"surprise\"** and thus **provides more information** when it occurs.\n",
    "    - An event with a high probability (like the sun rising tomorrow) has **a low degree of \"surprise\"** and **provides very little information**.\n",
    "\n",
    "In simple terms, the formula calculates **the average \"surprise\" you can expect** from a random variable. It does this by taking the \"surprise\" of each outcome and weighting it by the probability of that outcome occurring.\n",
    "\n",
    "### **Implementation & Parameters**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "\n",
    "def shannon_entropy(data, base=2):\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy\n",
    "    Parameters:\n",
    "    - base: logarithm base (2=bits, e=nats, 10=dits)\n",
    "    \"\"\"\n",
    "    counts = Counter(data)\n",
    "    probabilities = np.array(list(counts.values())) / len(data)\n",
    "    return entropy(probabilities, base=base)\n",
    "```\n",
    "\n",
    "### **Parameter Selection**\n",
    "- **Base 2**: Information in bits (most common)\n",
    "- **Base e**: Natural units (nats) - for mathematical analysis\n",
    "- **Base 10**: Decimal units (dits) - rarely used\n",
    "\n",
    "### **Interpretation**\n",
    "- **Range**: 0 to log₂(n) where n = number of categories\n",
    "- **0**: Perfect order (all same category)\n",
    "- **Maximum**: Perfect disorder (uniform distribution)\n",
    "- **Higher values**: More randomness/uncertainty\n",
    "\n",
    "### **Documentation & Resources**\n",
    "- [SciPy entropy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)\n",
    "- [Information Theory Primer](https://colah.github.io/posts/2015-09-Visual-Information/)\n",
    "- [Shannon's Original Paper](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Rényi Entropy (Generalized Entropy)**\n",
    "\n",
    "### **Mathematical Foundation**\n",
    "\n",
    "```plaintext\n",
    "H_α(X) = (1/(1-α)) * log(∑ p(x)^α)\n",
    "```\n",
    "\n",
    "### **Implementation**\n",
    "```python\n",
    "def renyi_entropy(data, alpha=2, base=2):\n",
    "    \"\"\"\n",
    "    Calculate Rényi entropy\n",
    "    Parameters:\n",
    "    - alpha: order parameter (α=1 gives Shannon entropy)\n",
    "    \"\"\"\n",
    "    counts = Counter(data)\n",
    "    probabilities = np.array(list(counts.values())) / len(data)\n",
    "    \n",
    "    if alpha == 1:\n",
    "        return shannon_entropy(data, base)\n",
    "    \n",
    "    sum_p_alpha = np.sum(probabilities ** alpha)\n",
    "    return (1 / (1 - alpha)) * np.log(sum_p_alpha) / np.log(base)\n",
    "```\n",
    "\n",
    "### **Parameter Selection**\n",
    "- **α = 0**: Hartley entropy (log of support size)\n",
    "- **α = 1**: Shannon entropy (limit case)\n",
    "- **α = 2**: Collision entropy (most common alternative)\n",
    "- **α → ∞**: Min-entropy (worst-case scenario)\n",
    "\n",
    "### **Interpretation**\n",
    "- **α < 1**: Emphasizes rare events\n",
    "- **α > 1**: Emphasizes common events\n",
    "- **Higher α**: More conservative entropy estimate\n",
    "\n",
    "### **Resources**\n",
    "- [Rényi Entropy Wikipedia](https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy)\n",
    "- [Entropy Measures in Machine Learning](https://arxiv.org/abs/1106.1791)\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Gini Impurity/Diversity Index**\n",
    "\n",
    "### **Mathematical Foundation**\n",
    "\n",
    "```plaintext\n",
    "Gini = 1 - ∑ p(x)²\n",
    "```\n",
    "\n",
    "\n",
    "### **Implementation**\n",
    "```python\n",
    "def gini_impurity(data):\n",
    "    \"\"\"Calculate Gini impurity\"\"\"\n",
    "    counts = Counter(data)\n",
    "    probabilities = np.array(list(counts.values())) / len(data)\n",
    "    return 1 - np.sum(probabilities ** 2)\n",
    "```\n",
    "\n",
    "### **Parameter Selection**\n",
    "- No parameters to tune\n",
    "- Works best with categorical data\n",
    "\n",
    "### **Interpretation**\n",
    "- **Range**: 0 to (1 - 1/n) where n = number of categories\n",
    "- **0**: Pure (all same category)\n",
    "- **Maximum**: Most diverse (uniform distribution)\n",
    "- **Used in**: Decision trees (CART algorithm)\n",
    "\n",
    "### **Resources**\n",
    "- [Gini Impurity Explained](https://victorzhou.com/blog/gini-impurity/)\n",
    "- [Decision Trees and Gini](https://scikit-learn.org/stable/modules/tree.html#mathematical-formulation)\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Simpson's Diversity Index**\n",
    "\n",
    "### **Mathematical Foundation**\n",
    "\n",
    "```plaintext\n",
    "D = ∑ p(x)²\n",
    "\n",
    "Simpson's Index = 1 - D (diversity)\n",
    "```\n",
    "\n",
    "\n",
    "### **Implementation**\n",
    "```python\n",
    "def simpson_diversity(data):\n",
    "    \"\"\"Calculate Simpson's diversity index\"\"\"\n",
    "    counts = Counter(data)\n",
    "    n = len(data)\n",
    "    D = sum((count * (count - 1)) for count in counts.values()) / (n * (n - 1))\n",
    "    return 1 - D  # Diversity (1 - dominance)\n",
    "```\n",
    "\n",
    "### **Interpretation**\n",
    "- **Range**: 0 to (1 - 1/n)\n",
    "- **0**: One category dominates completely\n",
    "- **Higher values**: More diverse\n",
    "- **Ecological interpretation**: Probability two random samples are different species\n",
    "\n",
    "### **Resources**\n",
    "- [Simpson's Index in Ecology](https://www.statology.org/simpsons-diversity-index/)\n",
    "- [Biodiversity Measures](https://bio.libretexts.org/Bookshelves/Ecology/Book%3A_Biodiversity_(Bynum)/07%3A_Measuring_Biodiversity/7.02%3A_Diversity_Indices)\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Kolmogorov Complexity (Algorithmic Information Theory)**\n",
    "\n",
    "### **Conceptual Foundation**\n",
    "- Measures shortest program that can generate the data\n",
    "- **Uncomputable** in general case\n",
    "- Approximated through compression algorithms\n",
    "\n",
    "### **Practical Approximation**\n",
    "```python\n",
    "import zlib\n",
    "import pickle\n",
    "\n",
    "def approximate_kolmogorov_complexity(data):\n",
    "    \"\"\"Approximate using compression ratio\"\"\"\n",
    "    serialized = pickle.dumps(data)\n",
    "    compressed = zlib.compress(serialized)\n",
    "    return len(compressed) / len(serialized)\n",
    "```\n",
    "\n",
    "### **Interpretation**\n",
    "- **Lower ratios**: More compressible (less random)\n",
    "- **Higher ratios**: Less compressible (more random)\n",
    "- **Limitation**: Depends on compression algorithm\n",
    "\n",
    "### **Resources**\n",
    "- [Kolmogorov Complexity Introduction](https://homepages.cwi.nl/~paulv/papers/info.pdf)\n",
    "- [Algorithmic Information Theory](https://www.cs.auckland.ac.nz/~chaitin/cup.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Mutual Information**\n",
    "\n",
    "### **Mathematical Foundation**\n",
    "\n",
    "```plaintext\n",
    "I(X;Y) = ∑∑ p(x,y) * log(p(x,y) / (p(x) * p(y)))\n",
    "```\n",
    "\n",
    "\n",
    "### **Implementation**\n",
    "```python\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "def mutual_information_analysis(data1, data2):\n",
    "    \"\"\"Calculate mutual information between two variables\"\"\"\n",
    "    return mutual_info_score(data1, data2)\n",
    "```\n",
    "\n",
    "### **Parameter Selection**\n",
    "- **Discrete**: Use as-is for categorical data\n",
    "- **Continuous**: Requires binning or KDE estimation\n",
    "\n",
    "### **Interpretation**\n",
    "- **0**: Variables are independent\n",
    "- **Higher values**: Stronger dependence\n",
    "- **Maximum**: Perfect dependence\n",
    "\n",
    "### **Resources**\n",
    "- [Scikit-learn Mutual Info](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html)\n",
    "- [Mutual Information Tutorial](https://machinelearningmastery.com/information-gain-and-mutual-information/)\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Normalized Compression Distance (NCD)**\n",
    "\n",
    "### **Implementation**\n",
    "```python\n",
    "def normalized_compression_distance(x, y, compressor=zlib.compress):\n",
    "    \"\"\"Calculate NCD between two sequences\"\"\"\n",
    "    cx = len(compressor(x))\n",
    "    cy = len(compressor(y))\n",
    "    cxy = len(compressor(x + y))\n",
    "    return (cxy - min(cx, cy)) / max(cx, cy)\n",
    "```\n",
    "\n",
    "### **Interpretation**\n",
    "- **Range**: 0 to 1\n",
    "- **0**: Sequences are identical\n",
    "- **1**: Sequences are completely different\n",
    "- **Universal**: Works for any data type\n",
    "\n",
    "---\n",
    "\n",
    "## **Data Type Considerations**\n",
    "\n",
    "### **Categorical Data**\n",
    "- **Direct application**: All entropy measures work directly\n",
    "- **No preprocessing**: Use frequency counts as-is\n",
    "- **Interpretation**: Straightforward probability-based\n",
    "\n",
    "### **Numerical Data**\n",
    "- **Binning required**: Convert to categorical first\n",
    "- **Bin selection critical**: \n",
    "  - Too few bins: Loss of information\n",
    "  - Too many bins: Sparse, unreliable estimates\n",
    "- **Methods**:\n",
    "  - Equal-width binning\n",
    "  - Equal-frequency binning (quantiles)\n",
    "  - Sturges' rule: k = 1 + log₂(n)\n",
    "  - Scott's rule: bin_width = 3.5σ/n^(1/3)\n",
    "\n",
    "### **Mixed Data**\n",
    "- **Discretization**: Convert numerical to categorical\n",
    "- **Standardization**: May be needed before binning\n",
    "- **Domain knowledge**: Use meaningful breakpoints\n",
    "\n",
    "---\n",
    "\n",
    "## **Parameter Selection Guidelines**\n",
    "\n",
    "### **For Entropy Measures**\n",
    "1. **Sample size considerations**:\n",
    "   - Small samples (n < 100): Use bias correction\n",
    "   - Large samples: Standard estimators work well\n",
    "\n",
    "2. **Number of categories**:\n",
    "   - Few categories (< 10): All methods work\n",
    "   - Many categories (> 50): Consider Rényi with α > 1\n",
    "\n",
    "3. **Distribution shape**:\n",
    "   - Uniform-like: Shannon entropy most informative\n",
    "   - Heavy-tailed: Rényi entropy with α > 1\n",
    "   - Sparse: Min-entropy (α → ∞)\n",
    "\n",
    "### **For Numerical Data Binning**\n",
    "```python\n",
    "# Automatic bin selection methods\n",
    "def optimal_bins(data, method='sturges'):\n",
    "    n = len(data)\n",
    "    if method == 'sturges':\n",
    "        return int(1 + np.log2(n))\n",
    "    elif method == 'scott':\n",
    "        h = 3.5 * np.std(data) / (n ** (1/3))\n",
    "        return int((np.max(data) - np.min(data)) / h)\n",
    "    elif method == 'freedman':\n",
    "        iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "        h = 2 * iqr / (n ** (1/3))\n",
    "        return int((np.max(data) - np.min(data)) / h)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Advanced Resources**\n",
    "\n",
    "### **Books**\n",
    "- \"Elements of Information Theory\" by Cover & Thomas\n",
    "- \"Information Theory, Inference, and Learning Algorithms\" by MacKay\n",
    "- \"The Information: A History, A Theory, A Flood\" by Gleick\n",
    "\n",
    "### **Online Courses**\n",
    "- [MIT Information Theory Course](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-441-information-theory-spring-2016/)\n",
    "- [Stanford CS229 Information Theory](http://cs229.stanford.edu/notes/cs229-notes-it.pdf)\n",
    "\n",
    "### **Research Papers**\n",
    "- [Entropy Estimation Survey](https://arxiv.org/abs/1811.04251)\n",
    "- [Information Theory in Machine Learning](https://arxiv.org/abs/1706.02263)\n",
    "\n",
    "This comprehensive framework allows you to analyze information content across different data types while understanding the theoretical foundations and practical considerations for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d7c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Gender' # the only categorical column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56af3492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9895875212220555\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(base_df[col])\n",
    "probabilities = np.array(list(counts.values())) / len(base_df[col])\n",
    "entropy_gender = entropy(probabilities, base=2)\n",
    "print(entropy_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c72588",
   "metadata": {},
   "source": [
    "## What does an entropy of 0.9895875212220555 mean?\n",
    "An entropy of 0.9896 for the 'Gender' column is very close to the maximum possible entropy for a binary categorical variable, which tells us a lot about the distribution of values in that column.\n",
    "\n",
    "Here's a breakdown of what it means:\n",
    "\n",
    "1. **Maximum Entropy**: For a column with two distinct categories (e.g., 'Male' and 'Female'), the maximum possible entropy is `log₂(2) = 1`. This occurs when the two categories are perfectly balanced (50% Male, 50% Female).\n",
    "\n",
    "2. **Your Value**: Your entropy value of ~0.99 is extremely close to 1.\n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "An entropy of **0.9896** for the 'Gender' column indicates that the distribution of genders in your dataset is **very close to being evenly split**. There is a high degree of uncertainty or randomness in this column, meaning that if you were to pick a person at random from your dataset, it would be very difficult to predict their gender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8723c",
   "metadata": {},
   "source": [
    "## Rényi entropy\n",
    "\n",
    "Rényi entropy, also known as the generalized entropy, is a family of functions that generalizes the concept of Shannon entropy. It provides a way to measure the diversity, uncertainty, or randomness of a system, with a tunable parameter that changes how different parts of the probability distribution are weighted.\n",
    "\n",
    "The formula for Rényi entropy is:\n",
    "\n",
    "```plaintext\n",
    "Hα(X) = (1 / (1 - α)) * log( Σi=1n piα )\n",
    "```\n",
    "\n",
    "Here is a breakdown of each parameter:\n",
    "\n",
    "- **Hα(X)**: This is the Rényi entropy of order α for a random variable X.\n",
    "\n",
    "- **α (alpha)**: This is the order of the entropy, and it's the key parameter that makes this a \"generalized\" entropy. It's a non-negative real number, where α ≠ 1. By changing α, you change how the entropy calculation is weighted:\n",
    "    - When **α > 1**, the entropy calculation gives more weight to the outcomes with **higher probabilities**.\n",
    "    - When **α < 1**, the calculation gives more weight to the outcomes with **lower probabilities**.\n",
    "\n",
    "- **X**: A random variable representing a set of possible outcomes {x₁, x₂, ..., xₙ}.\n",
    "\n",
    "- **pi**: The probability of the i-th outcome (p₁, p₂, ..., pₙ).\n",
    "\n",
    "- **Σ (Sigma)**: The summation symbol, indicating that you should sum the term piα over all possible outcomes i.\n",
    "\n",
    "- **log**: The logarithm. The base of the logarithm can be chosen (e.g., base 2, natural log), and this choice affects the units of the entropy (bits, nats, etc.).\n",
    "\n",
    "### The Significance of α\n",
    "The order α allows Rényi entropy to unify several other entropy measures as special cases:\n",
    "\n",
    "- **As α approaches 1**, Rényi entropy converges to Shannon Entropy.\n",
    "- When **α = 0**, it becomes the Hartley Entropy, which is simply the logarithm of the number of outcomes with a non-zero probability.\n",
    "- When **α = 2**, it is known as the Collision Entropy, which is important in fields like cryptography.\n",
    "- **As α approaches infinity**, it becomes the Min-Entropy, which is determined solely by the most likely outcome in the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11359652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon entropy:  0.9895875212220555\n",
      "Rényi entropy (alpha=0):  1.0\n",
      "Rényi entropy (alpha=2):  0.979373349410042\n",
      "Rényi entropy (alpha=10):  0.9156801746714123\n",
      "Rényi entropy (alpha=0.5):  0.9947780253507675\n",
      "Rényi entropy (alpha=0.1):  0.9989539865445485\n",
      "Rényi entropy (alpha=0.01):  0.9998953733628451\n"
     ]
    }
   ],
   "source": [
    "# Rényi Entropy\n",
    "counts = Counter(base_df['Gender'])\n",
    "probabilities = np.array(list(counts.values())) / len(base_df['Gender'])\n",
    "\n",
    "renyi_entropy = 0\n",
    "alpha = 1\n",
    "\n",
    "def renyi_entropy(probabilities, alpha=1):\n",
    "    if alpha == 1:\n",
    "        renyi_entropy = stats.entropy(probabilities, base=2) # shannon entropy\n",
    "    else:\n",
    "        sum_p_alpha = np.sum(probabilities ** alpha)\n",
    "        renyi_entropy = (1 / (1 - alpha)) * np.log(sum_p_alpha) / np.log(2)\n",
    "\n",
    "    return renyi_entropy\n",
    "\n",
    "print(\"Shannon entropy (alpha=1): \", renyi_entropy(probabilities, alpha=1))\n",
    "print(\"Rényi entropy (alpha=0): \", renyi_entropy(probabilities, alpha=0))\n",
    "print(\"Rényi entropy (alpha=2): \", renyi_entropy(probabilities, alpha=2))\n",
    "print(\"Rényi entropy (alpha=10): \", renyi_entropy(probabilities, alpha=10))\n",
    "print(\"Rényi entropy (alpha=0.5): \", renyi_entropy(probabilities, alpha=0.5))\n",
    "print(\"Rényi entropy (alpha=0.1): \", renyi_entropy(probabilities, alpha=0.1))\n",
    "print(\"Rényi entropy (alpha=0.01): \", renyi_entropy(probabilities, alpha=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f09e29a",
   "metadata": {},
   "source": [
    "# **Interpreting Rényi Entropy Results for `base_df['Gender']`**\n",
    "\n",
    "Based on your output values, here's how to interpret each Rényi entropy result:\n",
    "\n",
    "## **Your Results Summary:**\n",
    "```plaintext\n",
    "Shannon entropy (alpha=1): 0.9896\n",
    "Rényi entropy (alpha=0): 1.0000\n",
    "Rényi entropy (alpha=2): 0.9794\n",
    "Rényi entropy (alpha=10): 0.9157\n",
    "Rényi entropy (alpha=0.5): 0.9948\n",
    "Rényi entropy (alpha=0.1): 0.9990\n",
    "Rényi entropy (alpha=0.01): 0.9999\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **1. α = 1 (Shannon Entropy): 0.9896**\n",
    "- Shannon entropy (alpha=1): 0.9896\n",
    "- Rényi entropy (alpha=0): 1.0000\n",
    "- Rényi entropy (alpha=2): 0.9794\n",
    "- Rényi entropy (alpha=10): 0.9157\n",
    "- Rényi entropy (alpha=0.5): 0.9948\n",
    "- Rényi entropy (alpha=0.1): 0.9990\n",
    "- Rényi entropy (alpha=0.01): 0.9999\n",
    "\n",
    "**Mathematical Meaning:** This is the standard Shannon entropy - the \"baseline\" measure.\n",
    "\n",
    "**Interpretation for Gender:**\n",
    "- **Very close to maximum entropy** (1.0 for binary variable)\n",
    "- **Near-perfect balance** between Male/Female categories\n",
    "- **High uncertainty** - difficult to predict gender of random person\n",
    "- **Practical meaning:** Your dataset has approximately 50-50 gender split\n",
    "\n",
    "---\n",
    "\n",
    "## **2. α = 0 (Hartley Entropy): 1.0000**\n",
    "\n",
    "**Mathematical Meaning:** `H₀ = log₂(number of categories)`\n",
    "\n",
    "**Interpretation for Gender:**\n",
    "- **Always equals 1.0** for binary categorical data (log₂(2) = 1)\n",
    "- **Only counts distinct categories**, ignores their frequencies\n",
    "- **Practical meaning:** Confirms you have exactly 2 gender categories\n",
    "- **Use case:** Useful for checking data completeness, not distribution balance\n",
    "\n",
    "---\n",
    "\n",
    "## **3. α = 2 (Collision Entropy): 0.9794**\n",
    "\n",
    "**Mathematical Meaning:** Emphasizes the most frequent category more than Shannon entropy.\n",
    "\n",
    "**Interpretation for Gender:**\n",
    "- **Slightly lower than Shannon entropy** (0.9794 vs 0.9896)\n",
    "- **Indicates minor imbalance** - one gender is slightly more frequent\n",
    "- **More sensitive to majority class** than Shannon entropy\n",
    "- **Practical meaning:** There's a small but detectable gender imbalance in your dataset\n",
    "\n",
    "---\n",
    "\n",
    "## **4. α = 10 (High-Order Entropy): 0.9157**\n",
    "\n",
    "**Mathematical Meaning:** Heavily emphasizes the dominant category.\n",
    "\n",
    "**Interpretation for Gender:**\n",
    "- **Noticeably lower** than Shannon entropy (0.9157 vs 0.9896)\n",
    "- **Amplifies the effect** of the majority gender category\n",
    "- **Conservative estimate** of randomness\n",
    "- **Practical meaning:** When you care most about the dominant pattern, there's measurable predictability\n",
    "\n",
    "---\n",
    "\n",
    "## **5. α = 0.5 (Sub-Shannon): 0.9948**\n",
    "\n",
    "**Mathematical Meaning:** Emphasizes rare categories more than Shannon entropy.\n",
    "\n",
    "**Interpretation for Gender:**\n",
    "- **Higher than Shannon entropy** (0.9948 vs 0.9896)\n",
    "- **Gives more weight to minority gender**\n",
    "- **More optimistic about diversity**\n",
    "- **Practical meaning:** From minority representation perspective, distribution looks very balanced\n",
    "\n",
    "---\n",
    "\n",
    "## **6. α = 0.1 (Very Low Alpha): 0.9990**\n",
    "\n",
    "**Mathematical Meaning:** Strongly emphasizes the rarest categories.\n",
    "\n",
    "**Interpretation for Gender:**\n",
    "- **Very close to maximum** (0.9990 vs 1.0000)\n",
    "- **Heavily weights the minority gender**\n",
    "- **Almost ignores majority category**\n",
    "- **Practical meaning:** Minority representation is well-maintained\n",
    "\n",
    "---\n",
    "\n",
    "## **7. α = 0.01 (Extremely Low Alpha): 0.9999**\n",
    "\n",
    "**Mathematical Meaning:** Almost exclusively focuses on rare events.\n",
    "\n",
    "**Interpretation for Gender:**\n",
    "- **Nearly perfect entropy** (0.9999)\n",
    "- **Essentially measures minority presence only**\n",
    "- **Approaches Hartley entropy**\n",
    "- **Practical meaning:** Even the minority gender is well-represented\n",
    "\n",
    "---\n",
    "\n",
    "## **Overall Pattern Analysis**\n",
    "\n",
    "### **The Trend You're Seeing:**\n",
    "\n",
    "```plaintext\n",
    "α → 0: Entropy increases (0.9999 → 1.0000)\n",
    "α = 1: Shannon baseline (0.9896)\n",
    "α → ∞: Entropy decreases (0.9157)\n",
    "```\n",
    "\n",
    "\n",
    "### **What This Tells You About Your Gender Data:**\n",
    "\n",
    "1. **Near-Perfect Balance:** All values are very high (>0.91), indicating excellent gender balance\n",
    "\n",
    "2. **Slight Imbalance Detected:** The decrease from α=0.1 to α=10 reveals a small but consistent imbalance\n",
    "\n",
    "3. **Robust Distribution:** Even the most conservative measure (α=10) shows high entropy (0.9157)\n",
    "\n",
    "4. **Practical Implications:**\n",
    "   - **For ML models:** Gender won't be a strong predictor due to balance\n",
    "   - **For business analysis:** No significant gender bias in customer base\n",
    "   - **For sampling:** Dataset represents both genders fairly\n",
    "\n",
    "---\n",
    "\n",
    "## **Business Interpretation for Customer Segmentation:**\n",
    "\n",
    "**Excellent Gender Representation:**\n",
    "- Your customer base is nearly evenly split between genders\n",
    "- No significant sampling bias toward either gender\n",
    "- Gender-based marketing strategies would reach similar-sized audiences\n",
    "- Any patterns you find likely aren't due to gender imbalance artifacts\n",
    "\n",
    "**Recommendation:**\n",
    "- This balanced gender distribution is ideal for customer segmentation analysis\n",
    "- You can confidently analyze spending patterns, age groups, and income levels without worrying about gender bias\n",
    "- Consider gender as a potential segmentation variable, but expect it to interact with other features rather than dominate\n",
    "\n",
    "The Rényi entropy analysis confirms that your dataset has excellent gender balance with only minimal skew, making it suitable for robust customer segmentation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d929fe",
   "metadata": {},
   "source": [
    "## **Gini Impurity**\n",
    "Gini impurity is a measure of how often a randomly chosen element from the dataset would be incorrectly classified if it were randomly assigned to a class. It is defined as:\n",
    "\n",
    "The Gini impurity, sometimes called Gini entropy, is a measure of the \"purity\" of a set of data. It's most commonly used in machine learning, specifically in decision tree algorithms, to determine the best way to split a dataset.\n",
    "\n",
    "The formula for Gini impurity is:\n",
    "\n",
    "```plaintext\n",
    "Gini = 1 - Σ (pᵢ)²\n",
    "```\n",
    "\n",
    "Here's an explanation of the formula:\n",
    "\n",
    "- **pᵢ**: This represents the **proportion** (or probability) of items belonging to a specific class i within a given set of data. You calculate this for each class.\n",
    "\n",
    "- **Σ (Sigma)**: This is the summation symbol. It means you first square the proportion (pᵢ) for each class, and then you add all those squared values together.\n",
    "\n",
    "- **1 - ...**: The final Gini impurity score is calculated by subtracting the sum of the squared proportions from 1.\n",
    "\n",
    "### What the Gini Impurity Score Means\n",
    "\n",
    "The Gini impurity score tells you the probability of incorrectly classifying a randomly chosen element from the dataset if you were to label it according to the distribution of labels in that set.\n",
    "\n",
    "The score ranges from 0 to 0.5 (for a binary classification problem):\n",
    "\n",
    "- **Gini = 0**: This means the set is perfectly pure. All the elements in the set belong to a single class. There is no uncertainty.\n",
    "- **Gini = 0.5**: This means the set is maximally impure. The elements are split evenly among the classes (e.g., 50% in class A, 50% in class B). This is the highest level of uncertainty.\n",
    "\n",
    "### Simple Example\n",
    "\n",
    "Let's say you have a bucket with 10 balls: 7 are red and 3 are blue.\n",
    "\n",
    "1. **Calculate the proportions (pᵢ)**:\n",
    "    - Proportion of red balls (p_red) = 7/10 = 0.7\n",
    "    - Proportion of blue balls (p_blue) = 3/10 = 0.3\n",
    "2. **Square the proportions and sum them**:\n",
    "    - (0.7)² + (0.3)² = 0.49 + 0.09 = 0.58\n",
    "3. **Calculate the Gini impurity**:\n",
    "    - Gini = 1 - 0.58 = 0.42\n",
    "\n",
    "This Gini score of 0.42 is relatively high, indicating a notable amount of \"impurity\" or mixed classes in the bucket. In a decision tree, the algorithm would look for a way to split this data to create new, purer subsets with lower Gini scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65906441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4927999999999999\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(base_df['Gender'])\n",
    "probabilities = np.array(list(counts.values())) / len(base_df['Gender'])\n",
    "gini_impurity_gender = 1 - np.sum(probabilities ** 2)\n",
    "print(gini_impurity_gender)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488ef76",
   "metadata": {},
   "source": [
    "# **Gini Impurity Interpretation for `base_df['Gender']`**\n",
    "\n",
    "Based on your Gini impurity output of **0.4928** for the Gender column, here's how to interpret it:\n",
    "\n",
    "## **Your Result: 0.4928**\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Context**\n",
    "\n",
    "**Gini Impurity Formula:** `Gini = 1 - Σ(pᵢ)²`\n",
    "\n",
    "**Range for Binary Variables:**\n",
    "- **Minimum: 0.0** (perfectly pure - all one category)\n",
    "- **Maximum: 0.5** (maximally impure - perfectly balanced)\n",
    "\n",
    "---\n",
    "\n",
    "## **What 0.4928 Means**\n",
    "\n",
    "### **1. Proximity to Maximum Impurity**\n",
    "- Your value (0.4928) is **very close to the maximum** (0.5)\n",
    "- This indicates **near-perfect balance** between Male and Female categories\n",
    "- Only **1.44% away** from perfect balance: `(0.5 - 0.4928) / 0.5 = 0.0144 = 1.44%`\n",
    "\n",
    "### **2. Classification Uncertainty**\n",
    "- **49.28% probability** of misclassifying a randomly chosen person if you guess their gender randomly according to the dataset distribution\n",
    "- This is **very high uncertainty** - almost like flipping a coin\n",
    "\n",
    "### **3. Decision Tree Perspective**\n",
    "- If building a decision tree, Gender alone would provide **minimal information gain**\n",
    "- The algorithm would struggle to create meaningful splits based solely on gender\n",
    "- **Poor discriminative power** for prediction tasks\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Interpretation**\n",
    "\n",
    "### **Dataset Balance Quality**\n",
    "```plaintext\n",
    "Perfect Balance (0.5): ████████████████████████████████████████████████████ 100%\n",
    "Your Dataset (0.4928): ████████████████████████████████████████████████████ 98.56%\n",
    "Moderate Imbalance (0.4): ████████████████████████████████████████ 80%\n",
    "High Imbalance (0.2): ████████████████████ 40%\n",
    "Pure Dataset (0.0): 0%\n",
    "```\n",
    "\n",
    "### **Business Implications**\n",
    "\n",
    "**✅ Excellent for Analysis:**\n",
    "- **No gender bias** in customer acquisition\n",
    "- **Representative sample** of both genders\n",
    "- **Fair basis** for comparative analysis between genders\n",
    "\n",
    "**⚠️ Limited Predictive Power:**\n",
    "- Gender alone won't be a strong predictor of customer behavior\n",
    "- Need to combine with other features (age, income, spending) for meaningful insights\n",
    "- Gender-based segmentation should be secondary to other factors\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison with Other Measures**\n",
    "\n",
    "| Measure | Value | Interpretation |\n",
    "|---------|-------|---------------|\n",
    "| **Gini Impurity** | 0.4928 | Near-perfect balance (98.56% of maximum) |\n",
    "| **Shannon Entropy** | 0.9896 | Near-perfect balance (98.96% of maximum) |\n",
    "| **Both Agree** | ✅ | Consistently show excellent gender balance |\n",
    "\n",
    "---\n",
    "\n",
    "## **What Different Gini Values Would Mean**\n",
    "\n",
    "```python\n",
    "# Hypothetical scenarios for comparison:\n",
    "\n",
    "# Perfect balance: 50% Male, 50% Female\n",
    "gini_perfect = 0.5000    # \"Maximum impurity - can't predict gender\"\n",
    "\n",
    "# Your actual data: ~51% one gender, ~49% other\n",
    "gini_yours = 0.4928      # \"Near-perfect balance - minimal predictability\"\n",
    "\n",
    "# Moderate imbalance: 60% Male, 40% Female  \n",
    "gini_moderate = 0.4800   # \"Noticeable but acceptable imbalance\"\n",
    "\n",
    "# High imbalance: 70% Male, 30% Female\n",
    "gini_high = 0.4200       # \"Significant imbalance - some predictability\"\n",
    "\n",
    "# Extreme imbalance: 90% Male, 10% Female\n",
    "gini_extreme = 0.1800    # \"Severe bias - high predictability\"\n",
    "\n",
    "# Pure dataset: 100% Male, 0% Female\n",
    "gini_pure = 0.0000       # \"Perfect purity - complete predictability\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Actionable Insights**\n",
    "\n",
    "### **For Customer Segmentation:**\n",
    "1. **Don't rely on gender alone** for primary segmentation\n",
    "2. **Use gender as a secondary dimension** combined with spending patterns, age groups, or income levels\n",
    "3. **Both genders are well-represented** - any insights will be statistically robust\n",
    "\n",
    "### **For Marketing Strategy:**\n",
    "1. **Gender-neutral approaches** might be most effective given the balance\n",
    "2. **A/B testing** between genders will have adequate sample sizes\n",
    "3. **Personalization** should focus on behavioral patterns rather than gender stereotypes\n",
    "\n",
    "### **For Data Quality:**\n",
    "1. **Excellent representativeness** - no sampling bias concerns\n",
    "2. **Suitable for comparative analysis** between gender groups\n",
    "3. **Robust foundation** for building more complex customer profiles\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaway**\n",
    "\n",
    "**Your Gini impurity of 0.4928 indicates an exceptionally well-balanced gender distribution in your customer dataset.** This is ideal for unbiased analysis but means gender alone won't be a strong predictor of customer behavior. Focus on combining gender with other features to uncover meaningful customer segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b974dd5",
   "metadata": {},
   "source": [
    "## **Simpson's Diversity Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16f354",
   "metadata": {},
   "source": [
    "Simpson's Diversity Index is a measure used to quantify the diversity of a habitat or community. It takes into account both the number of different species (richness) and their relative abundance (evenness).\n",
    "\n",
    "There are a few different ways to express the index, which can sometimes be confusing. I'll explain the main formulas and how to interpret them.\n",
    "\n",
    "1. Simpson's Index (D)\n",
    "This is the core formula. It calculates the probability that two individuals, selected at random from a sample, will belong to the same species.\n",
    "\n",
    "The formula is:\n",
    "```plaintext\n",
    "D = Σ [ n(n-1) / N(N-1) ]\n",
    "```\n",
    "\n",
    "- **n**: The total number of individuals of a particular species.\n",
    "- **N**: The total number of individuals of all species.\n",
    "- **Σ (Sigma)**: The summation symbol. It means you perform the calculation n(n-1) for each species, sum those values, and then divide by N(N-1).\n",
    "\n",
    "**How to Interpret D**: The value of D ranges from 0 to 1.\n",
    "\n",
    "- **D = 1**: Means there is no diversity (only one species is present).\n",
    "- **D = 0**: Represents infinite diversity.\n",
    "This interpretation is often considered counter-intuitive because a higher value of D means lower diversity. Because of this, the following two variations are more commonly used.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Simpson's Index of Diversity (1 - D)**\n",
    "This is the most popular version of the index because its interpretation is more straightforward. It calculates the probability that two individuals, selected at random, will belong to different species.\n",
    "\n",
    "The formula is simply: **1 - D**\n",
    "\n",
    "**How to Interpret (1 - D):** The value ranges from 0 to 1.\n",
    "\n",
    "- A value closer to **1** indicates **high** diversity.\n",
    "- A value closer to **0** indicates **low** diversity.\n",
    "---\n",
    "\n",
    "### **3. Simpson's Reciprocal Index (1 / D)**\n",
    "This is another intuitive version where the value increases with diversity.\n",
    "\n",
    "The formula is: **1 / D**\n",
    "\n",
    "**How to Interpret (1 / D)**: The value starts at 1 and can increase.\n",
    "\n",
    "- The lowest possible value is **1**, which represents a community with only one species.\n",
    "- The higher the value, the **greater the diversity**. The maximum possible value is the total number of species in the community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3181d85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5047236180904523\n",
      "0.4952763819095477\n"
     ]
    }
   ],
   "source": [
    "n = len(base_df['Gender'])\n",
    "D = sum((count * (count -1)) for count in counts.values()) / (n * (n-1))\n",
    "print(D)\n",
    "\n",
    "simpsons_diversity_index = 1 - D\n",
    "print(simpsons_diversity_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787785f",
   "metadata": {},
   "source": [
    "## Interpret Simpson's Reciprocal Index\n",
    "\n",
    "# **Simpson's Reciprocal Index Interpretation for `base_df['Gender']`**\n",
    "\n",
    "Based on your output values, here's how to interpret Simpson's indices:\n",
    "\n",
    "## **Your Results:**\n",
    "\n",
    "```plaintext\n",
    "D (Simpson's Index): 0.5047\n",
    "1 - D (Diversity Index): 0.4953\n",
    "1 / D (Reciprocal Index): 1.9813\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Understanding the Three Simpson's Measures**\n",
    "\n",
    "### **1. Simpson's Index (D): 0.5047**\n",
    "**Formula:** `D = Σ[n(n-1)] / [N(N-1)]`\n",
    "\n",
    "**Interpretation:**\n",
    "- **Probability** that two randomly selected individuals belong to the **same category**\n",
    "- **Range:** 0 to 1\n",
    "- **Higher D = Lower diversity** (counter-intuitive)\n",
    "- **Your value:** 50.47% chance two random people have the same gender\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Simpson's Diversity Index (1-D): 0.4953**\n",
    "**Formula:** `1 - D`\n",
    "\n",
    "**Interpretation:**\n",
    "- **Probability** that two randomly selected individuals belong to **different categories**\n",
    "- **Range:** 0 to 1\n",
    "- **Higher value = Higher diversity** (intuitive)\n",
    "- **Your value:** 49.53% chance two random people have different genders\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Simpson's Reciprocal Index (1/D): 1.9813**\n",
    "**Formula:** `1 / D`\n",
    "\n",
    "**Interpretation:**\n",
    "- **Effective number of categories** in your dataset\n",
    "- **Range:** 1 to number of categories\n",
    "- **Higher value = Higher diversity**\n",
    "- **Your value:** ~1.98 ≈ **2 effective categories**\n",
    "\n",
    "---\n",
    "\n",
    "## **Detailed Interpretation of Reciprocal Index (1.9813)**\n",
    "\n",
    "### **What 1.9813 Means:**\n",
    "\n",
    "**1. Effective Diversity:**\n",
    "- Your dataset behaves as if it has **1.98 equally abundant categories**\n",
    "- Very close to the theoretical maximum of **2.0** for binary data\n",
    "- Indicates **near-perfect balance** between Male and Female\n",
    "\n",
    "**2. Comparison Scale:**\n",
    "\n",
    "```plaintext\n",
    "Perfect Balance: 2.0000 ████████████████████████████████████████████████████ 100%\n",
    "Your Dataset: 1.9813 ████████████████████████████████████████████████████ 99.1%\n",
    "Moderate Imbalance: 1.6000 ████████████████████████████████████████ 80%\n",
    "High Imbalance: 1.2500 █████████████████████████ 62.5%\n",
    "Complete Dominance: 1.0000 █████████████████████ 50%\n",
    "```\n",
    "\n",
    "\n",
    "**3. Practical Meaning:**\n",
    "- **99.1% of maximum possible diversity** for binary categorical data\n",
    "- Only **0.9% away** from perfect 50-50 split\n",
    "- **Excellent representativeness** of both genders\n",
    "\n",
    "---\n",
    "\n",
    "## **Business Interpretation**\n",
    "\n",
    "### **Customer Base Analysis:**\n",
    "**✅ Strengths:**\n",
    "- **Balanced customer acquisition** across genders\n",
    "- **No sampling bias** in your dataset\n",
    "- **Robust foundation** for gender-based comparisons\n",
    "- **Equal market representation** for both segments\n",
    "\n",
    "**📊 Statistical Implications:**\n",
    "- **High statistical power** for gender comparisons\n",
    "- **Reliable A/B testing** potential between genders\n",
    "- **Minimal confounding** from gender imbalance\n",
    "- **Strong generalizability** of findings\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison with Other Diversity Measures**\n",
    "\n",
    "| Measure | Value | Interpretation | Scale |\n",
    "|---------|-------|---------------|-------|\n",
    "| **Shannon Entropy** | 0.9896 | 98.96% of max | 0-1 |\n",
    "| **Gini Impurity** | 0.4928 | 98.56% of max | 0-0.5 |\n",
    "| **Simpson's 1-D** | 0.4953 | 99.06% of max | 0-0.5 |\n",
    "| **Simpson's 1/D** | 1.9813 | 99.07% of max | 1-2 |\n",
    "\n",
    "**All measures consistently show ~99% of maximum diversity!**\n",
    "\n",
    "---\n",
    "\n",
    "## **What Different Reciprocal Values Would Mean**\n",
    "\n",
    "```python\n",
    "# Hypothetical scenarios for binary gender data:\n",
    "\n",
    "# Perfect balance: 50% Male, 50% Female\n",
    "reciprocal_perfect = 2.0000    # \"Maximum diversity - 2 effective categories\"\n",
    "\n",
    "# Your actual data: ~50.5% one gender, ~49.5% other  \n",
    "reciprocal_yours = 1.9813      # \"Near-perfect balance - 1.98 effective categories\"\n",
    "\n",
    "# Moderate imbalance: 60% Male, 40% Female\n",
    "reciprocal_moderate = 1.6667   # \"Noticeable imbalance - 1.67 effective categories\"\n",
    "\n",
    "# High imbalance: 70% Male, 30% Female  \n",
    "reciprocal_high = 1.4286       # \"Significant imbalance - 1.43 effective categories\"\n",
    "\n",
    "# Extreme imbalance: 90% Male, 10% Female\n",
    "reciprocal_extreme = 1.1111    # \"Severe bias - 1.11 effective categories\"\n",
    "\n",
    "# Complete dominance: 100% Male, 0% Female\n",
    "reciprocal_pure = 1.0000       # \"No diversity - 1 effective category\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Actionable Insights**\n",
    "\n",
    "### **For Customer Segmentation:**\n",
    "1. **Gender is well-balanced** - excellent for comparative analysis\n",
    "2. **Use as secondary segmentation** variable, not primary\n",
    "3. **Combine with behavioral data** for meaningful segments\n",
    "4. **Both genders equally represented** in any insights\n",
    "\n",
    "### **For Marketing Strategy:**\n",
    "1. **Equal budget allocation** between gender-targeted campaigns justified\n",
    "2. **A/B testing** will have adequate sample sizes for both groups  \n",
    "3. **Gender-neutral messaging** might be most effective given balance\n",
    "4. **Personalization** should focus on behavior over demographics\n",
    "\n",
    "### **For Statistical Analysis:**\n",
    "1. **No need for gender-based stratification** in sampling\n",
    "2. **Robust comparative statistics** possible between genders\n",
    "3. **Minimal risk of gender bias** in model predictions\n",
    "4. **Strong external validity** for gender-related findings\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaway**\n",
    "\n",
    "**Your Simpson's Reciprocal Index of 1.9813 indicates exceptional gender balance in your customer dataset.** This represents 99.1% of the maximum possible diversity for binary categorical data, meaning your dataset effectively behaves as if it has nearly 2 equally abundant gender categories. This is ideal for unbiased customer segmentation analysis and ensures that any patterns you discover are not artifacts of gender imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ef76445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe saved successfully to: /Users/ravisharma/workdir/eda_practice/data/customer-segmentation/Mall_Customers.pkl\n",
      "File size: 7643 bytes\n",
      "Shape of saved dataframe: (200, 5)\n"
     ]
    }
   ],
   "source": [
    "# Save the dataframe as a pickle file in the data folder\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the pickle file path in the data directory\n",
    "pickle_file_path = DATA_DIR / \"Mall_Customers.pkl\"\n",
    "\n",
    "# Save the dataframe to pickle file\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(base_df, f)\n",
    "\n",
    "print(f\"Dataframe saved successfully to: {pickle_file_path}\")\n",
    "print(f\"File size: {pickle_file_path.stat().st_size} bytes\")\n",
    "print(f\"Shape of saved dataframe: {base_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0254850",
   "metadata": {},
   "source": [
    "## **Kolmogorov Complexity (Algorithmic Information Theory)**\n",
    "\n",
    "Kolmogorov Complexity is a measure of the complexity of a string of symbols, such as a sequence of characters from an alphabet. It is a fundamental concept in algorithmic information theory, which studies the properties of information and its representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87472dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 1029 bytes\n",
      "Compressed size: 511 bytes\n",
      "Kolmogorov complexity: 0.4965986394557823\n"
     ]
    }
   ],
   "source": [
    "import zlib\n",
    "import pickle\n",
    "\n",
    "serialized = pickle.dumps(base_df['Gender'])\n",
    "serialized_compressed = zlib.compress(serialized)\n",
    "\n",
    "print(f\"Original size: {len(serialized)} bytes\")\n",
    "print(f\"Compressed size: {len(serialized_compressed)} bytes\")\n",
    "\n",
    "kolmogorov_complexity = len(serialized_compressed) / len(serialized)\n",
    "print(f\"Kolmogorov complexity: {kolmogorov_complexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2258052",
   "metadata": {},
   "source": [
    "# **Kolmogorov Complexity Interpretation for `base_df['Gender']`**\n",
    "\n",
    "Based on your output values, here's how to interpret the compression-based Kolmogorov complexity approximation:\n",
    "\n",
    "## **Your Results:**\n",
    "\n",
    "```plaintext\n",
    "Original size: 1029 bytes\n",
    "Compressed size: 511 bytes\n",
    "Kolmogorov complexity: 0.4966\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Understanding Kolmogorov Complexity**\n",
    "\n",
    "### **Theoretical Foundation**\n",
    "- **True Kolmogorov Complexity**: Length of shortest program that can generate the data\n",
    "- **Uncomputable**: Cannot be calculated exactly for arbitrary data\n",
    "- **Compression Approximation**: Uses compression ratio as practical estimate\n",
    "\n",
    "### **Your Approximation Method**\n",
    "```plaintext\n",
    "K(x) ≈ compressed_size / original_size = 511 / 1029 = 0.4966\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Interpretation of 0.4966**\n",
    "\n",
    "### **1. Compression Efficiency**\n",
    "- **50.34% compression achieved** (1 - 0.4966 = 0.5034)\n",
    "- **Nearly 50% reduction** in data size\n",
    "- **Moderate compressibility** - data has detectable patterns\n",
    "\n",
    "### **2. Randomness Assessment**\n",
    "**Scale Interpretation:**\n",
    "\n",
    "```plaintext\n",
    "Pure Random Data: 1.0000 ████████████████████████████████████████████████████ 100%\n",
    "Your Gender Data: 0.4966 █████████████████████████ 49.7%\n",
    "Highly Structured: 0.2000 ██████████ 20%\n",
    "Perfect Pattern: 0.0500 ███ 5%\n",
    "```\n",
    "\n",
    "\n",
    "### **3. Pattern Detection**\n",
    "- **Significant structure detected** (compression ratio < 0.5)\n",
    "- **Not random data** - contains predictable patterns\n",
    "- **Moderate algorithmic information content**\n",
    "\n",
    "---\n",
    "\n",
    "## **What This Means for Gender Data**\n",
    "\n",
    "### **Data Characteristics:**\n",
    "**✅ Expected Results:**\n",
    "- **Binary categorical data** naturally compresses well\n",
    "- **Repeated values** (Male/Female) create compression opportunities\n",
    "- **Structured format** (pandas Series) adds overhead but enables compression\n",
    "\n",
    "**📊 Specific Insights:**\n",
    "- **Gender distribution has detectable patterns**\n",
    "- **Not uniformly random** - some predictability exists\n",
    "- **Balanced but not perfectly random** distribution\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison with Different Data Types**\n",
    "\n",
    "| Data Type | Expected K(x) | Your Result | Interpretation |\n",
    "|-----------|---------------|-------------|---------------|\n",
    "| **Random Binary** | ~0.95-1.0 | 0.4966 | Much more structured |\n",
    "| **Perfect Balance** | ~0.3-0.5 | 0.4966 | ✅ Matches expectation |\n",
    "| **Highly Skewed** | ~0.2-0.4 | 0.4966 | More balanced than this |\n",
    "| **Single Value** | ~0.05-0.1 | 0.4966 | Much more diverse |\n",
    "\n",
    "---\n",
    "\n",
    "## **Factors Affecting Your Result**\n",
    "\n",
    "### **1. Data Structure Overhead**\n",
    "```python\n",
    "# Components contributing to size:\n",
    "# - Pandas Series metadata\n",
    "# - Pickle serialization overhead  \n",
    "# - Data type information\n",
    "# - Index information\n",
    "```\n",
    "\n",
    "### **2. Compression Algorithm (zlib)**\n",
    "- **General-purpose compressor** - not optimized for categorical data\n",
    "- **Good for repeated patterns** - benefits from Male/Female repetition\n",
    "- **Dictionary-based compression** - builds patterns from repeated strings\n",
    "\n",
    "### **3. Sample Size Effect**\n",
    "- **200 observations** - sufficient for pattern detection\n",
    "- **Larger samples** might show different compression ratios\n",
    "- **Overhead becomes less significant** with more data\n",
    "\n",
    "---\n",
    "\n",
    "## **Business Interpretation**\n",
    "\n",
    "### **Data Quality Insights:**\n",
    "**✅ Positive Indicators:**\n",
    "- **Structured, non-random data** - good data quality\n",
    "- **Predictable patterns** - consistent data entry\n",
    "- **Moderate complexity** - balanced distribution\n",
    "\n",
    "**⚠️ Considerations:**\n",
    "- **Not perfectly random** - slight bias detectable\n",
    "- **Compression possible** - redundancy in representation\n",
    "- **Pattern exists** - gender distribution not uniform\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison with Other Complexity Measures**\n",
    "\n",
    "### **Hypothetical Scenarios:**\n",
    "```python\n",
    "# Different gender distributions and expected K(x):\n",
    "\n",
    "# Perfect 50-50 split, random order\n",
    "perfect_random = 0.95        # \"High complexity - truly random\"\n",
    "\n",
    "# Your actual data: balanced but ordered\n",
    "your_data = 0.4966          # \"Moderate complexity - structured balance\"\n",
    "\n",
    "# Alternating pattern: M,F,M,F,M,F...\n",
    "alternating = 0.15          # \"Low complexity - perfect pattern\"\n",
    "\n",
    "# All same gender: M,M,M,M,M,M...\n",
    "homogeneous = 0.05          # \"Minimal complexity - no information\"\n",
    "\n",
    "# Clustered: MMM...FFF (groups together)\n",
    "clustered = 0.25            # \"Low-moderate complexity - some pattern\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Applications**\n",
    "\n",
    "### **1. Data Validation**\n",
    "- **Detects artificial patterns** in supposedly random data\n",
    "- **Identifies data entry artifacts** (clustering, ordering)\n",
    "- **Validates randomness assumptions** for statistical tests\n",
    "\n",
    "### **2. Compression Strategy**\n",
    "- **50% compression achievable** for similar categorical data\n",
    "- **Storage optimization** possible for large datasets\n",
    "- **Transmission efficiency** for data sharing\n",
    "\n",
    "### **3. Pattern Analysis**\n",
    "- **Baseline complexity** for comparing other variables\n",
    "- **Anomaly detection** - unusual compression ratios indicate issues\n",
    "- **Data generation validation** - synthetic data should have similar complexity\n",
    "\n",
    "---\n",
    "\n",
    "## **Limitations of This Approximation**\n",
    "\n",
    "### **Method Limitations:**\n",
    "1. **Compression algorithm dependent** - different compressors give different results\n",
    "2. **Overhead effects** - small datasets affected by serialization overhead\n",
    "3. **Not true Kolmogorov complexity** - only an approximation\n",
    "4. **Context dependent** - results vary with data representation\n",
    "\n",
    "### **Interpretation Caveats:**\n",
    "1. **Relative measure** - best used for comparison, not absolute assessment\n",
    "2. **Sample size sensitive** - larger samples may show different patterns\n",
    "3. **Format dependent** - pickle vs JSON vs CSV would give different results\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "\n",
    "**Your Kolmogorov complexity of 0.4966 indicates:**\n",
    "\n",
    "1. **Structured but balanced data** - not random, but well-distributed\n",
    "2. **Moderate algorithmic information** - contains detectable patterns\n",
    "3. **Good compression potential** - ~50% size reduction possible\n",
    "4. **Expected result** for balanced binary categorical data\n",
    "5. **Quality indicator** - suggests consistent, non-random data collection\n",
    "\n",
    "**Bottom Line:** Your gender data shows the expected level of structure for a well-balanced binary categorical variable, with enough pattern to enable compression but sufficient balance to avoid extreme bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd5bcb",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "\n",
    "The Mutual Information (MI) score quantifies the dependency between two variables. It measures how much information one random variable provides about another.\n",
    "\n",
    "### Formula\n",
    "The most common formula for mutual information for discrete variables is:\n",
    "\n",
    "```plaintext\n",
    "I(X;Y) = Σ Σ p(x,y) log( p(x,y) / (p(x)p(y)) )\n",
    "```\n",
    "\n",
    "### Parameter Explanation\n",
    "- **I(X;Y)**: The Mutual Information between variables X and Y.\n",
    "- **p(x,y)**: The joint probability of observing variables X and Y together with specific values x and y.\n",
    "- **p(x)**: The marginal probability of observing the value x for variable X.\n",
    "- **p(y)**: The marginal probability of observing the value y for variable Y.\n",
    "- **Σ Σ**: This indicates a summation over all possible values of X and Y.\n",
    "\n",
    "In simpler terms, the formula compares the probability of observing X and Y together (the joint probability) with the probability of observing them independently (the product of their individual probabilities).\n",
    "\n",
    "### Interpretation\n",
    "- **Range**: The MI score is always non-negative (>= 0).\n",
    "- **Independence**: If the MI score is 0, it means the two variables are completely independent. Knowing the value of one variable gives you no information about the value of the other.\n",
    "- **Dependence**: A higher MI score indicates a stronger relationship or dependency between the two variables. The higher the value, the more information one variable provides about the other.\n",
    "- **Symmetry**: The MI is symmetric, meaning I(X;Y) = I(Y;X). The information that X provides about Y is the same as the information that Y provides about X.\n",
    "- **Non-linear Relationships**: Unlike correlation coefficients (like Pearson's), which only measure linear relationships, mutual information can capture non-linear dependencies between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8bd704d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19442479684402172\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "print(mutual_info_score(base_df['Gender'], base_df['Age']))\n",
    "\n",
    "# Mutual Information Interpretation\n",
    "\n",
    "## **The mutual information between Gender and Age is 0.0000. This indicates that there is no relationship between Gender and Age.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc11974",
   "metadata": {},
   "source": [
    "# **Mutual Information Interpretation: Gender vs Age**\n",
    "\n",
    "Based on your mutual information score of **0.0000** between Gender and Age, here's how to interpret this result:\n",
    "\n",
    "## **Your Result: 0.0000**\n",
    "\n",
    "---\n",
    "\n",
    "## **Understanding Mutual Information**\n",
    "\n",
    "### **Mathematical Foundation**\n",
    "**Formula:** `I(X;Y) = ∑∑ p(x,y) * log(p(x,y) / (p(x) * p(y)))`\n",
    "\n",
    "**Conceptual Meaning:**\n",
    "- **Measures shared information** between two variables\n",
    "- **Quantifies dependence** - how much knowing one variable tells you about the other\n",
    "- **Information-theoretic measure** of association\n",
    "\n",
    "---\n",
    "\n",
    "## **Interpretation of 0.0000**\n",
    "\n",
    "### **1. Statistical Independence**\n",
    "- **Perfect independence** between Gender and Age\n",
    "- **Knowing someone's gender provides zero information** about their age\n",
    "- **Age distribution is identical** across both genders\n",
    "\n",
    "### **2. Practical Meaning**\n",
    "\n",
    "```plaintext\n",
    "No Relationship: 0.0000 ████████████████████████████████████████████████████\n",
    "Weak Relationship: 0.1000 ██████████\n",
    "Moderate: 0.3000 ██████████████████████████████████\n",
    "Strong: 0.5000 ████████████████████████████████████████████████████\n",
    "Perfect Dependence: 1.0000+ ████████████████████████████████████████████████████\n",
    "```\n",
    "\n",
    "\n",
    "### **3. Business Implications**\n",
    "- **No age bias** in gender representation\n",
    "- **Gender-neutral age distribution** in your customer base\n",
    "- **Age-based strategies** don't need gender considerations\n",
    "\n",
    "---\n",
    "\n",
    "## **What This Means for Your Dataset**\n",
    "\n",
    "### **Customer Demographics:**\n",
    "**✅ Positive Indicators:**\n",
    "- **Unbiased sampling** across age groups for both genders\n",
    "- **Natural demographic distribution** - no artificial patterns\n",
    "- **Representative customer base** without age-gender skew\n",
    "\n",
    "**📊 Statistical Implications:**\n",
    "- **Independent variables** for modeling purposes\n",
    "- **No confounding effects** between age and gender\n",
    "- **Separate analysis possible** for each variable\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison with Other MI Values**\n",
    "\n",
    "### **Hypothetical Scenarios:**\n",
    "```python\n",
    "# Different relationships and expected MI scores:\n",
    "\n",
    "# Your result: Complete independence\n",
    "gender_age_yours = 0.0000      # \"No relationship detected\"\n",
    "\n",
    "# Weak association (slight age differences by gender)\n",
    "weak_association = 0.05        # \"Minimal but detectable relationship\"\n",
    "\n",
    "# Moderate association (clear age patterns by gender)  \n",
    "moderate_association = 0.15    # \"Noticeable age-gender relationship\"\n",
    "\n",
    "# Strong association (distinct age groups by gender)\n",
    "strong_association = 0.30      # \"Strong age-gender dependency\"\n",
    "\n",
    "# Perfect dependence (age completely determines gender)\n",
    "perfect_dependence = 1.0+      # \"Complete predictability\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Factors Affecting Your Result**\n",
    "\n",
    "### **1. Data Characteristics**\n",
    "- **Continuous vs Discrete**: Age is continuous, Gender is categorical\n",
    "- **Binning Effect**: `mutual_info_score` automatically bins continuous variables\n",
    "- **Sample Size**: 200 observations may limit detection of weak relationships\n",
    "\n",
    "### **2. Method Limitations**\n",
    "```python\n",
    "# sklearn's mutual_info_score for mixed data types:\n",
    "# - Automatically discretizes continuous variables (Age)\n",
    "# - Uses histogram-based estimation\n",
    "# - May miss subtle nonlinear relationships\n",
    "```\n",
    "\n",
    "### **3. True vs Apparent Independence**\n",
    "- **Genuine independence**: Age and gender truly unrelated in population\n",
    "- **Insufficient sensitivity**: Very weak relationships might not be detected\n",
    "- **Appropriate binning**: Default binning might be masking subtle patterns\n",
    "\n",
    "---\n",
    "\n",
    "## **Validation and Further Analysis**\n",
    "\n",
    "### **1. Cross-Validation Methods**\n",
    "```python\n",
    "# Alternative approaches to verify the result:\n",
    "\n",
    "# 1. Correlation analysis (for continuous age)\n",
    "from scipy.stats import pointbiserial\n",
    "correlation = pointbiserial(gender_binary, age)\n",
    "\n",
    "# 2. Statistical tests\n",
    "from scipy.stats import ttest_ind\n",
    "male_ages = df[df['Gender']=='Male']['Age']\n",
    "female_ages = df[df['Gender']=='Female']['Age']\n",
    "t_stat, p_value = ttest_ind(male_ages, female_ages)\n",
    "\n",
    "# 3. Visual inspection\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='Gender', y='Age', data=df)\n",
    "```\n",
    "\n",
    "### **2. Different MI Estimators**\n",
    "```python\n",
    "# Try different estimation methods:\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "mi_regression = mutual_info_regression(age.reshape(-1,1), gender_encoded)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Business Interpretation**\n",
    "\n",
    "### **Customer Segmentation Insights:**\n",
    "**✅ Strategic Advantages:**\n",
    "- **Age-agnostic gender marketing** - no need to target specific age-gender combinations\n",
    "- **Simplified segmentation** - can analyze age and gender independently\n",
    "- **Unbiased customer acquisition** - no systematic age-gender patterns\n",
    "\n",
    "**📈 Marketing Implications:**\n",
    "- **Broad demographic appeal** - products/services attract all age-gender combinations\n",
    "- **No demographic gaps** - well-represented across age-gender matrix\n",
    "- **Flexible targeting** - can focus on age OR gender without interaction effects\n",
    "\n",
    "### **Data Quality Assessment:**\n",
    "**✅ Quality Indicators:**\n",
    "- **Natural demographic distribution** - suggests authentic customer data\n",
    "- **No collection bias** - sampling appears unbiased across demographics\n",
    "- **Representative dataset** - good foundation for analysis\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison with Other Variable Pairs**\n",
    "\n",
    "### **Expected MI Scores in Customer Data:**\n",
    "```python\n",
    "# Typical mutual information ranges:\n",
    "\n",
    "# Demographics (Gender-Age): 0.00-0.05  ← Your result fits here\n",
    "# Behavior-Demographics: 0.05-0.20\n",
    "# Behavior-Behavior: 0.10-0.40\n",
    "# Derived-Source: 0.30-0.80\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Actionable Insights**\n",
    "\n",
    "### **For Analysis Strategy:**\n",
    "1. **Treat independently** - analyze age and gender effects separately\n",
    "2. **No interaction terms needed** in statistical models\n",
    "3. **Simplified feature engineering** - no age-gender combinations required\n",
    "4. **Parallel analysis** - can run age-based and gender-based analyses independently\n",
    "\n",
    "### **For Business Strategy:**\n",
    "1. **Universal appeal** - products/services work across age-gender combinations\n",
    "2. **Simplified messaging** - no need for age-gender specific campaigns\n",
    "3. **Broad market reach** - marketing can focus on age OR gender, not both\n",
    "4. **Demographic flexibility** - expansion strategies don't need age-gender considerations\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaway**\n",
    "\n",
    "**Your mutual information score of 0.0000 indicates perfect statistical independence between Gender and Age in your customer dataset.** This is actually a positive finding, suggesting:\n",
    "\n",
    "1. **Unbiased, representative data** with natural demographic distribution\n",
    "2. **Simplified analysis approach** - treat variables independently  \n",
    "3. **Broad market appeal** - no demographic gaps or biases\n",
    "4. **Flexible business strategies** - age and gender can be targeted separately\n",
    "\n",
    "This independence makes your dataset ideal for robust customer segmentation analysis without demographic confounding effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44735d",
   "metadata": {},
   "source": [
    "## **Normalized Compression Distance (NCD)** \n",
    "\n",
    "The Normalized Compression Distance (NCD) is a way to measure the similarity between two pieces of data (like text files, images, or music). It's based on the idea that if two files are very similar, you can compress them together more efficiently than if they are very different.\n",
    "\n",
    "### Formula\n",
    "\n",
    "The formula for NCD is:\n",
    "\n",
    "**NCD(x, y) = (C(xy) - min(C(x), C(y))) / max(C(x), C(y))**\n",
    "\n",
    "### Parameter Explanation\n",
    "\n",
    "*   **x, y**: These are the two data objects (e.g., files, strings) you are comparing.\n",
    "*   **C(x)**: The size (in bytes) of file `x` after it has been compressed by a chosen compression algorithm (like gzip, bzip2, or zlib).\n",
    "*   **C(y)**: The size (in bytes) of file `y` after it has been compressed.\n",
    "*   **C(xy)**: The size (in bytes) of the file created by concatenating (joining) `x` and `y` together, and then compressing the combined file.\n",
    "*   **min(C(x), C(y))**: The smaller of the two compressed file sizes.\n",
    "*   **max(C(x), C(y))**: The larger of the two compressed file sizes.\n",
    "\n",
    "The choice of compressor is important. A \"good\" compressor, in this context, is one that is efficient at finding and representing patterns in data.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "*   **Range**: The NCD score is typically between 0 and 1, though it can sometimes be slightly greater than 1 depending on the compressor used.\n",
    "*   **High Similarity**: An NCD score close to **0** means the two files are very similar. This happens when compressing them together is much more efficient than compressing them separately (i.e., `C(xy)` is not much larger than `C(x)` or `C(y)`).\n",
    "*   **Low Similarity**: An NCD score close to **1** (or greater) means the files are very different. This indicates that there is no shared information between them that the compressor can take advantage of, so compressing them together offers no benefit.\n",
    "*   **Universality**: A key advantage of NCD is that it's \"feature-free.\" You don't need to tell it what features to look for (e.g., word counts, pixel colors). It discovers patterns on its own, making it a universal similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c86bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800711743772242\n"
     ]
    }
   ],
   "source": [
    "# Calculate NCD between two sequences\n",
    "\n",
    "compressor = zlib.compress\n",
    "gender_encoded = str(base_df['Gender'].tolist()).encode('utf-8')\n",
    "cx = len(compressor(gender_encoded))\n",
    "age_encoded = str(base_df['Age'].tolist()).encode('utf-8')\n",
    "cy = len(compressor(age_encoded))\n",
    "\n",
    "age_income_encoded = str((base_df['Age'] + base_df['Annual Income (k$)']).tolist()).encode('utf-8')\n",
    "cxy = len(compressor(age_income_encoded))\n",
    "\n",
    "numerator = cxy - min(cx, cy)\n",
    "denominator = max(cx, cy)\n",
    "\n",
    "normalized_compression_distance = numerator / denominator\n",
    "\n",
    "print(normalized_compression_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07f097",
   "metadata": {},
   "source": [
    "# **Understanding Normalized Compression Distance (NCD)**\n",
    "\n",
    "## **Mathematical Foundation**\n",
    "\n",
    "**Formula:** `NCD(x,y) = (C(xy) - min(C(x), C(y))) / max(C(x), C(y))`\n",
    "\n",
    "Where:\n",
    "- `C(x)` = compressed size of sequence x\n",
    "- `C(y)` = compressed size of sequence y  \n",
    "- `C(xy)` = compressed size of concatenated sequence x+y\n",
    "\n",
    "## **Interpretation Guide**\n",
    "\n",
    "### **NCD Value Ranges:**\n",
    "- **NCD ≈ 0.0**: Sequences are very similar/identical\n",
    "- **NCD ≈ 0.5**: Moderate similarity/relationship\n",
    "- **NCD ≈ 1.0**: Sequences are completely different/independent\n",
    "\n",
    "### **What NCD Measures:**\n",
    "1. **Algorithmic similarity** between data sequences\n",
    "2. **Shared patterns** that enable compression\n",
    "3. **Information overlap** between variables\n",
    "4. **Universal distance metric** - works for any data type\n",
    "\n",
    "## **Practical Applications**\n",
    "\n",
    "### **For Customer Segmentation:**\n",
    "- **Feature similarity**: Identify redundant variables\n",
    "- **Pattern detection**: Find hidden relationships\n",
    "- **Data quality**: Detect artificial patterns or duplicates\n",
    "- **Dimensionality reduction**: Group similar features\n",
    "\n",
    "### **Business Insights:**\n",
    "- **Low NCD**: Variables contain similar information (may be redundant)\n",
    "- **High NCD**: Variables are complementary (both needed for analysis)\n",
    "- **Medium NCD**: Partial overlap (consider interaction effects)\n",
    "\n",
    "## **Implementation Notes**\n",
    "\n",
    "### **Two Approaches Provided:**\n",
    "\n",
    "1. **String-based**: Converts data to strings, good for mixed types\n",
    "2. **Pickle-based**: Uses binary serialization, more robust for pandas\n",
    "\n",
    "### **Compression Algorithm Choice:**\n",
    "- **zlib**: General purpose, good balance of speed/compression\n",
    "- **bz2**: Better compression, slower\n",
    "- **lzma**: Best compression, slowest\n",
    "\n",
    "### **Data Type Considerations:**\n",
    "- **Categorical**: Direct string representation works well\n",
    "- **Numerical**: May need normalization or binning\n",
    "- **Mixed types**: Pickle-based approach recommended\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ff24e",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "1. Two Robust Methods:\n",
    "- String-based: Converts pandas Series to strings, handles mixed data types\n",
    "- Pickle-based: Uses binary serialization, more accurate for numerical data\n",
    "\n",
    "2. Proper Data Handling:\n",
    "- Converts pandas Series to lists before concatenation\n",
    "- Handles different data types (categorical + numerical)\n",
    "- Encodes to bytes for compression compatibility\n",
    "\n",
    "3. Comprehensive Examples:\n",
    "- Gender vs Age: Cross-type comparison (categorical vs numerical)\n",
    "- Self-similarity: Gender vs Gender (should be ~0)\n",
    "- Numerical comparison: Age vs Income\n",
    "- Method comparison: String vs Pickle approaches\n",
    "\n",
    "4. Error Prevention:\n",
    "- No direct concatenation of incompatible types\n",
    "- Proper encoding to UTF-8 bytes\n",
    "- Robust error handling for different pandas dtypes\n",
    "\n",
    "### Usage Examples:\n",
    "```python\n",
    "# Basic usage\n",
    "ncd_value, cx, cy, cxy = normalized_compression_distance(df['col1'], df['col2'])\n",
    "\n",
    "# With different compressor\n",
    "import bz2\n",
    "ncd_value = normalized_compression_distance(df['col1'], df['col2'], bz2.compress)\n",
    "\n",
    "# Pickle-based for mixed types\n",
    "ncd_value = ncd_pickle_based(df['categorical'], df['numerical'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c03bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Compression Distance (NCD) Implementation for Pandas Columns\n",
    "\n",
    "import zlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def normalized_compression_distance(x, y, compressor=zlib.compress):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Compression Distance between two pandas Series\n",
    "    \n",
    "    Parameters:\n",
    "    - x, y: pandas Series or array-like objects\n",
    "    - compressor: compression function (default: zlib.compress)\n",
    "    \n",
    "    Returns:\n",
    "    - NCD value between 0 and 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to strings and then to bytes for compression\n",
    "    x_str = str(x.tolist()).encode('utf-8')\n",
    "    y_str = str(y.tolist()).encode('utf-8') \n",
    "    xy_str = str(x.tolist() + y.tolist()).encode('utf-8')\n",
    "    \n",
    "    # Compress each sequence\n",
    "    cx = len(compressor(x_str))\n",
    "    cy = len(compressor(y_str))\n",
    "    cxy = len(compressor(xy_str))\n",
    "    \n",
    "    # Calculate NCD\n",
    "    ncd = (cxy - min(cx, cy)) / max(cx, cy)\n",
    "    \n",
    "    return ncd, cx, cy, cxy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "256aee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NCD: Gender vs Age ===\n",
      "Compressed size of Gender: 104 bytes\n",
      "Compressed size of Age: 281 bytes\n",
      "Compressed size of Combined: 391 bytes\n",
      "NCD (Gender, Age): 1.021352\n",
      "\n",
      "==================================================\n",
      "=== NCD: Gender vs Gender (Self-Similarity) ===\n",
      "NCD (Gender, Gender): 0.307692\n",
      "\n",
      "==================================================\n",
      "=== NCD: Age vs Annual Income ===\n",
      "Compressed size of Age: 281 bytes\n",
      "Compressed size of Income: 128 bytes\n",
      "Compressed size of Combined: 409 bytes\n",
      "NCD (Age, Income): 1.000000\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Example 1: NCD between Gender and Age\n",
    "ncd_gender_age, cx_gender, cy_age, cxy_combined = normalized_compression_distance(\n",
    "    base_df['Gender'], \n",
    "    base_df['Age']\n",
    ")\n",
    "\n",
    "print(\"=== NCD: Gender vs Age ===\")\n",
    "print(f\"Compressed size of Gender: {cx_gender} bytes\")\n",
    "print(f\"Compressed size of Age: {cy_age} bytes\") \n",
    "print(f\"Compressed size of Combined: {cxy_combined} bytes\")\n",
    "print(f\"NCD (Gender, Age): {ncd_gender_age:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Example 2: NCD between Gender and itself (should be close to 0)\n",
    "ncd_gender_self, cx1, cy1, cxy1 = normalized_compression_distance(\n",
    "    base_df['Gender'], \n",
    "    base_df['Gender']\n",
    ")\n",
    "\n",
    "print(\"=== NCD: Gender vs Gender (Self-Similarity) ===\")\n",
    "print(f\"NCD (Gender, Gender): {ncd_gender_self:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Example 3: NCD between different numerical columns\n",
    "ncd_age_income, cx2, cy2, cxy2 = normalized_compression_distance(\n",
    "    base_df['Age'], \n",
    "    base_df['Annual Income (k$)']\n",
    ")\n",
    "\n",
    "print(\"=== NCD: Age vs Annual Income ===\")\n",
    "print(f\"Compressed size of Age: {cx2} bytes\")\n",
    "print(f\"Compressed size of Income: {cy2} bytes\")\n",
    "print(f\"Compressed size of Combined: {cxy2} bytes\") \n",
    "print(f\"NCD (Age, Income): {ncd_age_income:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bacfc2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pickle-based NCD Implementation ===\n",
      "Pickle-based NCD (Gender, Age): 0.537155\n",
      "String-based NCD (Gender, Age): 1.021352\n",
      "Difference: 0.484197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Alternative implementation using pickle serialization (more robust for mixed types)\n",
    "def ncd_pickle_based(x, y, compressor=zlib.compress):\n",
    "    \"\"\"\n",
    "    NCD implementation using pickle serialization\n",
    "    Better for handling different pandas data types\n",
    "    \"\"\"\n",
    "    \n",
    "    # Serialize using pickle\n",
    "    x_bytes = pickle.dumps(x.values)\n",
    "    y_bytes = pickle.dumps(y.values)\n",
    "    xy_bytes = pickle.dumps(np.concatenate([x.values, y.values]))\n",
    "    \n",
    "    # Compress\n",
    "    cx = len(compressor(x_bytes))\n",
    "    cy = len(compressor(y_bytes))\n",
    "    cxy = len(compressor(xy_bytes))\n",
    "    \n",
    "    # Calculate NCD\n",
    "    ncd = (cxy - min(cx, cy)) / max(cx, cy)\n",
    "    \n",
    "    return ncd, cx, cy, cxy\n",
    "\n",
    "print(\"=== Pickle-based NCD Implementation ===\")\n",
    "\n",
    "# Compare with pickle-based approach\n",
    "ncd_pickle, cx_p, cy_p, cxy_p = ncd_pickle_based(\n",
    "    base_df['Gender'], \n",
    "    base_df['Age']\n",
    ")\n",
    "\n",
    "print(f\"Pickle-based NCD (Gender, Age): {ncd_pickle:.6f}\")\n",
    "print(f\"String-based NCD (Gender, Age): {ncd_gender_age:.6f}\")\n",
    "print(f\"Difference: {abs(ncd_pickle - ncd_gender_age):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cd62e",
   "metadata": {},
   "source": [
    "# **NCD Scores Interpretation for Your Customer Data**\n",
    "\n",
    "Based on your output values, here's how to interpret each NCD result:\n",
    "\n",
    "## **Your Results Summary:**\n",
    "\n",
    "```plaintext\n",
    "NCD (Gender, Age): 1.0214\n",
    "NCD (Gender, Gender): 0.3077\n",
    "NCD (Age, Income): 1.0000\n",
    "Pickle-based (Gender, Age): 0.5372\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **1. NCD (Gender, Age): 1.0214**\n",
    "\n",
    "### **Interpretation:**\n",
    "- **Value > 1.0**: Indicates the sequences are **completely independent**\n",
    "- **Maximum dissimilarity**: Gender and Age share no compressible patterns\n",
    "- **No algorithmic relationship**: Knowing one provides no compression benefit for the other\n",
    "\n",
    "### **What This Means:**\n",
    "**✅ Business Insights:**\n",
    "- **Perfect independence** between gender and age in your customer base\n",
    "- **No demographic bias** - age distribution is identical across genders\n",
    "- **Unbiased sampling** - confirms your mutual information result of 0.0000\n",
    "- **Universal appeal** - products attract all age-gender combinations equally\n",
    "\n",
    "**📊 Technical Implications:**\n",
    "- **No shared patterns** detectable by compression algorithms\n",
    "- **Complementary variables** - both needed for complete customer profiling\n",
    "- **No redundancy** - neither variable can predict the other\n",
    "\n",
    "---\n",
    "\n",
    "## **2. NCD (Gender, Gender): 0.3077**\n",
    "\n",
    "### **Interpretation:**\n",
    "- **Should theoretically be 0.0** (identical sequences)\n",
    "- **0.3077 indicates compression overhead** from data structure\n",
    "- **Pandas/string conversion artifacts** affecting the calculation\n",
    "\n",
    "### **Why Not Zero:**\n",
    "**Technical Factors:**\n",
    "- **Serialization overhead** - pandas metadata adds noise\n",
    "- **String conversion** - repeated \"Male\"/\"Female\" strings have structure\n",
    "- **Index information** - pandas Series includes positional data\n",
    "\n",
    "**Expected Behavior:**\n",
    "- **True identical sequences** would give NCD ≈ 0.0\n",
    "- **Your result shows method limitations** rather than data properties\n",
    "\n",
    "---\n",
    "\n",
    "## **3. NCD (Age, Income): 1.0000**\n",
    "\n",
    "### **Interpretation:**\n",
    "- **Perfect independence** between Age and Annual Income\n",
    "- **No compressible relationship** between these variables\n",
    "- **Maximum algorithmic distance** - completely different information content\n",
    "\n",
    "### **Business Implications:**\n",
    "**✅ Customer Insights:**\n",
    "- **Age doesn't predict income** in your customer base\n",
    "- **Diverse economic demographics** across all age groups\n",
    "- **No age-income correlation** - young and old customers have similar income ranges\n",
    "- **Broad market appeal** - products attract customers regardless of age-income combination\n",
    "\n",
    "**📈 Strategic Value:**\n",
    "- **Age-agnostic pricing** strategies are viable\n",
    "- **Income-based segmentation** independent of age considerations\n",
    "- **No demographic constraints** on product positioning\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Pickle vs String Methods: 0.5372 vs 1.0214**\n",
    "\n",
    "### **Method Comparison:**\n",
    "\n",
    "```plaintext\n",
    "String-based NCD: 1.0214 (Higher - suggests more independence)\n",
    "Pickle-based NCD: 0.5372 (Lower - detects some relationship)\n",
    "Difference: 0.4842 (Significant method dependency)\n",
    "```\n",
    "\n",
    "\n",
    "### **Why The Difference:**\n",
    "**String Method (1.0214):**\n",
    "- **Converts to human-readable strings** - loses numerical relationships\n",
    "- **Less sensitive** to subtle numerical patterns\n",
    "- **Better for categorical comparisons**\n",
    "\n",
    "**Pickle Method (0.5372):**\n",
    "- **Preserves binary data structure** - maintains numerical relationships  \n",
    "- **More sensitive** to data type similarities\n",
    "- **Better for mixed-type comparisons**\n",
    "\n",
    "### **Which To Trust:**\n",
    "- **For Gender-Age**: **String method (1.0214)** more appropriate\n",
    "- **Categorical vs Numerical**: String conversion makes sense\n",
    "- **Confirms independence**: Both methods show high NCD values\n",
    "\n",
    "---\n",
    "\n",
    "## **Overall Pattern Analysis**\n",
    "\n",
    "### **Consistency Check:**\n",
    "\n",
    "```plaintext\n",
    "Mutual Information (Gender, Age): 0.0000 ✅ Perfect independence\n",
    "NCD (Gender, Age): 1.0214 ✅ Perfect independence\n",
    "NCD (Age, Income): 1.0000 ✅ Perfect independence\n",
    "```\n",
    "\n",
    "\n",
    "**All measures consistently show independence!**\n",
    "\n",
    "---\n",
    "\n",
    "## **Business Interpretation Matrix**\n",
    "\n",
    "| Variable Pair | NCD Score | Relationship | Business Implication |\n",
    "|---------------|-----------|--------------|---------------------|\n",
    "| **Gender-Age** | 1.0214 | Independent | Age-neutral gender strategies |\n",
    "| **Age-Income** | 1.0000 | Independent | Income-neutral age targeting |\n",
    "| **Gender-Gender** | 0.3077 | Identical* | *Method artifact, not meaningful |\n",
    "\n",
    "---\n",
    "\n",
    "## **Actionable Insights**\n",
    "\n",
    "### **For Customer Segmentation:**\n",
    "1. **Independent variables** - analyze Gender, Age, and Income separately\n",
    "2. **No interaction effects** needed in statistical models\n",
    "3. **Simplified segmentation** - can use single-variable approaches\n",
    "4. **Broad targeting** - no demographic constraints\n",
    "\n",
    "### **For Marketing Strategy:**\n",
    "1. **Universal products** - appeal across all demographic combinations\n",
    "2. **Flexible campaigns** - can target age OR gender OR income independently  \n",
    "3. **No demographic gaps** - well-represented across all combinations\n",
    "4. **Simplified messaging** - no need for complex demographic interactions\n",
    "\n",
    "### **For Data Quality:**\n",
    "1. **Excellent representativeness** - no sampling biases detected\n",
    "2. **Natural distributions** - authentic customer demographics\n",
    "3. **Robust foundation** - ideal for unbiased analysis\n",
    "4. **High-quality dataset** - suitable for machine learning\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "\n",
    "**Your NCD scores confirm exceptional demographic independence in your customer dataset:**\n",
    "\n",
    "1. **Gender and Age are completely independent** (NCD = 1.02) - no age bias by gender\n",
    "2. **Age and Income are completely independent** (NCD = 1.00) - no income bias by age  \n",
    "3. **Perfect demographic balance** - ideal for unbiased customer segmentation\n",
    "4. **Universal market appeal** - products/services attract diverse demographics\n",
    "5. **Simplified analysis approach** - can treat demographic variables independently\n",
    "\n",
    "**Bottom Line:** Your customer base represents a perfectly balanced, unbiased sample with no demographic interdependencies - ideal for robust customer segmentation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461e4ce",
   "metadata": {},
   "source": [
    "# **Optimal Binning Methods: Complete Guide**\n",
    "\n",
    "Based on your notebook's `optimal_bins()` function, here's a comprehensive analysis of the three main binning methods for converting continuous data to categorical for entropy analysis.\n",
    "\n",
    "## **1. Sturges' Rule**\n",
    "\n",
    "### **Mathematical Formula:**\n",
    "```plaintext\n",
    "k = 1 + log₂(n)\n",
    "```\n",
    "\n",
    "### **Parameter Breakdown:**\n",
    "- **k**: Number of bins (output)\n",
    "- **n**: Sample size (number of observations)\n",
    "- **log₂**: Base-2 logarithm\n",
    "- **1**: Constant offset ensuring minimum of 1 bin\n",
    "\n",
    "### **Conceptual Foundation:**\n",
    "\n",
    "#### **Information Theory Basis:**\n",
    "- **Developed by Herbert Sturges (1926)** - one of the earliest systematic binning rules\n",
    "- **Based on binary decision tree logic** - each observation can be classified through a series of binary splits\n",
    "- **Assumes normal distribution** - optimal when data follows Gaussian distribution\n",
    "- **Information content principle** - number of bins should grow logarithmically with sample size\n",
    "\n",
    "#### **How It Works:**\n",
    "```python\n",
    "# Conceptual reasoning:\n",
    "# - With n observations, you need log₂(n) binary decisions to distinguish them\n",
    "# - Each binary decision corresponds to a potential bin boundary\n",
    "# - The +1 accounts for the fact that k-1 boundaries create k bins\n",
    "```\n",
    "\n",
    "### **Strengths:**\n",
    "- **Simple and fast** - only depends on sample size\n",
    "- **Theoretically grounded** - based on information theory\n",
    "- **Conservative** - tends to create fewer bins, avoiding over-binning\n",
    "- **Widely used** - standard in many statistical packages\n",
    "\n",
    "### **Weaknesses:**\n",
    "- **Ignores data distribution** - doesn't consider actual data characteristics\n",
    "- **Assumes normality** - may be suboptimal for skewed or multimodal data\n",
    "- **Fixed relationship** - doesn't adapt to data complexity\n",
    "- **Can under-bin** - may lose important distributional features\n",
    "\n",
    "### **When to Use:**\n",
    "```python\n",
    "# Ideal scenarios:\n",
    "- Normal or near-normal distributions\n",
    "- Exploratory data analysis (quick overview)\n",
    "- Small to medium datasets (n < 1000)\n",
    "- When computational speed is priority\n",
    "- As a baseline comparison for other methods\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Scott's Rule**\n",
    "\n",
    "### **Mathematical Formula:**\n",
    "```plaintext\n",
    "h = 3.5 × σ × n^(-1/3)\n",
    "k = (max - min) / h\n",
    "```\n",
    "\n",
    "### **Parameter Breakdown:**\n",
    "- **h**: Bin width (intermediate calculation)\n",
    "- **σ**: Standard deviation of the data (`np.std(data)`)\n",
    "- **n**: Sample size\n",
    "- **3.5**: Empirical constant derived from normal distribution theory\n",
    "- **n^(-1/3)**: Cube root scaling factor\n",
    "- **max - min**: Data range\n",
    "- **k**: Number of bins (final output)\n",
    "\n",
    "### **Conceptual Foundation:**\n",
    "\n",
    "#### **Statistical Theory Basis:**\n",
    "- **Developed by David Scott (1979)** - based on minimizing integrated mean squared error\n",
    "- **Optimal histogram theory** - minimizes bias-variance tradeoff in density estimation\n",
    "- **Normal reference rule** - assumes underlying normal distribution\n",
    "- **Asymptotic optimality** - becomes more accurate as sample size increases\n",
    "\n",
    "#### **How It Works:**\n",
    "```python\n",
    "# Theoretical reasoning:\n",
    "# 1. Assumes data follows normal distribution\n",
    "# 2. Derives optimal bin width that minimizes estimation error\n",
    "# 3. Balances bias (too few bins) vs variance (too many bins)\n",
    "# 4. Uses sample standard deviation as measure of data spread\n",
    "# 5. Scales bin width inversely with cube root of sample size\n",
    "```\n",
    "\n",
    "### **Mathematical Derivation:**\n",
    "The 3.5 constant comes from:\n",
    "```plaintext\n",
    "h = (24√π/35)^(1/3) × σ × n^(-1/3) ≈ 3.49 × σ × n^(-1/3)\n",
    "```\n",
    "\n",
    "### **Strengths:**\n",
    "- **Data-aware** - considers actual data spread (standard deviation)\n",
    "- **Theoretically optimal** - minimizes mean squared error for normal data\n",
    "- **Scales appropriately** - more bins for larger datasets\n",
    "- **Robust to sample size** - performs well across different n values\n",
    "\n",
    "### **Weaknesses:**\n",
    "- **Assumes normality** - suboptimal for non-normal distributions\n",
    "- **Sensitive to outliers** - standard deviation affected by extreme values\n",
    "- **May over-bin** - can create too many bins for small datasets\n",
    "- **Computationally intensive** - requires calculating standard deviation\n",
    "\n",
    "### **When to Use:**\n",
    "```python\n",
    "# Ideal scenarios:\n",
    "- Normal or approximately normal distributions\n",
    "- Medium to large datasets (n > 100)\n",
    "- When data spread varies significantly\n",
    "- Density estimation applications\n",
    "- When you want theoretically optimal binning\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Freedman-Diaconis Rule**\n",
    "\n",
    "### **Mathematical Formula:**\n",
    "```plaintext\n",
    "h = 2 × IQR × n^(-1/3)\n",
    "k = (max - min) / h\n",
    "```\n",
    "\n",
    "### **Parameter Breakdown:**\n",
    "- **h**: Bin width (intermediate calculation)\n",
    "- **IQR**: Interquartile Range (Q₃ - Q₁)\n",
    "- **Q₃**: 75th percentile (`np.percentile(data, 75)`)\n",
    "- **Q₁**: 25th percentile (`np.percentile(data, 25)`)\n",
    "- **n**: Sample size\n",
    "- **2**: Empirical constant\n",
    "- **n^(-1/3)**: Cube root scaling factor\n",
    "- **max - min**: Data range\n",
    "- **k**: Number of bins (final output)\n",
    "\n",
    "### **Conceptual Foundation:**\n",
    "\n",
    "#### **Robust Statistics Basis:**\n",
    "- **Developed by Freedman & Diaconis (1981)** - improvement over Scott's rule\n",
    "- **Robust to outliers** - uses IQR instead of standard deviation\n",
    "- **Non-parametric approach** - doesn't assume specific distribution\n",
    "- **Resistant to extreme values** - IQR unaffected by outliers\n",
    "- **Empirically validated** - performs well across various distributions\n",
    "\n",
    "#### **How It Works:**\n",
    "```python\n",
    "# Theoretical reasoning:\n",
    "# 1. Uses IQR as robust measure of data spread\n",
    "# 2. IQR captures central 50% of data, ignoring extreme tails\n",
    "# 3. Maintains same scaling relationship as Scott's rule (n^(-1/3))\n",
    "# 4. Factor of 2 empirically determined for optimal performance\n",
    "# 5. Works well for skewed, heavy-tailed, or contaminated data\n",
    "```\n",
    "\n",
    "### **Why IQR Instead of Standard Deviation:**\n",
    "```python\n",
    "# Comparison of spread measures:\n",
    "std_dev = np.std(data)      # Sensitive to outliers\n",
    "iqr = np.percentile(data, 75) - np.percentile(data, 25)  # Robust to outliers\n",
    "\n",
    "# Example impact:\n",
    "normal_data = [1, 2, 3, 4, 5]\n",
    "with_outlier = [1, 2, 3, 4, 100]\n",
    "\n",
    "# Standard deviation changes dramatically with outlier\n",
    "# IQR remains stable\n",
    "```\n",
    "\n",
    "### **Strengths:**\n",
    "- **Outlier robust** - unaffected by extreme values\n",
    "- **Distribution agnostic** - works well for various distribution shapes\n",
    "- **Empirically validated** - performs well in practice\n",
    "- **Balanced approach** - good compromise between simplicity and adaptability\n",
    "- **Handles skewness** - better than Scott's rule for asymmetric data\n",
    "\n",
    "### **Weaknesses:**\n",
    "- **May under-bin** - for very peaked distributions\n",
    "- **Less theoretical foundation** - more empirical than Scott's rule\n",
    "- **Requires percentile calculation** - slightly more computational overhead\n",
    "- **May not capture tail behavior** - focuses on central data\n",
    "\n",
    "### **When to Use:**\n",
    "```python\n",
    "# Ideal scenarios:\n",
    "- Skewed or asymmetric distributions\n",
    "- Data with potential outliers\n",
    "- Unknown or mixed distribution types\n",
    "- Robust analysis requirements\n",
    "- When you want distribution-agnostic binning\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparative Analysis**\n",
    "\n",
    "### **Performance by Data Type:**\n",
    "\n",
    "| Distribution Type | Sturges | Scott | Freedman-Diaconis | Best Choice |\n",
    "|------------------|---------|-------|-------------------|-------------|\n",
    "| **Normal** | Good | Excellent | Good | Scott |\n",
    "| **Skewed** | Poor | Poor | Excellent | Freedman-Diaconis |\n",
    "| **Heavy-tailed** | Poor | Poor | Good | Freedman-Diaconis |\n",
    "| **With outliers** | Poor | Poor | Excellent | Freedman-Diaconis |\n",
    "| **Multimodal** | Poor | Fair | Good | Freedman-Diaconis |\n",
    "| **Uniform** | Fair | Good | Good | Scott or F-D |\n",
    "\n",
    "### **Sample Size Considerations:**\n",
    "\n",
    "```python\n",
    "# Small samples (n < 50):\n",
    "- Sturges: Often too few bins (3-6 bins)\n",
    "- Scott: May create too many bins\n",
    "- F-D: Usually appropriate\n",
    "\n",
    "# Medium samples (50 < n < 500):\n",
    "- Sturges: Reasonable for normal data\n",
    "- Scott: Good for normal data\n",
    "- F-D: Good general choice\n",
    "\n",
    "# Large samples (n > 500):\n",
    "- Sturges: Often too few bins\n",
    "- Scott: Good for normal data\n",
    "- F-D: Excellent general choice\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Decision Framework**\n",
    "\n",
    "### **Step 1: Assess Your Data**\n",
    "```python\n",
    "# Quick data assessment:\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Check normality\n",
    "shapiro_stat, shapiro_p = stats.shapiro(data[:5000])  # Limit for large datasets\n",
    "normal_like = shapiro_p > 0.05\n",
    "\n",
    "# Check for outliers\n",
    "q1, q3 = np.percentile(data, [25, 75])\n",
    "iqr = q3 - q1\n",
    "outlier_bounds = [q1 - 1.5*iqr, q3 + 1.5*iqr]\n",
    "has_outliers = (data < outlier_bounds[0]).any() or (data > outlier_bounds[1]).any()\n",
    "\n",
    "# Check skewness\n",
    "skewness = stats.skew(data)\n",
    "highly_skewed = abs(skewness) > 1\n",
    "```\n",
    "\n",
    "### **Step 2: Choose Method**\n",
    "```python\n",
    "def choose_binning_method(data):\n",
    "    n = len(data)\n",
    "    \n",
    "    # Assess data characteristics\n",
    "    normal_like = stats.shapiro(data[:5000])[1] > 0.05 if n > 50 else True\n",
    "    skewness = abs(stats.skew(data))\n",
    "    has_outliers = detect_outliers(data)\n",
    "    \n",
    "    if normal_like and not has_outliers and skewness < 0.5:\n",
    "        return \"scott\"  # Optimal for normal data\n",
    "    elif has_outliers or skewness > 1:\n",
    "        return \"freedman\"  # Robust to outliers and skewness\n",
    "    elif n < 100:\n",
    "        return \"sturges\"  # Simple and adequate for small samples\n",
    "    else:\n",
    "        return \"freedman\"  # Safe general choice\n",
    "```\n",
    "\n",
    "### **Step 3: Validate Results**\n",
    "```python\n",
    "def validate_binning(data, method, k):\n",
    "    \"\"\"Validate binning results\"\"\"\n",
    "    \n",
    "    # Check for reasonable number of bins\n",
    "    reasonable_range = (3, min(50, len(data)//10))\n",
    "    if not (reasonable_range[0] <= k <= reasonable_range[1]):\n",
    "        print(f\"Warning: {k} bins may be suboptimal\")\n",
    "    \n",
    "    # Check for empty bins\n",
    "    hist, _ = np.histogram(data, bins=k)\n",
    "    empty_bins = (hist == 0).sum()\n",
    "    if empty_bins > 0:\n",
    "        print(f\"Warning: {empty_bins} empty bins detected\")\n",
    "    \n",
    "    # Check bin occupancy\n",
    "    min_occupancy = hist.min()\n",
    "    if min_occupancy < 5:\n",
    "        print(f\"Warning: Some bins have very few observations ({min_occupancy})\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Implementation**\n",
    "\n",
    "### **Enhanced Binning Function:**\n",
    "```python\n",
    "def optimal_bins_enhanced(data, method='auto', validate=True):\n",
    "    \"\"\"\n",
    "    Enhanced binning with automatic method selection and validation\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    \n",
    "    if method == 'auto':\n",
    "        method = choose_binning_method(data)\n",
    "        print(f\"Auto-selected method: {method}\")\n",
    "    \n",
    "    if method == 'sturges':\n",
    "        k = int(1 + np.log2(n))\n",
    "        \n",
    "    elif method == 'scott':\n",
    "        h = 3.5 * np.std(data) / (n ** (1/3))\n",
    "        k = int((np.max(data) - np.min(data)) / h)\n",
    "        \n",
    "    elif method == 'freedman':\n",
    "        iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "        h = 2 * iqr / (n ** (1/3))\n",
    "        k = int((np.max(data) - np.min(data)) / h)\n",
    "    \n",
    "    # Ensure minimum bins\n",
    "    k = max(k, 1)\n",
    "    \n",
    "    if validate:\n",
    "        validate_binning(data, method, k)\n",
    "    \n",
    "    return k, method\n",
    "```\n",
    "\n",
    "### **Comparison Function:**\n",
    "```python\n",
    "def compare_binning_methods(data):\n",
    "    \"\"\"Compare all three methods for given data\"\"\"\n",
    "    \n",
    "    methods = ['sturges', 'scott', 'freedman']\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        k = optimal_bins(data, method)\n",
    "        hist, edges = np.histogram(data, bins=k)\n",
    "        \n",
    "        results[method] = {\n",
    "            'bins': k,\n",
    "            'empty_bins': (hist == 0).sum(),\n",
    "            'min_count': hist.min(),\n",
    "            'max_count': hist.max(),\n",
    "            'avg_count': hist.mean()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Interpretation Guidelines**\n",
    "\n",
    "### **Number of Bins Interpretation:**\n",
    "```python\n",
    "# General guidelines:\n",
    "bins_interpretation = {\n",
    "    \"1-5\": \"Very coarse - may lose important patterns\",\n",
    "    \"6-15\": \"Reasonable for most analyses\", \n",
    "    \"16-30\": \"Fine-grained - good for large datasets\",\n",
    "    \"31-50\": \"Very fine - may introduce noise\",\n",
    "    \"50+\": \"Likely too many - consider data reduction\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **Method-Specific Considerations:**\n",
    "\n",
    "#### **Sturges Results:**\n",
    "- **Low bin count**: Expected for small datasets\n",
    "- **Consistent across data types**: Method doesn't adapt to distribution\n",
    "- **Use as baseline**: Compare other methods against this\n",
    "\n",
    "#### **Scott Results:**\n",
    "- **Higher than Sturges**: Usually produces more bins\n",
    "- **Sensitive to outliers**: Dramatically different results with extreme values\n",
    "- **Optimal for normal data**: Should give best results for Gaussian distributions\n",
    "\n",
    "#### **Freedman-Diaconis Results:**\n",
    "- **Robust consistency**: Similar results even with outliers\n",
    "- **Moderate bin counts**: Usually between Sturges and Scott\n",
    "- **Good general choice**: Reliable across various distributions\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "---\n",
    "\n",
    "## **Your Customer Segmentation Data Example**\n",
    "\n",
    "Based on your notebook output showing all methods return **8 bins** for all columns:\n",
    "\n",
    "### **Analysis:**\n",
    "```python\n",
    "# Your results: n = 200 observations\n",
    "sturges_bins = 1 + log₂(200) = 1 + 7.64 ≈ 8 bins\n",
    "```\n",
    "\n",
    "### **Why All Methods Give Same Result:**\n",
    "1. **Small dataset (n=200)** - methods converge for smaller samples\n",
    "2. **Well-behaved data** - no extreme outliers or skewness\n",
    "3. **Reasonable distributions** - data characteristics don't trigger major differences\n",
    "\n",
    "### **Recommendations for Your Data:**\n",
    "- **8 bins is reasonable** for 200 observations\n",
    "- **Use Freedman-Diaconis** as default - most robust choice\n",
    "- **Validate with histograms** - check for empty bins or poor distribution\n",
    "- **Consider domain knowledge** - business-meaningful breakpoints may be better\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "\n",
    "1. **Start with Freedman-Diaconis** - best general-purpose method\n",
    "2. **Use Scott for normal data** - when you're confident about normality\n",
    "3. **Use Sturges for quick exploration** - fast baseline assessment\n",
    "4. **Always validate results** - check for empty bins and reasonable occupancy\n",
    "5. **Consider domain knowledge** - statistical optimality isn't always business optimal\n",
    "\n",
    "The choice of binning method can significantly impact your entropy calculations, so understanding these differences is crucial for accurate analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0862b25f",
   "metadata": {},
   "source": [
    "## Mermaid flowchart that guides you through choosing the optimal binning method based on data properties and statistical tests.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"Start: Continuous Data<br/>Need Binning for Entropy Analysis\"] --> B[\"Calculate Basic Statistics<br/>n = sample size<br/>σ = standard deviation<br/>IQR = Q3 - Q1\"]\n",
    "    \n",
    "    B --> C{\"Sample Size<br/>Check\"}\n",
    "    \n",
    "    C -->|\"n < 50\"| D[\"Small Sample<br/>Use Sturges Rule<br/>k = 1 + log₂(n)\"]\n",
    "    \n",
    "    C -->|\"n ≥ 50\"| E[\"Test Data Properties\"]\n",
    "    \n",
    "    E --> F[\"Normality Test<br/>Shapiro-Wilk Test<br/>H₀: Data is normal\"]\n",
    "    \n",
    "    F --> G[\"Outlier Detection<br/>IQR Method<br/>Outliers: < Q1-1.5×IQR or > Q3+1.5×IQR\"]\n",
    "    \n",
    "    G --> H[\"Skewness Test<br/>Calculate skewness coefficient<br/>|skew| > 1 = highly skewed\"]\n",
    "    \n",
    "    H --> I{\"Is Data<br/>Normal?<br/>(p > 0.05)\"}\n",
    "    \n",
    "    I -->|\"Yes\"| J{\"Has<br/>Outliers?\"}\n",
    "    \n",
    "    J -->|\"No\"| K{\"Skewness<br/>|skew| < 0.5?\"}\n",
    "    \n",
    "    K -->|\"Yes\"| L[\"✅ Scott's Rule<br/>h = 3.5 × σ × n^(-1/3)<br/>k = (max-min) / h<br/><br/>Best for normal data\"]\n",
    "    \n",
    "    K -->|\"No\"| M[\"⚠️ Freedman-Diaconis<br/>h = 2 × IQR × n^(-1/3)<br/>k = (max-min) / h<br/><br/>Robust to mild skewness\"]\n",
    "    \n",
    "    J -->|\"Yes\"| M\n",
    "    \n",
    "    I -->|\"No\"| N{\"Has Severe<br/>Outliers or<br/>|skew| > 1?\"}\n",
    "    \n",
    "    N -->|\"Yes\"| O[\"✅ Freedman-Diaconis<br/>h = 2 × IQR × n^(-1/3)<br/>k = (max-min) / h<br/><br/>Robust to outliers & skewness\"]\n",
    "    \n",
    "    N -->|\"No\"| P{\"Distribution<br/>Type Known?\"}\n",
    "    \n",
    "    P -->|\"Unknown\"| O\n",
    "    P -->|\"Heavy-tailed\"| O\n",
    "    P -->|\"Multimodal\"| O\n",
    "    \n",
    "    D --> Q[\"Validation Step\"]\n",
    "    L --> Q\n",
    "    M --> Q\n",
    "    O --> Q\n",
    "    \n",
    "    Q --> R[\"Check Results:<br/>• 3 ≤ k ≤ min(50, n/10)<br/>• No empty bins<br/>• Min bin count ≥ 5\"]\n",
    "    \n",
    "    R --> S{\"Results<br/>Valid?\"}\n",
    "    \n",
    "    S -->|\"Yes\"| T[\"✅ Use Selected Method<br/>Proceed with Entropy Analysis\"]\n",
    "    \n",
    "    S -->|\"No\"| U[\"⚠️ Adjust Method:<br/>• Too few bins → try Scott's<br/>• Too many bins → try Sturges<br/>• Empty bins → reduce k\"]\n",
    "    \n",
    "    U --> V[\"Manual Override<br/>Consider domain knowledge<br/>or business requirements\"]\n",
    "    \n",
    "    V --> T\n",
    "    \n",
    "    style A fill:#ef6c00\n",
    "    style T fill:#2e7d32\n",
    "    style L fill:#1565c0\n",
    "    style O fill:#1565c0\n",
    "    style M fill:#1565c0\n",
    "    style D fill:#1565c0\n",
    "    style U fill:#ef6c00\n",
    "```\n",
    "\n",
    "I've created a comprehensive flowchart that guides you through selecting the optimal binning method based on systematic data property testing. Here's how to use this decision tree:\n",
    "\n",
    "## **Key Decision Points:**\n",
    "\n",
    "### **1. Sample Size Gateway**\n",
    "- **n < 50**: Automatically use Sturges (simple, adequate for small samples)\n",
    "- **n ≥ 50**: Proceed to detailed testing\n",
    "\n",
    "### **2. Statistical Tests Sequence**\n",
    "- **Normality**: Shapiro-Wilk test (p > 0.05 suggests normal)\n",
    "- **Outliers**: IQR method (values beyond Q1-1.5×IQR or Q3+1.5×IQR)\n",
    "- **Skewness**: |skewness| > 1 indicates high skewness\n",
    "\n",
    "### **3. Method Selection Logic**\n",
    "- **Scott's Rule**: Best for normal, non-skewed data without outliers\n",
    "- **Freedman-Diaconis**: Robust choice for problematic data (outliers, skewness, unknown distributions)\n",
    "- **Sturges**: Fallback for small samples or quick exploration\n",
    "\n",
    "### **4. Validation Step**\n",
    "Always validate results:\n",
    "- **Reasonable range**: 3 ≤ k ≤ min(50, n/10)\n",
    "- **No empty bins**: All bins should have observations\n",
    "- **Adequate occupancy**: Minimum 5 observations per bin\n",
    "\n",
    "## **Implementation Code for the Flowchart:**\n",
    "\n",
    "```python\n",
    "def choose_binning_method_systematic(data):\n",
    "    \"\"\"\n",
    "    Systematic binning method selection following the flowchart\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    n = len(data)\n",
    "    \n",
    "    # Step 1: Sample size check\n",
    "    if n < 50:\n",
    "        return \"sturges\", \"Small sample size\"\n",
    "    \n",
    "    # Step 2: Calculate statistics\n",
    "    sigma = np.std(data)\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Step 3: Normality test\n",
    "    if n <= 5000:\n",
    "        _, p_normal = stats.shapiro(data)\n",
    "    else:\n",
    "        _, p_normal = stats.normaltest(data)  # For large samples\n",
    "    \n",
    "    is_normal = p_normal > 0.05\n",
    "    \n",
    "    # Step 4: Outlier detection\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    has_outliers = np.any((data < lower_bound) | (data > upper_bound))\n",
    "    \n",
    "    # Step 5: Skewness test\n",
    "    skewness = abs(stats.skew(data))\n",
    "    highly_skewed = skewness > 1\n",
    "    mildly_skewed = skewness > 0.5\n",
    "    \n",
    "    # Decision logic\n",
    "    if is_normal and not has_outliers and not mildly_skewed:\n",
    "        return \"scott\", \"Normal distribution, no outliers, low skewness\"\n",
    "    elif is_normal and not has_outliers and mildly_skewed:\n",
    "        return \"freedman\", \"Normal but mildly skewed\"\n",
    "    elif has_outliers or highly_skewed:\n",
    "        return \"freedman\", \"Has outliers or highly skewed\"\n",
    "    else:\n",
    "        return \"freedman\", \"Safe general choice for unknown distribution\"\n",
    "\n",
    "def validate_binning_results(data, k, method):\n",
    "    \"\"\"\n",
    "    Validate binning results according to flowchart criteria\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    \n",
    "    # Check reasonable range\n",
    "    min_bins, max_bins = 3, min(50, n//10)\n",
    "    range_ok = min_bins <= k <= max_bins\n",
    "    \n",
    "    # Check for empty bins\n",
    "    hist, _ = np.histogram(data, bins=k)\n",
    "    empty_bins = np.sum(hist == 0)\n",
    "    no_empty = empty_bins == 0\n",
    "    \n",
    "    # Check minimum occupancy\n",
    "    min_occupancy = hist.min() if len(hist) > 0 else 0\n",
    "    adequate_occupancy = min_occupancy >= 5\n",
    "    \n",
    "    validation = {\n",
    "        'range_ok': range_ok,\n",
    "        'no_empty_bins': no_empty,\n",
    "        'adequate_occupancy': adequate_occupancy,\n",
    "        'all_valid': range_ok and no_empty and adequate_occupancy,\n",
    "        'empty_bins_count': empty_bins,\n",
    "        'min_occupancy': min_occupancy,\n",
    "        'recommended_range': (min_bins, max_bins)\n",
    "    }\n",
    "    \n",
    "    return validation\n",
    "```\n",
    "\n",
    "This flowchart provides a systematic, data-driven approach to binning method selection that removes guesswork and ensures optimal entropy analysis results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd516e65",
   "metadata": {},
   "source": [
    "## Binning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f44f31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerID\n",
      "method used is sturges\n",
      "8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gender\n",
      "method used is sturges\n",
      "8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Age\n",
      "method used is sturges\n",
      "8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Annual Income (k$)\n",
      "method used is sturges\n",
      "8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Spending Score (1-100)\n",
      "method used is sturges\n",
      "8\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def optimal_bins(data, method='sturges'):\n",
    "    n = len(data)\n",
    "    if method == 'sturges':\n",
    "        print(\"method used is sturges\")\n",
    "        return int(1 + np.log2(n))\n",
    "    elif method == 'scott':\n",
    "        print(\"method used is scott\")\n",
    "        h = 3.5 * np.std(data) / (n ** (1/3))\n",
    "        return int((np.max(data) - np.min(data)) / h)\n",
    "    elif method == 'freedman':\n",
    "        print(\"method used is freedman\")\n",
    "        iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "        h = 2 * iqr / (n ** (1/3))\n",
    "        return int((np.max(data) - np.min(data)) / h)\n",
    "\n",
    "# print(optimal_bins(base_df['Annual Income (k$)']))\n",
    "for cols in base_df.columns:\n",
    "    print(cols)\n",
    "    print(optimal_bins(base_df[cols]))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9eee17",
   "metadata": {},
   "source": [
    "# **Entropy & Information Methods: Comprehensive Selection Guide**\n",
    "\n",
    "Based on your notebook analysis, here's a detailed comparison of when to use each method and their relative strengths:\n",
    "\n",
    "## **1. Shannon Entropy**\n",
    "\n",
    "### **Strengths:**\n",
    "- **Most widely used and understood** - standard baseline for information theory\n",
    "- **Balanced sensitivity** - treats all probability ranges equally\n",
    "- **Theoretical foundation** - extensive mathematical literature and applications\n",
    "- **Interpretable units** - bits (base 2) have clear meaning\n",
    "- **Optimal for general uncertainty** - maximizes at uniform distribution\n",
    "\n",
    "### **When to Use:**\n",
    "- **Primary analysis** - always start here as your baseline\n",
    "- **General diversity assessment** - when you want overall randomness measure\n",
    "- **Comparative studies** - most papers use Shannon for comparison\n",
    "- **Machine learning** - decision trees, feature selection\n",
    "- **Equal weighting scenarios** - when all categories matter equally\n",
    "\n",
    "### **Best For:**\n",
    "```python\n",
    "# Scenarios where Shannon excels:\n",
    "- Balanced categorical distributions (like your Gender: 0.9896)\n",
    "- Information gain calculations in ML\n",
    "- General-purpose entropy measurement\n",
    "- When you need a single, interpretable diversity score\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Rényi Entropy (Generalized Entropy)**\n",
    "\n",
    "### **Strengths:**\n",
    "- **Tunable sensitivity** - α parameter controls focus on rare vs common events\n",
    "- **Unifies multiple measures** - Shannon, collision, min-entropy as special cases\n",
    "- **Flexible analysis** - can emphasize different aspects of distribution\n",
    "- **Robust to outliers** - higher α values reduce impact of rare categories\n",
    "\n",
    "### **When to Use by α Value:**\n",
    "\n",
    "#### **α < 1 (e.g., 0.1, 0.5):**\n",
    "```python\n",
    "# Use when you want to emphasize RARE categories:\n",
    "- Fraud detection (rare fraudulent transactions)\n",
    "- Quality control (rare defects)\n",
    "- Minority representation analysis\n",
    "- Tail risk assessment\n",
    "```\n",
    "\n",
    "#### **α = 2 (Collision Entropy):**\n",
    "```python\n",
    "# Use for:\n",
    "- Cryptographic applications\n",
    "- Password strength assessment  \n",
    "- When you care about probability of collisions\n",
    "- Concentration risk analysis\n",
    "```\n",
    "\n",
    "#### **α > 2 (e.g., 10):**\n",
    "```python\n",
    "# Use when you want to emphasize DOMINANT categories:\n",
    "- Market concentration analysis\n",
    "- Monopoly detection\n",
    "- When majority class is most important\n",
    "- Conservative diversity estimates\n",
    "```\n",
    "\n",
    "### **Selection Criteria:**\n",
    "- **Imbalanced data**: Use α > 1 to focus on dominant classes\n",
    "- **Rare event focus**: Use α < 1 to emphasize minorities\n",
    "- **Risk assessment**: Higher α for conservative estimates\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Gini Impurity**\n",
    "\n",
    "### **Strengths:**\n",
    "- **Computationally efficient** - no logarithms required\n",
    "- **Decision tree standard** - widely used in CART algorithms\n",
    "- **Intuitive interpretation** - probability of misclassification\n",
    "- **Fast calculation** - good for large datasets\n",
    "\n",
    "### **When to Use:**\n",
    "- **Machine learning preprocessing** - feature selection for tree-based models\n",
    "- **Quick diversity assessment** - when computational speed matters\n",
    "- **Binary classification focus** - optimal for 2-class problems\n",
    "- **Decision tree analysis** - understanding split quality\n",
    "\n",
    "### **Limitations:**\n",
    "- **Less sensitive than Shannon** - may miss subtle patterns\n",
    "- **Limited theoretical foundation** - compared to information-theoretic measures\n",
    "- **Range depends on categories** - not normalized across different category counts\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Simpson's Diversity Index**\n",
    "\n",
    "### **Strengths:**\n",
    "- **Ecological interpretation** - probability two samples are different species\n",
    "- **Business intuition** - easily explained to stakeholders\n",
    "- **Robust to sample size** - less affected by small samples than Shannon\n",
    "- **Three variants** - D, 1-D, 1/D for different perspectives\n",
    "\n",
    "### **When to Use:**\n",
    "- **Business presentations** - easy to explain to non-technical audiences\n",
    "- **Customer diversity** - \"probability two customers are from different segments\"\n",
    "- **Market analysis** - understanding customer base composition\n",
    "- **Ecological analogies** - when diversity concepts apply naturally\n",
    "\n",
    "### **Variant Selection:**\n",
    "```python\n",
    "# Simpson's D: Use when you want dominance probability\n",
    "# Simpson's 1-D: Use when you want diversity probability  \n",
    "# Simpson's 1/D: Use when you want \"effective number of categories\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Kolmogorov Complexity (Compression-based)**\n",
    "\n",
    "### **Strengths:**\n",
    "- **Universal applicability** - works with any data type\n",
    "- **Pattern detection** - finds algorithmic structure\n",
    "- **No assumptions** - doesn't assume probability distributions\n",
    "- **Practical approximation** - compression gives useful estimates\n",
    "\n",
    "### **When to Use:**\n",
    "- **Data quality assessment** - detecting artificial patterns\n",
    "- **Anomaly detection** - unusual compression ratios indicate problems\n",
    "- **Cross-domain comparison** - comparing different data types\n",
    "- **Storage optimization** - understanding compression potential\n",
    "\n",
    "### **Limitations:**\n",
    "- **Method dependent** - results vary by compression algorithm\n",
    "- **Overhead sensitive** - small datasets affected by serialization\n",
    "- **Approximation only** - not true Kolmogorov complexity\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Mutual Information**\n",
    "\n",
    "### **Strengths:**\n",
    "- **Relationship detection** - measures dependence between variables\n",
    "- **Non-linear relationships** - captures complex associations\n",
    "- **Information-theoretic foundation** - principled approach\n",
    "- **Symmetric measure** - I(X,Y) = I(Y,X)\n",
    "\n",
    "### **When to Use:**\n",
    "- **Feature selection** - identifying redundant variables\n",
    "- **Dependency analysis** - understanding variable relationships\n",
    "- **Causal inference** - preliminary relationship screening\n",
    "- **Dimensionality reduction** - finding independent components\n",
    "\n",
    "### **Data Type Considerations:**\n",
    "```python\n",
    "# Categorical-Categorical: Direct application\n",
    "# Numerical-Numerical: Requires binning or KDE\n",
    "# Mixed types: Discretize continuous variables first\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Normalized Compression Distance (NCD)**\n",
    "\n",
    "### **Strengths:**\n",
    "- **Universal similarity** - works across all data types\n",
    "- **No feature engineering** - discovers patterns automatically\n",
    "- **Robust comparison** - normalized for different data sizes\n",
    "- **Practical implementation** - uses standard compression algorithms\n",
    "\n",
    "### **When to Use:**\n",
    "- **Cross-type comparison** - comparing categorical vs numerical\n",
    "- **Similarity assessment** - understanding data relationships\n",
    "- **Clustering preprocessing** - distance metric for algorithms\n",
    "- **Data validation** - detecting duplicate or similar datasets\n",
    "\n",
    "---\n",
    "\n",
    "## **Decision Framework: Which Method to Choose**\n",
    "\n",
    "### **Start Here (Always):**\n",
    "1. **Shannon Entropy** - your baseline measurement\n",
    "2. **Gini Impurity** - quick computational check\n",
    "\n",
    "### **Then Choose Based on Goal:**\n",
    "\n",
    "#### **For General Analysis:**\n",
    "```python\n",
    "if goal == \"basic_diversity_assessment\":\n",
    "    use_shannon_entropy()\n",
    "elif goal == \"machine_learning_preprocessing\":\n",
    "    use_gini_impurity()\n",
    "elif goal == \"business_presentation\":\n",
    "    use_simpsons_diversity()\n",
    "```\n",
    "\n",
    "#### **For Specific Sensitivities:**\n",
    "```python\n",
    "if focus == \"rare_events\":\n",
    "    use_renyi_entropy(alpha < 1)\n",
    "elif focus == \"dominant_patterns\":\n",
    "    use_renyi_entropy(alpha > 1)\n",
    "elif focus == \"balanced_view\":\n",
    "    use_shannon_entropy()\n",
    "```\n",
    "\n",
    "#### **For Relationship Analysis:**\n",
    "```python\n",
    "if analyzing == \"variable_relationships\":\n",
    "    use_mutual_information()\n",
    "elif analyzing == \"data_similarity\":\n",
    "    use_ncd()\n",
    "elif analyzing == \"pattern_detection\":\n",
    "    use_kolmogorov_complexity()\n",
    "```\n",
    "\n",
    "### **Data Type Considerations:**\n",
    "\n",
    "#### **Categorical Data:**\n",
    "- **Primary**: Shannon Entropy, Gini Impurity\n",
    "- **Secondary**: Simpson's Diversity, Rényi Entropy\n",
    "- **Avoid**: Direct Kolmogorov (use string representation)\n",
    "\n",
    "#### **Numerical Data:**\n",
    "- **Primary**: Bin first, then apply categorical methods\n",
    "- **Secondary**: Kolmogorov Complexity (direct application)\n",
    "- **Binning**: Use Sturges, Scott, or Freedman-Diaconis rules\n",
    "\n",
    "#### **Mixed Data:**\n",
    "- **Primary**: Mutual Information, NCD\n",
    "- **Secondary**: Convert to common format first\n",
    "- **Avoid**: Direct comparison without preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Workflow Recommendation**\n",
    "\n",
    "### **Phase 1: Foundation (Always Do This)**\n",
    "```python\n",
    "# 1. Calculate Shannon entropy (baseline)\n",
    "shannon = entropy(probabilities, base=2)\n",
    "\n",
    "# 2. Calculate Gini impurity (computational check)  \n",
    "gini = 1 - sum(probabilities**2)\n",
    "\n",
    "# 3. Calculate Simpson's diversity (business interpretation)\n",
    "simpsons = 1 - sum(p**2 for p in probabilities)\n",
    "```\n",
    "\n",
    "### **Phase 2: Specific Analysis (Choose Based on Need)**\n",
    "```python\n",
    "# For sensitivity analysis:\n",
    "renyi_conservative = renyi_entropy(probabilities, alpha=2)\n",
    "renyi_liberal = renyi_entropy(probabilities, alpha=0.5)\n",
    "\n",
    "# For relationship analysis:\n",
    "mi_score = mutual_info_score(var1, var2)\n",
    "ncd_score = normalized_compression_distance(var1, var2)\n",
    "\n",
    "# For pattern detection:\n",
    "kolmogorov_approx = compression_ratio(data)\n",
    "```\n",
    "\n",
    "### **Phase 3: Interpretation (Context-Dependent)**\n",
    "```python\n",
    "# Business context: Use Simpson's for stakeholder communication\n",
    "# Technical context: Use Shannon for peer review\n",
    "# Risk context: Use Rényi with appropriate alpha\n",
    "# Quality context: Use Kolmogorov for pattern detection\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Selection Principles**\n",
    "\n",
    "### **1. Purpose-Driven Selection:**\n",
    "- **Exploration**: Shannon Entropy\n",
    "- **Optimization**: Gini Impurity  \n",
    "- **Communication**: Simpson's Diversity\n",
    "- **Investigation**: Rényi Entropy with tuned α\n",
    "\n",
    "### **2. Data-Driven Selection:**\n",
    "- **Balanced data**: Shannon works best\n",
    "- **Imbalanced data**: Rényi with α > 1\n",
    "- **Rare events**: Rényi with α < 1\n",
    "- **Mixed types**: NCD or Mutual Information\n",
    "\n",
    "### **3. Context-Driven Selection:**\n",
    "- **Academic**: Shannon (most cited)\n",
    "- **Business**: Simpson's (most intuitive)\n",
    "- **ML/AI**: Gini (most practical)\n",
    "- **Research**: Rényi (most flexible)\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "## **Summary: Your Gender Analysis Example**\n",
    "\n",
    "Looking at your Gender column results:\n",
    "- **Shannon Entropy (0.9896)**: Excellent baseline - shows near-perfect balance\n",
    "- **Rényi α=2 (0.9794)**: Slightly lower - detects minor imbalance  \n",
    "- **Gini Impurity (0.4928)**: Confirms balance in ML-friendly format\n",
    "- **Simpson's 1/D (1.9813)**: Business-friendly - \"effectively 2 equal categories\"\n",
    "\n",
    "**Recommendation for your use case**: Start with Shannon for technical analysis, use Simpson's for business communication, and apply Rényi with different α values if you want to explore sensitivity to the slight gender imbalance detected.\n",
    "\n",
    "The key is to **always start with Shannon as your baseline**, then choose additional methods based on your specific analytical goals and audience needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a930a",
   "metadata": {},
   "source": [
    "## Mermaid flowchart that guides you through selecting the appropriate entropy method based on your analysis goals, data characteristics, and context.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[\"Start: Need Entropy Analysis<br/>for Categorical Data\"] --> B[\"Define Analysis Purpose\"]\n",
    "    \n",
    "    B --> C{\"What is your<br/>primary goal?\"}\n",
    "    \n",
    "    C -->|\"Exploration & Understanding\"| D[\"Exploratory Analysis Path\"]\n",
    "    C -->|\"Machine Learning\"| E[\"ML Optimization Path\"]\n",
    "    C -->|\"Business Communication\"| F[\"Stakeholder Communication Path\"]\n",
    "    C -->|\"Research & Investigation\"| G[\"Advanced Research Path\"]\n",
    "    \n",
    "    D --> D1[\"Always Start:<br/>Shannon Entropy<br/>(baseline measure)\"]\n",
    "    D1 --> D2{\"Data<br/>Characteristics?\"}\n",
    "    \n",
    "    D2 -->|\"Balanced Distribution\"| D3[\"✅ Shannon Entropy<br/>H = -Σ p(x) log₂(p(x))<br/><br/>Perfect for exploration\"]\n",
    "    D2 -->|\"Imbalanced Data\"| D4[\"Add Rényi Entropy<br/>α > 1 (focus on dominant)<br/>α < 1 (focus on rare)\"]\n",
    "    D2 -->|\"Unknown Distribution\"| D5[\"Add Simpson's Diversity<br/>1/D = effective categories<br/>Easy interpretation\"]\n",
    "    \n",
    "    E --> E1[\"ML Preprocessing Focus\"]\n",
    "    E1 --> E2{\"Model Type?\"}\n",
    "    \n",
    "    E2 -->|\"Tree-based Models\"| E3[\"✅ Gini Impurity<br/>Gini = 1 - Σ p²<br/><br/>Fast, no logarithms\"]\n",
    "    E2 -->|\"Feature Selection\"| E4[\"Shannon Entropy +<br/>Mutual Information<br/>I(X;Y) for relationships\"]\n",
    "    E2 -->|\"Classification\"| E5[\"Gini Impurity<br/>(binary problems)<br/>Shannon (multi-class)\"]\n",
    "    \n",
    "    F --> F1[\"Business Stakeholder Focus\"]\n",
    "    F1 --> F2{\"Audience<br/>Technical Level?\"}\n",
    "    \n",
    "    F2 -->|\"Non-Technical\"| F3[\"✅ Simpson's Diversity<br/>1/D = 'effective categories'<br/><br/>Intuitive explanation\"]\n",
    "    F2 -->|\"Technical\"| F4[\"Shannon Entropy<br/>+ Simpson's for validation<br/>Both perspectives\"]\n",
    "    F2 -->|\"Executive\"| F5[\"Simpson's 1/D<br/>'Your data behaves like<br/>X equal categories'\"]\n",
    "    \n",
    "    G --> G1[\"Advanced Research Focus\"]\n",
    "    G1 --> G2{\"Research<br/>Question Type?\"}\n",
    "    \n",
    "    G2 -->|\"Sensitivity Analysis\"| G3[\"✅ Rényi Entropy Family<br/>Multiple α values<br/>α=0.5, 1, 2, 10\"]\n",
    "    G2 -->|\"Pattern Detection\"| G4[\"Kolmogorov Complexity<br/>Compression-based<br/>Universal patterns\"]\n",
    "    G2 -->|\"Variable Relationships\"| G5[\"Mutual Information +<br/>Normalized Compression Distance<br/>Cross-variable analysis\"]\n",
    "    \n",
    "    D3 --> H[\"Validation & Interpretation\"]\n",
    "    D4 --> H\n",
    "    D5 --> H\n",
    "    E3 --> H\n",
    "    E4 --> H\n",
    "    E5 --> H\n",
    "    F3 --> H\n",
    "    F4 --> H\n",
    "    F5 --> H\n",
    "    G3 --> H\n",
    "    G4 --> H\n",
    "    G5 --> H\n",
    "    \n",
    "    H --> I[\"Data Type Check\"]\n",
    "    I --> J{\"Data Type?\"}\n",
    "    \n",
    "    J -->|\"Pure Categorical\"| K[\"Direct Application<br/>Use selected method\"]\n",
    "    J -->|\"Numerical\"| L[\"Binning Required<br/>Use Freedman-Diaconis<br/>Then apply entropy method\"]\n",
    "    J -->|\"Mixed Types\"| M[\"Use Mutual Information<br/>or NCD for comparisons<br/>Bin numerical first\"]\n",
    "    \n",
    "    K --> N[\"Final Validation\"]\n",
    "    L --> N\n",
    "    M --> N\n",
    "    \n",
    "    N --> O[\"Check Results:<br/>• Reasonable values (0-1 range)<br/>• Consistent with expectations<br/>• Compare multiple methods\"]\n",
    "    \n",
    "    O --> P{\"Results<br/>Make Sense?\"}\n",
    "    \n",
    "    P -->|\"Yes\"| Q[\"✅ Proceed with Analysis<br/>Document method choice<br/>and rationale\"]\n",
    "    \n",
    "    P -->|\"No\"| R[\"⚠️ Troubleshoot:<br/>• Check data quality<br/>• Verify binning<br/>• Try alternative method\"]\n",
    "    \n",
    "    R --> S[\"Consider Context:<br/>• Domain knowledge<br/>• Business requirements<br/>• Statistical assumptions\"]\n",
    "    \n",
    "    S --> T{\"Need Different<br/>Approach?\"}\n",
    "    \n",
    "    T -->|\"Yes\"| C\n",
    "    T -->|\"No\"| Q\n",
    "    \n",
    "    style A fill:#1565c0,stroke:#0d47a1,stroke-width:3px,color:#ffffff\n",
    "    style Q fill:#2e7d32,stroke:#1b5e20,stroke-width:3px,color:#ffffff\n",
    "    style D3 fill:#ef6c00,stroke:#e65100,stroke-width:2px,color:#ffffff\n",
    "    style E3 fill:#ef6c00,stroke:#e65100,stroke-width:2px,color:#ffffff\n",
    "    style F3 fill:#ef6c00,stroke:#e65100,stroke-width:2px,color:#ffffff\n",
    "    style G3 fill:#ef6c00,stroke:#e65100,stroke-width:2px,color:#ffffff\n",
    "    style R fill:#c62828,stroke:#b71c1c,stroke-width:2px,color:#ffffff\n",
    "    style C fill:#5e35b1,stroke:#4527a0,stroke-width:2px,color:#ffffff\n",
    "    style D2 fill:#5e35b1,stroke:#4527a0,stroke-width:2px,color:#ffffff\n",
    "    style E2 fill:#5e35b1,stroke:#4527a0,stroke-width:2px,color:#ffffff\n",
    "    style F2 fill:#5e35b1,stroke:#4527a0,stroke-width:2px,color:#ffffff\n",
    "    style G2 fill:#5e35b1,stroke:#4527a0,stroke-width:2px,color:#ffffff\n",
    "    style J fill:#5e35b1,stroke:#4527a0,stroke-width:2px,color:#ffffff\n",
    "    style P fill:#5e35b1,stroke:#4527a0,stroke-width:2px,color:#ffffff\n",
    "    style T fill:#5e35b1,stroke:#4527a0,stroke-width:2px,color:#ffffff\n",
    "```\n",
    "\n",
    "I've created a comprehensive flowchart for entropy method selection based on the principles from your notebook. Here's how to use this decision tree:\n",
    "\n",
    "## **Key Decision Pathways:**\n",
    "\n",
    "### **1. Purpose-Driven Selection (Main Branches)**\n",
    "- **Exploration**: Shannon Entropy as baseline, add others based on data characteristics\n",
    "- **Machine Learning**: Gini for trees, Shannon for general ML, Mutual Information for feature selection\n",
    "- **Business Communication**: Simpson's Diversity for intuitive explanation\n",
    "- **Research**: Rényi family for sensitivity analysis, Kolmogorov for patterns\n",
    "\n",
    "### **2. Context-Specific Recommendations**\n",
    "\n",
    "#### **Exploratory Analysis Path:**\n",
    "```python\n",
    "# Always start with Shannon, then add based on data:\n",
    "if balanced_data:\n",
    "    primary = \"Shannon Entropy\"\n",
    "elif imbalanced_data:\n",
    "    primary = \"Shannon + Rényi (α>1 for dominant, α<1 for rare)\"\n",
    "else:\n",
    "    primary = \"Shannon + Simpson's for interpretation\"\n",
    "```\n",
    "\n",
    "#### **ML Optimization Path:**\n",
    "```python\n",
    "# Choose based on model type:\n",
    "if tree_based_model:\n",
    "    use = \"Gini Impurity\"  # Fast, no logarithms\n",
    "elif feature_selection:\n",
    "    use = \"Shannon + Mutual Information\"\n",
    "elif classification:\n",
    "    use = \"Gini (binary) or Shannon (multi-class)\"\n",
    "```\n",
    "\n",
    "#### **Business Communication Path:**\n",
    "```python\n",
    "# Choose based on audience:\n",
    "if non_technical_audience:\n",
    "    use = \"Simpson's 1/D\"  # \"Effective number of categories\"\n",
    "elif technical_audience:\n",
    "    use = \"Shannon + Simpson's validation\"\n",
    "elif executive_summary:\n",
    "    use = \"Simpson's with business interpretation\"\n",
    "```\n",
    "\n",
    "#### **Research Path:**\n",
    "```python\n",
    "# Choose based on research question:\n",
    "if sensitivity_analysis:\n",
    "    use = \"Rényi family (multiple α values)\"\n",
    "elif pattern_detection:\n",
    "    use = \"Kolmogorov Complexity\"\n",
    "elif variable_relationships:\n",
    "    use = \"Mutual Information + NCD\"\n",
    "```\n",
    "\n",
    "### **3. Data Type Considerations**\n",
    "- **Pure Categorical**: Direct application of selected method\n",
    "- **Numerical**: Bin first (use Freedman-Diaconis), then apply entropy\n",
    "- **Mixed Types**: Use Mutual Information or NCD for cross-type comparisons\n",
    "\n",
    "### **4. Validation Framework**\n",
    "Always validate your results:\n",
    "- **Range check**: Values should be in expected ranges (0-1 for most measures)\n",
    "- **Consistency**: Results should align with data characteristics\n",
    "- **Multiple methods**: Compare different approaches for robustness\n",
    "\n",
    "## **Implementation Code for the Flowchart:**\n",
    "\n",
    "```python\n",
    "def select_entropy_method(purpose, data_type, audience=None, model_type=None):\n",
    "    \"\"\"\n",
    "    Entropy method selection following the flowchart logic\n",
    "    \"\"\"\n",
    "    \n",
    "    if purpose == \"exploration\":\n",
    "        base_method = \"shannon\"\n",
    "        \n",
    "        # Check data characteristics\n",
    "        if is_balanced(data):\n",
    "            return [base_method]\n",
    "        elif is_imbalanced(data):\n",
    "            return [base_method, \"renyi_alpha_2\", \"renyi_alpha_0.5\"]\n",
    "        else:\n",
    "            return [base_method, \"simpsons\"]\n",
    "    \n",
    "    elif purpose == \"machine_learning\":\n",
    "        if model_type == \"tree_based\":\n",
    "            return [\"gini\"]\n",
    "        elif model_type == \"feature_selection\":\n",
    "            return [\"shannon\", \"mutual_information\"]\n",
    "        else:\n",
    "            return [\"gini\", \"shannon\"]  # Both for comparison\n",
    "    \n",
    "    elif purpose == \"business_communication\":\n",
    "        if audience == \"non_technical\":\n",
    "            return [\"simpsons_reciprocal\"]\n",
    "        elif audience == \"technical\":\n",
    "            return [\"shannon\", \"simpsons\"]\n",
    "        else:\n",
    "            return [\"simpsons_reciprocal\"]\n",
    "    \n",
    "    elif purpose == \"research\":\n",
    "        return [\"renyi_family\", \"kolmogorov\", \"mutual_information\"]\n",
    "    \n",
    "    else:\n",
    "        return [\"shannon\"]  # Default baseline\n",
    "\n",
    "def validate_entropy_results(results, data_characteristics):\n",
    "    \"\"\"\n",
    "    Validate entropy analysis results\n",
    "    \"\"\"\n",
    "    validation = {\n",
    "        'range_check': all(0 <= r <= max_expected_value(method) for r, method in results),\n",
    "        'consistency': check_consistency_with_data(results, data_characteristics),\n",
    "        'interpretability': check_business_sense(results)\n",
    "    }\n",
    "    \n",
    "    return validation\n",
    "```\n",
    "\n",
    "## **Quick Reference Guide:**\n",
    "\n",
    "### **Most Common Use Cases:**\n",
    "1. **General Analysis**: Start with Shannon Entropy\n",
    "2. **ML Preprocessing**: Use Gini Impurity for speed\n",
    "3. **Business Reports**: Use Simpson's 1/D for clarity\n",
    "4. **Imbalanced Data**: Add Rényi with α > 1\n",
    "5. **Rare Events**: Use Rényi with α < 1\n",
    "6. **Variable Relationships**: Use Mutual Information\n",
    "\n",
    "### **Decision Shortcuts:**\n",
    "- **When in doubt**: Shannon Entropy (universal baseline)\n",
    "- **Need speed**: Gini Impurity (no logarithms)\n",
    "- **Need explanation**: Simpson's Diversity (intuitive)\n",
    "- **Need flexibility**: Rényi Entropy (tunable sensitivity)\n",
    "\n",
    "This flowchart ensures you select the most appropriate entropy method based on your specific context, goals, and constraints while maintaining scientific rigor and practical applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9039f1",
   "metadata": {},
   "source": [
    "What are the different ways of exploring entropy, randomness, information content etc. for a column? Give links to documentation and helpful blogs to understand the model being used, deeper and more thoroughly. \n",
    "For each test, explain how to decide different values of parameters based on the data being analysed, wherever applicable. \n",
    "Also explain what are the different ways of interpreting the output of these tests?  Does the nature of the data in the column ie. numerical vs. categorical, have an impact on the tests and the interpretation of these tests?\n",
    "\n",
    "Generate the complete output of the previous response in markdown format here for me to copy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
