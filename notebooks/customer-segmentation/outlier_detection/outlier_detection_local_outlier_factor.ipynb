{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a98bc8c",
   "metadata": {},
   "source": [
    "## üîç Local Outlier Factor (LOF): Density-Based Outlier Detection\n",
    "\n",
    "### üìã Code Breakdown\n",
    "```python\n",
    "# Local Outlier Factor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "outliers = lof.fit_predict(base_df[['Age']])\n",
    "print(outliers)\n",
    "```\n",
    "\n",
    "**Line-by-line explanation:**\n",
    "1. **Import LOF** from sklearn neighbors module\n",
    "2. **Create LOF instance** with 20 neighbors and 10% contamination expectation\n",
    "3. **Fit and predict** on Age column (returns -1 for outliers, 1 for normal)\n",
    "4. **Print binary classification** results\n",
    "\n",
    "### üìö Essential Documentation & Resources\n",
    "\n",
    "#### **Official Documentation:**\n",
    "- **[Scikit-learn LocalOutlierFactor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html)** - Official API reference\n",
    "- **[Scikit-learn Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html#local-outlier-factor)** - Comprehensive guide\n",
    "- **[Original Paper: \"LOF: Identifying Density-based Local Outliers\" by Breunig et al. (2000)](https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf)** - Foundational research paper\n",
    "\n",
    "#### **Helpful Blogs & Tutorials:**\n",
    "- **[Towards Data Science: Local Outlier Factor Explained](https://towardsdatascience.com/local-outlier-factor-for-anomaly-detection-cc0c770d2ebe)**\n",
    "- **[Machine Learning Mastery: LOF for Outlier Detection](https://machinelearningmastery.com/local-outlier-factor-for-outlier-detection/)**\n",
    "- **[Analytics Vidhya: Understanding LOF Algorithm](https://www.analyticsvidhya.com/blog/2021/01/anomaly-detection-using-local-outlier-factor-lof/)**\n",
    "\n",
    "#### **Advanced Resources:**\n",
    "- **[Comparative Study: LOF vs Other Methods](https://link.springer.com/article/10.1007/s10618-017-0519-2)**\n",
    "- **[LOF Improvements and Variants](https://ieeexplore.ieee.org/document/8594818)**\n",
    "- **[Density-Based Anomaly Detection Survey](https://www.sciencedirect.com/science/article/pii/S0167739X19306296)**\n",
    "\n",
    "### üîç How Local Outlier Factor Works\n",
    "\n",
    "#### **Core Algorithm Concept:**\n",
    "1. **Local Density Estimation**: Calculate local density around each point\n",
    "2. **Neighborhood Analysis**: Compare point's density with its k-nearest neighbors\n",
    "3. **Relative Density**: Points in sparse regions (low local density) are outliers\n",
    "4. **LOF Score**: Ratio of neighbor densities to point's own density\n",
    "\n",
    "#### **Mathematical Foundation:**\n",
    "\n",
    "```python\n",
    "# LOF Calculation Steps:\n",
    "\n",
    "# 1. k-distance: Distance to k-th nearest neighbor\n",
    "# k_distance(p) = distance from point p to its k-th nearest neighbor\n",
    "\n",
    "# 2. Reachability Distance: \n",
    "# reach_dist_k(p,q) = max(k_distance(q), distance(p,q))\n",
    "\n",
    "# 3. Local Reachability Density (LRD):\n",
    "# LRD_k(p) = 1 / (average reachability distance of p's k-neighbors)\n",
    "\n",
    "# 4. Local Outlier Factor:\n",
    "# LOF_k(p) = average(LRD_k(neighbors)) / LRD_k(p)\n",
    "\n",
    "# LOF ‚âà 1: Normal point (similar density to neighbors)\n",
    "# LOF > 1: Outlier (lower density than neighbors)\n",
    "# LOF < 1: Dense region center\n",
    "```\n",
    "\n",
    "#### **Visual Intuition:**\n",
    "- **Normal points**: Surrounded by similar density ‚Üí LOF ‚âà 1\n",
    "- **Global outliers**: Far from everything ‚Üí High LOF\n",
    "- **Local outliers**: In sparse region of dense cluster ‚Üí High LOF\n",
    "- **Cluster centers**: Higher density than surroundings ‚Üí LOF < 1\n",
    "\n",
    "### üìä Output Interpretation\n",
    "\n",
    "Your output will be an array like: `[1, 1, -1, 1, 1, -1, ...]`\n",
    "\n",
    "**Interpretation:**\n",
    "- **`1`**: Normal point (inlier)\n",
    "- **`-1`**: Outlier (anomaly)\n",
    "\n",
    "**Practical Usage:**\n",
    "```python\n",
    "# Get outlier indices and LOF scores\n",
    "outlier_indices = np.where(outliers == -1)[0]\n",
    "outlier_customers = base_df.iloc[outlier_indices]\n",
    "\n",
    "# Get LOF scores (confidence measure)\n",
    "lof_scores = lof.negative_outlier_factor_\n",
    "# More negative = more outlier-like\n",
    "# Closer to -1 = more normal\n",
    "\n",
    "# Convert to positive LOF scores (traditional interpretation)\n",
    "positive_lof_scores = -lof_scores\n",
    "\n",
    "# Practical analysis\n",
    "print(f\"Found {len(outlier_customers)} outliers out of {len(base_df)} customers\")\n",
    "print(f\"Outlier percentage: {len(outlier_customers)/len(base_df)*100:.1f}%\")\n",
    "\n",
    "# Show outliers with their LOF scores\n",
    "outlier_analysis = pd.DataFrame({\n",
    "    'Customer_Index': outlier_indices,\n",
    "    'Age': base_df.iloc[outlier_indices]['Age'].values,\n",
    "    'LOF_Score': positive_lof_scores[outlier_indices]\n",
    "})\n",
    "print(\"\\nOutliers with LOF Scores:\")\n",
    "print(outlier_analysis.sort_values('LOF_Score', ascending=False))\n",
    "```\n",
    "\n",
    "**LOF Score Interpretation:**\n",
    "- **LOF ‚âà 1.0**: Normal point, similar density to neighbors\n",
    "- **LOF = 1.2-1.5**: Mild outlier, somewhat isolated\n",
    "- **LOF = 1.5-2.0**: Moderate outlier, clearly isolated\n",
    "- **LOF > 2.0**: Strong outlier, very isolated\n",
    "\n",
    "### ‚öñÔ∏è LOF vs Other Outlier Detection Methods\n",
    "\n",
    "| **Method** | **Strengths** | **Weaknesses** | **Best Use Case** |\n",
    "|------------|---------------|----------------|-------------------|\n",
    "| **Standard Z-Score** | ‚úÖ Simple, fast<br/>‚úÖ Interpretable<br/>‚úÖ Global outliers | ‚ùå Assumes normality<br/>‚ùå Misses local outliers<br/>‚ùå Univariate only | Normally distributed, global outliers |\n",
    "| **Modified Z-Score** | ‚úÖ Robust to outliers<br/>‚úÖ No normality assumption<br/>‚úÖ Interpretable | ‚ùå Still global approach<br/>‚ùå Univariate only<br/>‚ùå Misses local patterns | Robust univariate outlier detection |\n",
    "| **Isolation Forest** | ‚úÖ Multivariate<br/>‚úÖ No assumptions<br/>‚úÖ Scalable<br/>‚úÖ Global patterns | ‚ùå Parameter sensitive<br/>‚ùå Poor with local outliers<br/>‚ùå Less interpretable | Large datasets, global anomalies |\n",
    "| **Local Outlier Factor** | ‚úÖ **Detects local outliers**<br/>‚úÖ **Density-aware**<br/>‚úÖ **Intuitive scores**<br/>‚úÖ **Handles clusters well**<br/>‚úÖ **No distribution assumptions** | ‚ùå **Sensitive to k parameter**<br/>‚ùå **Computationally expensive O(n¬≤)**<br/>‚ùå **Poor with high dimensions**<br/>‚ùå **Struggles with uniform density** | **Clustered data, local anomalies** |\n",
    "\n",
    "### üéØ Detailed Comparison\n",
    "\n",
    "#### **LOF Unique Strengths:**\n",
    "1. **Local Context Awareness**: Can find outliers within clusters\n",
    "2. **Density-Based Logic**: Intuitive concept of \"sparse neighborhood\"\n",
    "3. **Interpretable Scores**: LOF values have clear meaning\n",
    "4. **No Global Assumptions**: Works with multiple clusters of different densities\n",
    "5. **Robust to Noise**: Local approach reduces impact of distant noise\n",
    "\n",
    "#### **LOF Weaknesses:**\n",
    "1. **k-Parameter Sensitivity**: Results vary significantly with neighbor count\n",
    "2. **Computational Complexity**: O(n¬≤) for distance calculations\n",
    "3. **Curse of Dimensionality**: Performance degrades with many features\n",
    "4. **Uniform Data Issues**: Struggles when data has uniform density\n",
    "5. **Border Effects**: Points near data boundaries may be misclassified\n",
    "\n",
    "### üöÄ Advanced Usage Tips\n",
    "\n",
    "```python\n",
    "# Better implementation with parameter tuning\n",
    "def optimize_lof_parameters(X, k_range=None, contamination_range=None):\n",
    "    \"\"\"Find optimal LOF parameters\"\"\"\n",
    "    \n",
    "    if k_range is None:\n",
    "        k_range = range(5, min(50, len(X)//4), 5)\n",
    "    if contamination_range is None:\n",
    "        contamination_range = [0.05, 0.1, 0.15, 0.2]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        for contamination in contamination_range:\n",
    "            lof = LocalOutlierFactor(\n",
    "                n_neighbors=k,\n",
    "                contamination=contamination,\n",
    "                metric='euclidean'\n",
    "            )\n",
    "            \n",
    "            predictions = lof.fit_predict(X)\n",
    "            scores = -lof.negative_outlier_factor_\n",
    "            \n",
    "            # Calculate metrics\n",
    "            outlier_count = np.sum(predictions == -1)\n",
    "            score_variance = np.var(scores)\n",
    "            mean_lof_outliers = np.mean(scores[predictions == -1])\n",
    "            \n",
    "            results.append({\n",
    "                'k': k,\n",
    "                'contamination': contamination,\n",
    "                'outlier_count': outlier_count,\n",
    "                'score_variance': score_variance,\n",
    "                'mean_outlier_lof': mean_lof_outliers,\n",
    "                'outlier_ratio': outlier_count / len(X)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Optimize for your data\n",
    "optimization_results = optimize_lof_parameters(base_df[['Age']])\n",
    "print(\"Optimization Results:\")\n",
    "print(optimization_results.head())\n",
    "\n",
    "# Advanced LOF with better parameters\n",
    "lof_optimized = LocalOutlierFactor(\n",
    "    n_neighbors=15,          # Often good starting point\n",
    "    contamination=0.1,       # Based on domain knowledge\n",
    "    metric='euclidean',      # For numerical data\n",
    "    p=2,                     # Euclidean distance parameter\n",
    "    novelty=False           # For outlier detection (not novelty)\n",
    ")\n",
    "```\n",
    "\n",
    "### üìè Parameter Selection Guidelines\n",
    "\n",
    "#### **`n_neighbors` (k) Selection:**\n",
    "```python\n",
    "def suggest_k_parameter(n_samples, n_features):\n",
    "    \"\"\"Suggest k parameter based on data characteristics\"\"\"\n",
    "    \n",
    "    if n_samples < 50:\n",
    "        return max(3, n_samples // 10)\n",
    "    elif n_samples < 200:\n",
    "        return max(5, n_samples // 20)\n",
    "    elif n_samples < 1000:\n",
    "        return max(10, n_samples // 50)\n",
    "    else:\n",
    "        return max(20, min(50, n_samples // 100))\n",
    "\n",
    "suggested_k = suggest_k_parameter(len(base_df), 1)\n",
    "print(f\"Suggested k for your data: {suggested_k}\")\n",
    "```\n",
    "\n",
    "**k Parameter Rules:**\n",
    "- **Too small** (k<5): Sensitive to noise, unstable\n",
    "- **Too large** (k>n/4): Becomes global method, loses local sensitivity\n",
    "- **Sweet spot**: k = 10-20 for most datasets\n",
    "- **Rule of thumb**: k ‚âà ‚àön for balanced performance\n",
    "\n",
    "#### **Distance Metrics:**\n",
    "```python\n",
    "# Different metrics for different data types\n",
    "lof_euclidean = LocalOutlierFactor(metric='euclidean')    # Numerical data\n",
    "lof_manhattan = LocalOutlierFactor(metric='manhattan')    # When features have different scales\n",
    "lof_cosine = LocalOutlierFactor(metric='cosine')         # High-dimensional, sparse data\n",
    "```\n",
    "\n",
    "### üéØ When to Use LOF\n",
    "\n",
    "**‚úÖ Use LOF when:**\n",
    "- **Clustered data** with potential local outliers\n",
    "- **Different cluster densities** in your dataset\n",
    "- **Need interpretable** outlier scores\n",
    "- **Local context matters** more than global patterns\n",
    "- **Moderate dataset size** (<10,000 points)\n",
    "\n",
    "**‚ùå Don't use LOF when:**\n",
    "- **Very large datasets** (>100,000 points) - too slow\n",
    "- **High-dimensional data** (>20 features) - curse of dimensionality\n",
    "- **Uniform density** throughout dataset\n",
    "- **Need real-time detection** - too computationally expensive\n",
    "- **Only global outliers** expected\n",
    "\n",
    "### üèÜ Recommendation for Your Customer Segmentation\n",
    "\n",
    "For customer segmentation analysis, **LOF is excellent** because:\n",
    "\n",
    "1. **Customer clusters**: Different age groups may have different densities\n",
    "2. **Local anomalies**: Unusual customers within age groups\n",
    "3. **Business interpretability**: LOF scores are intuitive\n",
    "4. **Small dataset**: 200 customers is perfect for LOF\n",
    "\n",
    "**Optimal implementation for your case:**\n",
    "```python\n",
    "# Recommended LOF setup for customer age analysis\n",
    "lof_customer = LocalOutlierFactor(\n",
    "    n_neighbors=15,          # Good for 200 customers\n",
    "    contamination=0.08,      # Conservative for customer data\n",
    "    metric='euclidean',      # Appropriate for age\n",
    "    algorithm='auto'         # Let sklearn choose best algorithm\n",
    ")\n",
    "\n",
    "# For multivariate analysis\n",
    "lof_multivariate = LocalOutlierFactor(\n",
    "    n_neighbors=20,          # Slightly higher for multiple features\n",
    "    contamination=0.05,      # More conservative for multivariate\n",
    "    metric='euclidean'       # Standard for numerical features\n",
    ")\n",
    "\n",
    "# Apply and analyze\n",
    "outliers = lof_customer.fit_predict(base_df[['Age']])\n",
    "lof_scores = -lof_customer.negative_outlier_factor_\n",
    "\n",
    "# Create detailed analysis\n",
    "outlier_analysis = pd.DataFrame({\n",
    "    'Customer_ID': base_df.index,\n",
    "    'Age': base_df['Age'],\n",
    "    'LOF_Score': lof_scores,\n",
    "    'Is_Outlier': outliers == -1\n",
    "})\n",
    "\n",
    "# Sort by LOF score to see most anomalous customers\n",
    "print(\"Most anomalous customers:\")\n",
    "print(outlier_analysis.sort_values('LOF_Score', ascending=False).head(10))\n",
    "```\n",
    "\n",
    "### üéØ Summary: LOF vs Your Other Methods\n",
    "\n",
    "**Perfect Complementary Approach:**\n",
    "1. **Z-Score**: Global statistical outliers in age\n",
    "2. **Modified Z-Score**: Robust global outliers  \n",
    "3. **Isolation Forest**: Multivariate global anomalies\n",
    "4. **LOF**: Local density-based outliers within age clusters\n",
    "\n",
    "**Use LOF specifically when you want to find:**\n",
    "- Customers with unusual ages **within their peer group**\n",
    "- Local anomalies that global methods miss\n",
    "- Interpretable anomaly scores for business decisions\n",
    "\n",
    "LOF is particularly powerful for customer segmentation because it can identify customers who are outliers **relative to their local neighborhood**, which often has more business relevance than global outliers! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da12fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1\n",
      " -1 -1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1 -1 -1  1  1 -1  1  1  1  1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1 -1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "# Local Outlier Factor\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "outliers = lof.fit_predict(base_df[['Age']])\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff84eb",
   "metadata": {},
   "source": [
    "## üéØ Local Outlier Factor Parameter Tuning: Comprehensive Guide\n",
    "\n",
    "### üìä Core Parameters and Data-Driven Selection\n",
    "\n",
    "#### **1. `n_neighbors` (k) - The Most Critical Parameter**\n",
    "\n",
    "**What it controls:** Number of neighbors used for density estimation\n",
    "\n",
    "**Data-driven selection methods:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_k_parameter_impact(X, k_range=None, contamination=0.1):\n",
    "    \"\"\"\n",
    "    Analyze the impact of k parameter on LOF results\n",
    "    \"\"\"\n",
    "    \n",
    "    if k_range is None:\n",
    "        max_k = min(50, len(X) // 4)\n",
    "        k_range = range(3, max_k + 1, 2)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Fit LOF with current k\n",
    "        lof = LocalOutlierFactor(n_neighbors=k, contamination=contamination)\n",
    "        predictions = lof.fit_predict(X)\n",
    "        lof_scores = -lof.negative_outlier_factor_\n",
    "        \n",
    "        # Calculate stability metrics\n",
    "        outlier_count = np.sum(predictions == -1)\n",
    "        mean_lof_score = np.mean(lof_scores)\n",
    "        std_lof_score = np.std(lof_scores)\n",
    "        \n",
    "        # Calculate score separation (how well outliers are separated)\n",
    "        outlier_scores = lof_scores[predictions == -1]\n",
    "        normal_scores = lof_scores[predictions == 1]\n",
    "        \n",
    "        if len(outlier_scores) > 0 and len(normal_scores) > 0:\n",
    "            score_separation = np.mean(outlier_scores) - np.mean(normal_scores)\n",
    "            silhouette = silhouette_score(X, predictions)\n",
    "        else:\n",
    "            score_separation = 0\n",
    "            silhouette = -1\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'outlier_count': outlier_count,\n",
    "            'outlier_ratio': outlier_count / len(X),\n",
    "            'mean_lof_score': mean_lof_score,\n",
    "            'std_lof_score': std_lof_score,\n",
    "            'score_separation': score_separation,\n",
    "            'silhouette_score': silhouette,\n",
    "            'cv_score': std_lof_score / mean_lof_score if mean_lof_score > 0 else np.inf\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    axes[0,0].plot(df_results['k'], df_results['outlier_ratio'], 'bo-')\n",
    "    axes[0,0].set_xlabel('k (number of neighbors)')\n",
    "    axes[0,0].set_ylabel('Outlier Ratio')\n",
    "    axes[0,0].set_title('Outlier Ratio vs k')\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    axes[0,1].plot(df_results['k'], df_results['score_separation'], 'ro-')\n",
    "    axes[0,1].set_xlabel('k (number of neighbors)')\n",
    "    axes[0,1].set_ylabel('Score Separation')\n",
    "    axes[0,1].set_title('Score Separation vs k')\n",
    "    axes[0,1].grid(True)\n",
    "    \n",
    "    axes[1,0].plot(df_results['k'], df_results['silhouette_score'], 'go-')\n",
    "    axes[1,0].set_xlabel('k (number of neighbors)')\n",
    "    axes[1,0].set_ylabel('Silhouette Score')\n",
    "    axes[1,0].set_title('Silhouette Score vs k')\n",
    "    axes[1,0].grid(True)\n",
    "    \n",
    "    axes[1,1].plot(df_results['k'], df_results['cv_score'], 'mo-')\n",
    "    axes[1,1].set_xlabel('k (number of neighbors)')\n",
    "    axes[1,1].set_ylabel('Coefficient of Variation')\n",
    "    axes[1,1].set_title('Stability (CV) vs k')\n",
    "    axes[1,1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Apply to your data\n",
    "k_analysis = analyze_k_parameter_impact(base_df[['Age']])\n",
    "print(\"K Parameter Analysis Results:\")\n",
    "print(k_analysis.head(10))\n",
    "```\n",
    "\n",
    "**Heuristic Rules for k Selection:**\n",
    "\n",
    "```python\n",
    "def suggest_optimal_k(X, domain_knowledge=None):\n",
    "    \"\"\"\n",
    "    Suggest optimal k based on multiple criteria\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Rule 1: Statistical rule based on sample size\n",
    "    statistical_k = max(5, min(int(np.sqrt(n_samples)), 50))\n",
    "    \n",
    "    # Rule 2: Density-based rule\n",
    "    # Estimate local neighborhood size based on data spread\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(20, n_samples-1)).fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "    avg_distance = np.mean(distances[:, -1])  # Average distance to 20th neighbor\n",
    "    \n",
    "    # Rule 3: Elbow method for k selection\n",
    "    k_range = range(3, min(51, n_samples//3))\n",
    "    stability_scores = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Run LOF multiple times with different random states\n",
    "        scores_list = []\n",
    "        for seed in range(3):\n",
    "            np.random.seed(seed)\n",
    "            sample_indices = np.random.choice(len(X), min(len(X), 100), replace=False)\n",
    "            X_sample = X.iloc[sample_indices] if hasattr(X, 'iloc') else X[sample_indices]\n",
    "            \n",
    "            lof = LocalOutlierFactor(n_neighbors=k, contamination=0.1)\n",
    "            lof_scores = -lof.fit(X_sample).negative_outlier_factor_\n",
    "            scores_list.append(lof_scores)\n",
    "        \n",
    "        # Calculate stability (coefficient of variation across runs)\n",
    "        mean_scores = np.mean(scores_list, axis=0)\n",
    "        std_scores = np.std(scores_list, axis=0)\n",
    "        cv = np.mean(std_scores / (mean_scores + 1e-8))\n",
    "        stability_scores.append(cv)\n",
    "    \n",
    "    # Find elbow point\n",
    "    if len(stability_scores) > 1:\n",
    "        # Simple elbow detection\n",
    "        diffs = np.diff(stability_scores)\n",
    "        elbow_k = k_range[np.argmin(diffs)] if len(diffs) > 0 else statistical_k\n",
    "    else:\n",
    "        elbow_k = statistical_k\n",
    "    \n",
    "    # Domain-specific adjustments\n",
    "    if domain_knowledge:\n",
    "        if 'cluster_expected' in domain_knowledge and domain_knowledge['cluster_expected']:\n",
    "            # For clustered data, use smaller k to capture local structure\n",
    "            domain_k = max(5, min(statistical_k, 15))\n",
    "        elif 'uniform_density' in domain_knowledge and domain_knowledge['uniform_density']:\n",
    "            # For uniform density, use larger k\n",
    "            domain_k = max(statistical_k, 20)\n",
    "        else:\n",
    "            domain_k = statistical_k\n",
    "    else:\n",
    "        domain_k = statistical_k\n",
    "    \n",
    "    # Final recommendation (conservative approach)\n",
    "    recommendations = [statistical_k, elbow_k, domain_k]\n",
    "    final_k = int(np.median(recommendations))\n",
    "    \n",
    "    print(f\"Sample size: {n_samples}\")\n",
    "    print(f\"Statistical k recommendation: {statistical_k}\")\n",
    "    print(f\"Elbow method k: {elbow_k}\")\n",
    "    print(f\"Domain-adjusted k: {domain_k}\")\n",
    "    print(f\"Final k recommendation: {final_k}\")\n",
    "    \n",
    "    return final_k, {\n",
    "        'statistical_k': statistical_k,\n",
    "        'elbow_k': elbow_k,\n",
    "        'domain_k': domain_k,\n",
    "        'stability_scores': stability_scores\n",
    "    }\n",
    "\n",
    "# Apply to your customer data\n",
    "domain_knowledge = {\n",
    "    'cluster_expected': True,  # Customer age groups likely form clusters\n",
    "    'uniform_density': False\n",
    "}\n",
    "\n",
    "optimal_k, k_details = suggest_optimal_k(base_df[['Age']], domain_knowledge)\n",
    "```\n",
    "\n",
    "#### **2. `contamination` - Expected Outlier Proportion**\n",
    "\n",
    "```python\n",
    "def estimate_contamination_for_lof(X, methods=['iqr', 'zscore', 'modified_zscore']):\n",
    "    \"\"\"\n",
    "    Estimate contamination using multiple statistical methods\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    contamination_estimates = {}\n",
    "    \n",
    "    for col_idx, col_name in enumerate(X.columns if hasattr(X, 'columns') else range(X.shape[1])):\n",
    "        if hasattr(X, 'iloc'):\n",
    "            data = X.iloc[:, col_idx]\n",
    "        else:\n",
    "            data = X[:, col_idx]\n",
    "        \n",
    "        # Method 1: IQR\n",
    "        if 'iqr' in methods:\n",
    "            Q1 = np.percentile(data, 25)\n",
    "            Q3 = np.percentile(data, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            iqr_outliers = np.sum((data < lower_bound) | (data > upper_bound))\n",
    "            contamination_estimates[f'{col_name}_iqr'] = iqr_outliers / len(data)\n",
    "        \n",
    "        # Method 2: Z-score\n",
    "        if 'zscore' in methods:\n",
    "            z_scores = np.abs(stats.zscore(data))\n",
    "            zscore_outliers = np.sum(z_scores > 3)\n",
    "            contamination_estimates[f'{col_name}_zscore'] = zscore_outliers / len(data)\n",
    "        \n",
    "        # Method 3: Modified Z-score\n",
    "        if 'modified_zscore' in methods:\n",
    "            median = np.median(data)\n",
    "            mad = np.median(np.abs(data - median))\n",
    "            if mad > 0:\n",
    "                modified_z_scores = 0.6745 * (data - median) / mad\n",
    "                mod_zscore_outliers = np.sum(np.abs(modified_z_scores) > 3.5)\n",
    "                contamination_estimates[f'{col_name}_modified_zscore'] = mod_zscore_outliers / len(data)\n",
    "    \n",
    "    # Calculate conservative estimate\n",
    "    estimates = list(contamination_estimates.values())\n",
    "    conservative_estimate = min(estimates) if estimates else 0.1\n",
    "    liberal_estimate = max(estimates) if estimates else 0.1\n",
    "    median_estimate = np.median(estimates) if estimates else 0.1\n",
    "    \n",
    "    print(\"Contamination Estimates:\")\n",
    "    for method, estimate in contamination_estimates.items():\n",
    "        print(f\"{method}: {estimate:.3f}\")\n",
    "    \n",
    "    print(f\"\\nConservative estimate: {conservative_estimate:.3f}\")\n",
    "    print(f\"Liberal estimate: {liberal_estimate:.3f}\")\n",
    "    print(f\"Median estimate: {median_estimate:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'conservative': conservative_estimate,\n",
    "        'liberal': liberal_estimate,\n",
    "        'median': median_estimate,\n",
    "        'all_estimates': contamination_estimates\n",
    "    }\n",
    "\n",
    "contamination_analysis = estimate_contamination_for_lof(base_df[['Age']])\n",
    "```\n",
    "\n",
    "#### **3. Distance Metric Selection**\n",
    "\n",
    "```python\n",
    "def select_optimal_metric(X, k=20, contamination=0.1):\n",
    "    \"\"\"\n",
    "    Test different distance metrics and select the best one\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
    "    results = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        try:\n",
    "            lof = LocalOutlierFactor(\n",
    "                n_neighbors=k,\n",
    "                contamination=contamination,\n",
    "                metric=metric\n",
    "            )\n",
    "            \n",
    "            predictions = lof.fit_predict(X)\n",
    "            lof_scores = -lof.negative_outlier_factor_\n",
    "            \n",
    "            # Calculate quality metrics\n",
    "            outlier_count = np.sum(predictions == -1)\n",
    "            if outlier_count > 0 and outlier_count < len(X):\n",
    "                silhouette = silhouette_score(X, predictions)\n",
    "                \n",
    "                # Score separation\n",
    "                outlier_scores = lof_scores[predictions == -1]\n",
    "                normal_scores = lof_scores[predictions == 1]\n",
    "                score_separation = np.mean(outlier_scores) - np.mean(normal_scores)\n",
    "            else:\n",
    "                silhouette = -1\n",
    "                score_separation = 0\n",
    "            \n",
    "            results.append({\n",
    "                'metric': metric,\n",
    "                'silhouette_score': silhouette,\n",
    "                'score_separation': score_separation,\n",
    "                'outlier_count': outlier_count,\n",
    "                'mean_lof_score': np.mean(lof_scores)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with metric {metric}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Rank metrics\n",
    "    df_results['rank_silhouette'] = df_results['silhouette_score'].rank(ascending=False)\n",
    "    df_results['rank_separation'] = df_results['score_separation'].rank(ascending=False)\n",
    "    df_results['combined_rank'] = df_results['rank_silhouette'] + df_results['rank_separation']\n",
    "    \n",
    "    best_metric = df_results.loc[df_results['combined_rank'].idxmin(), 'metric']\n",
    "    \n",
    "    print(\"Distance Metric Comparison:\")\n",
    "    print(df_results.sort_values('combined_rank'))\n",
    "    print(f\"\\nRecommended metric: {best_metric}\")\n",
    "    \n",
    "    return best_metric, df_results\n",
    "\n",
    "optimal_metric, metric_results = select_optimal_metric(base_df[['Age']], k=optimal_k)\n",
    "```\n",
    "\n",
    "### üß™ LOF-Specific Validation Methodologies\n",
    "\n",
    "#### **1. Local Density Validation**\n",
    "\n",
    "```python\n",
    "def validate_local_density_detection(X, k_range, contamination=0.1):\n",
    "    \"\"\"\n",
    "    Validate LOF's ability to detect local density anomalies\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_results = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        lof = LocalOutlierFactor(n_neighbors=k, contamination=contamination)\n",
    "        predictions = lof.fit_predict(X)\n",
    "        lof_scores = -lof.negative_outlier_factor_\n",
    "        \n",
    "        # Calculate local density characteristics\n",
    "        outlier_indices = np.where(predictions == -1)[0]\n",
    "        normal_indices = np.where(predictions == 1)[0]\n",
    "        \n",
    "        # Measure how well outliers are isolated in terms of density\n",
    "        if len(outlier_indices) > 0 and len(normal_indices) > 0:\n",
    "            # Calculate average distance to k nearest neighbors for outliers vs normal\n",
    "            from sklearn.neighbors import NearestNeighbors\n",
    "            nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
    "            distances, _ = nbrs.kneighbors(X)\n",
    "            \n",
    "            outlier_avg_distances = np.mean(distances[outlier_indices])\n",
    "            normal_avg_distances = np.mean(distances[normal_indices])\n",
    "            \n",
    "            density_separation = outlier_avg_distances / normal_avg_distances\n",
    "            \n",
    "            # Measure LOF score consistency\n",
    "            outlier_lof_scores = lof_scores[outlier_indices]\n",
    "            normal_lof_scores = lof_scores[normal_indices]\n",
    "            \n",
    "            lof_score_separation = np.mean(outlier_lof_scores) / np.mean(normal_lof_scores)\n",
    "            \n",
    "            validation_results.append({\n",
    "                'k': k,\n",
    "                'density_separation': density_separation,\n",
    "                'lof_score_separation': lof_score_separation,\n",
    "                'outlier_lof_mean': np.mean(outlier_lof_scores),\n",
    "                'outlier_lof_std': np.std(outlier_lof_scores),\n",
    "                'normal_lof_mean': np.mean(normal_lof_scores),\n",
    "                'normal_lof_std': np.std(normal_lof_scores)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(validation_results)\n",
    "\n",
    "# Validate density detection\n",
    "k_range = range(5, 31, 5)\n",
    "density_validation = validate_local_density_detection(base_df[['Age']], k_range)\n",
    "print(\"Local Density Validation Results:\")\n",
    "print(density_validation)\n",
    "```\n",
    "\n",
    "#### **2. Stability Analysis**\n",
    "\n",
    "```python\n",
    "def lof_stability_analysis(X, k, contamination=0.1, n_runs=10, sample_fraction=0.8):\n",
    "    \"\"\"\n",
    "    Analyze LOF stability across different subsamples\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    sample_size = int(n_samples * sample_fraction)\n",
    "    \n",
    "    stability_results = []\n",
    "    all_outlier_indices = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        # Random subsample\n",
    "        np.random.seed(run)\n",
    "        sample_indices = np.random.choice(n_samples, sample_size, replace=False)\n",
    "        X_sample = X.iloc[sample_indices] if hasattr(X, 'iloc') else X[sample_indices]\n",
    "        \n",
    "        # Run LOF\n",
    "        lof = LocalOutlierFactor(n_neighbors=k, contamination=contamination)\n",
    "        predictions = lof.fit_predict(X_sample)\n",
    "        lof_scores = -lof.negative_outlier_factor_\n",
    "        \n",
    "        # Store results\n",
    "        outlier_mask = predictions == -1\n",
    "        outlier_indices_in_sample = np.where(outlier_mask)[0]\n",
    "        # Map back to original indices\n",
    "        original_outlier_indices = sample_indices[outlier_indices_in_sample]\n",
    "        all_outlier_indices.append(set(original_outlier_indices))\n",
    "        \n",
    "        stability_results.append({\n",
    "            'run': run,\n",
    "            'outlier_count': len(original_outlier_indices),\n",
    "            'mean_lof_score': np.mean(lof_scores),\n",
    "            'std_lof_score': np.std(lof_scores),\n",
    "            'outlier_indices': original_outlier_indices\n",
    "        })\n",
    "    \n",
    "    # Calculate consensus outliers (detected in multiple runs)\n",
    "    from collections import Counter\n",
    "    all_detected = [idx for outlier_set in all_outlier_indices for idx in outlier_set]\n",
    "    detection_counts = Counter(all_detected)\n",
    "    \n",
    "    # Points detected in at least 50% of runs\n",
    "    consensus_threshold = n_runs * 0.5\n",
    "    consensus_outliers = [idx for idx, count in detection_counts.items() if count >= consensus_threshold]\n",
    "    \n",
    "    # Calculate stability metrics\n",
    "    outlier_counts = [result['outlier_count'] for result in stability_results]\n",
    "    count_stability = np.std(outlier_counts) / np.mean(outlier_counts) if np.mean(outlier_counts) > 0 else float('inf')\n",
    "    \n",
    "    jaccard_similarities = []\n",
    "    for i in range(len(all_outlier_indices)):\n",
    "        for j in range(i+1, len(all_outlier_indices)):\n",
    "            set1, set2 = all_outlier_indices[i], all_outlier_indices[j]\n",
    "            intersection = len(set1.intersection(set2))\n",
    "            union = len(set1.union(set2))\n",
    "            jaccard = intersection / union if union > 0 else 0\n",
    "            jaccard_similarities.append(jaccard)\n",
    "    \n",
    "    avg_jaccard = np.mean(jaccard_similarities) if jaccard_similarities else 0\n",
    "    \n",
    "    print(f\"Stability Analysis (k={k}):\")\n",
    "    print(f\"Count stability (CV): {count_stability:.3f}\")\n",
    "    print(f\"Average Jaccard similarity: {avg_jaccard:.3f}\")\n",
    "    print(f\"Consensus outliers: {len(consensus_outliers)}\")\n",
    "    print(f\"Detection frequency range: {min(detection_counts.values())} - {max(detection_counts.values())}\")\n",
    "    \n",
    "    return {\n",
    "        'count_stability': count_stability,\n",
    "        'avg_jaccard': avg_jaccard,\n",
    "        'consensus_outliers': consensus_outliers,\n",
    "        'detection_counts': detection_counts,\n",
    "        'stability_results': stability_results\n",
    "    }\n",
    "\n",
    "# Analyze stability\n",
    "stability_analysis = lof_stability_analysis(base_df[['Age']], optimal_k)\n",
    "```\n",
    "\n",
    "#### **3. Business Logic Validation for LOF**\n",
    "\n",
    "```python\n",
    "def business_logic_validation(X, lof_results, domain_constraints):\n",
    "    \"\"\"\n",
    "    Validate LOF results against business logic\n",
    "    \"\"\"\n",
    "    \n",
    "    outlier_indices = np.where(lof_results['predictions'] == -1)[0]\n",
    "    outlier_data = X.iloc[outlier_indices] if hasattr(X, 'iloc') else X[outlier_indices]\n",
    "    lof_scores = lof_results['lof_scores']\n",
    "    \n",
    "    validation_metrics = {}\n",
    "    \n",
    "    # Age-specific business validation for customer data\n",
    "    if 'Age' in X.columns:\n",
    "        outlier_ages = outlier_data['Age']\n",
    "        \n",
    "        # Check if outliers are within reasonable business range\n",
    "        min_reasonable = domain_constraints.get('min_age', 16)\n",
    "        max_reasonable = domain_constraints.get('max_age', 80)\n",
    "        \n",
    "        reasonable_outliers = outlier_ages[(outlier_ages >= min_reasonable) & \n",
    "                                         (outlier_ages <= max_reasonable)]\n",
    "        \n",
    "        validation_metrics['reasonable_outlier_ratio'] = len(reasonable_outliers) / len(outlier_ages) if len(outlier_ages) > 0 else 0\n",
    "        \n",
    "        # Check age distribution of outliers\n",
    "        validation_metrics['outlier_age_stats'] = {\n",
    "            'mean': outlier_ages.mean() if len(outlier_ages) > 0 else None,\n",
    "            'median': outlier_ages.median() if len(outlier_ages) > 0 else None,\n",
    "            'min': outlier_ages.min() if len(outlier_ages) > 0 else None,\n",
    "            'max': outlier_ages.max() if len(outlier_ages) > 0 else None,\n",
    "            'std': outlier_ages.std() if len(outlier_ages) > 0 else None\n",
    "        }\n",
    "        \n",
    "        # Compare with normal population\n",
    "        normal_indices = np.where(lof_results['predictions'] == 1)[0]\n",
    "        normal_ages = X.iloc[normal_indices]['Age'] if hasattr(X, 'iloc') else X[normal_indices, 0]\n",
    "        \n",
    "        # Statistical test for difference\n",
    "        from scipy.stats import mannwhitneyu\n",
    "        if len(outlier_ages) > 0 and len(normal_ages) > 0:\n",
    "            statistic, p_value = mannwhitneyu(outlier_ages, normal_ages, alternative='two-sided')\n",
    "            validation_metrics['age_difference_significant'] = p_value < 0.05\n",
    "            validation_metrics['mannwhitney_pvalue'] = p_value\n",
    "    \n",
    "    # LOF score validation\n",
    "    outlier_lof_scores = lof_scores[outlier_indices]\n",
    "    normal_lof_scores = lof_scores[lof_results['predictions'] == 1]\n",
    "    \n",
    "    validation_metrics['lof_score_stats'] = {\n",
    "        'outlier_mean_lof': np.mean(outlier_lof_scores) if len(outlier_lof_scores) > 0 else None,\n",
    "        'normal_mean_lof': np.mean(normal_lof_scores) if len(normal_lof_scores) > 0 else None,\n",
    "        'score_separation_ratio': (np.mean(outlier_lof_scores) / np.mean(normal_lof_scores)) if len(outlier_lof_scores) > 0 and len(normal_lof_scores) > 0 else None\n",
    "    }\n",
    "    \n",
    "    # Expected vs actual outlier ratio\n",
    "    expected_ratio = domain_constraints.get('expected_outlier_ratio', 0.1)\n",
    "    actual_ratio = len(outlier_indices) / len(X)\n",
    "    validation_metrics['outlier_ratio_match'] = abs(actual_ratio - expected_ratio) < 0.05\n",
    "    \n",
    "    return validation_metrics\n",
    "\n",
    "# Define domain constraints for customer data\n",
    "domain_constraints = {\n",
    "    'min_age': 18,\n",
    "    'max_age': 70,\n",
    "    'expected_outlier_ratio': 0.1\n",
    "}\n",
    "\n",
    "# Run business validation\n",
    "lof_final = LocalOutlierFactor(n_neighbors=optimal_k, contamination=0.1)\n",
    "predictions = lof_final.fit_predict(base_df[['Age']])\n",
    "lof_scores = -lof_final.negative_outlier_factor_\n",
    "\n",
    "lof_results = {\n",
    "    'predictions': predictions,\n",
    "    'lof_scores': lof_scores\n",
    "}\n",
    "\n",
    "business_validation = business_logic_validation(base_df[['Age']], lof_results, domain_constraints)\n",
    "print(\"Business Logic Validation:\")\n",
    "for key, value in business_validation.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "```\n",
    "\n",
    "### üéØ Complete LOF Parameter Tuning Pipeline\n",
    "\n",
    "```python\n",
    "def complete_lof_tuning_pipeline(X, domain_knowledge=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline for LOF parameter optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîç Step 1: Analyzing optimal k parameter...\")\n",
    "    optimal_k, k_details = suggest_optimal_k(X, domain_knowledge)\n",
    "    \n",
    "    print(f\"\\nüìä Step 2: Estimating contamination...\")\n",
    "    contamination_analysis = estimate_contamination_for_lof(X)\n",
    "    recommended_contamination = contamination_analysis['median']\n",
    "    \n",
    "    print(f\"\\nüìè Step 3: Selecting distance metric...\")\n",
    "    optimal_metric, _ = select_optimal_metric(X, optimal_k, recommended_contamination)\n",
    "    \n",
    "    print(f\"\\nüß™ Step 4: Validation analysis...\")\n",
    "    \n",
    "    # Stability analysis\n",
    "    stability_results = lof_stability_analysis(X, optimal_k, recommended_contamination)\n",
    "    \n",
    "    # If stability is poor, adjust k\n",
    "    if stability_results['count_stability'] > 0.3 or stability_results['avg_jaccard'] < 0.5:\n",
    "        print(\"‚ö†Ô∏è  Poor stability detected, adjusting k...\")\n",
    "        # Try with larger k for better stability\n",
    "        adjusted_k = min(optimal_k + 5, len(X) // 4)\n",
    "        stability_results_adjusted = lof_stability_analysis(X, adjusted_k, recommended_contamination)\n",
    "        \n",
    "        if stability_results_adjusted['count_stability'] < stability_results['count_stability']:\n",
    "            optimal_k = adjusted_k\n",
    "            print(f\"‚úÖ Adjusted k to {optimal_k} for better stability\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Keeping original k={optimal_k}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Final Recommended Parameters:\")\n",
    "    print(f\"n_neighbors: {optimal_k}\")\n",
    "    print(f\"contamination: {recommended_contamination:.3f}\")\n",
    "    print(f\"metric: {optimal_metric}\")\n",
    "    \n",
    "    # Create final optimized LOF\n",
    "    lof_optimized = LocalOutlierFactor(\n",
    "        n_neighbors=optimal_k,\n",
    "        contamination=recommended_contamination,\n",
    "        metric=optimal_metric,\n",
    "        algorithm='auto'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'n_neighbors': optimal_k,\n",
    "        'contamination': recommended_contamination,\n",
    "        'metric': optimal_metric,\n",
    "        'lof_model': lof_optimized,\n",
    "        'validation_results': {\n",
    "            'k_analysis': k_details,\n",
    "            'contamination_analysis': contamination_analysis,\n",
    "            'stability_results': stability_results\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Apply complete pipeline to your customer data\n",
    "optimal_lof_params = complete_lof_tuning_pipeline(\n",
    "    base_df[['Age']], \n",
    "    domain_knowledge={'cluster_expected': True, 'uniform_density': False}\n",
    ")\n",
    "```\n",
    "\n",
    "### üìù LOF Parameter Validation Checklist\n",
    "\n",
    "#### **‚úÖ Parameters are Well-Tuned When:**\n",
    "\n",
    "1. **k Parameter Validation:**\n",
    "   - Stability across subsamples (Jaccard similarity > 0.6)\n",
    "   - Consistent outlier detection (CV of outlier counts < 0.3)\n",
    "   - Good score separation between outliers and normal points\n",
    "   - Elbow point in stability curve\n",
    "\n",
    "2. **Contamination Validation:**\n",
    "   - Detected outlier ratio ‚âà expected ratio ¬±3%\n",
    "   - Outliers are interpretable in business context\n",
    "   - Statistical significance in outlier vs normal comparison\n",
    "\n",
    "3. **Overall Model Validation:**\n",
    "   - High silhouette score (> 0.3)\n",
    "   - LOF scores > 1.2 for outliers, ‚âà 1.0 for normal points\n",
    "   - Consensus outliers across multiple runs\n",
    "   - Business logic validation passes\n",
    "\n",
    "#### **üö® Red Flags (Poor Tuning):**\n",
    "\n",
    "- **High instability**: Different outliers detected across runs\n",
    "- **Extreme k values**: k < 5 or k > n/3\n",
    "- **Poor score separation**: Outlier and normal LOF scores overlap significantly\n",
    "- **Business contradiction**: Outliers don't make domain sense\n",
    "- **Uniform detection**: All points have similar LOF scores\n",
    "\n",
    "### üéØ Specific Recommendations for Your Customer Data\n",
    "\n",
    "```python\n",
    "# Optimized LOF for customer age analysis\n",
    "def create_optimized_customer_lof(customer_data):\n",
    "    \"\"\"\n",
    "    Create optimized LOF specifically for customer age data\n",
    "    \"\"\"\n",
    "    \n",
    "    n_customers = len(customer_data)\n",
    "    \n",
    "    # Customer-specific parameter selection\n",
    "    if n_customers < 100:\n",
    "        recommended_k = max(5, n_customers // 10)\n",
    "    elif n_customers < 500:\n",
    "        recommended_k = max(10, n_customers // 20)\n",
    "    else:\n",
    "        recommended_k = max(15, min(30, n_customers // 25))\n",
    "    \n",
    "    # Conservative contamination for business data\n",
    "    contamination = 0.08  # 8% is reasonable for customer outliers\n",
    "    \n",
    "    lof_customer = LocalOutlierFactor(\n",
    "        n_neighbors=recommended_k,\n",
    "        contamination=contamination,\n",
    "        metric='euclidean',        # Standard for age data\n",
    "        algorithm='auto',          # Let sklearn optimize\n",
    "        leaf_size=30,             # Good default for moderate data\n",
    "        p=2                       # Euclidean distance parameter\n",
    "    )\n",
    "    \n",
    "    return lof_customer\n",
    "\n",
    "# Create and validate your optimal LOF\n",
    "optimal_customer_lof = create_optimized_customer_lof(base_df)\n",
    "\n",
    "# Apply and analyze results\n",
    "predictions = optimal_customer_lof.fit_predict(base_df[['Age']])\n",
    "lof_scores = -optimal_customer_lof.negative_outlier_factor_\n",
    "\n",
    "# Final analysis\n",
    "print(\"üéØ Final LOF Results for Customer Segmentation:\")\n",
    "print(f\"Total customers: {len(base_df)}\")\n",
    "print(f\"Detected outliers: {np.sum(predictions == -1)}\")\n",
    "print(f\"Outlier percentage: {np.sum(predictions == -1)/len(base_df)*100:.1f}%\")\n",
    "print(f\"Mean LOF score for outliers: {np.mean(lof_scores[predictions == -1]):.2f}\")\n",
    "print(f\"Mean LOF score for normal: {np.mean(lof_scores[predictions == 1]):.2f}\")\n",
    "\n",
    "# Show top outliers\n",
    "outlier_indices = np.where(predictions == -1)[0]\n",
    "if len(outlier_indices) > 0:\n",
    "    outlier_details = pd.DataFrame({\n",
    "        'Customer_Index': outlier_indices,\n",
    "        'Age': base_df.iloc[outlier_indices]['Age'].values,\n",
    "        'LOF_Score': lof_scores[outlier_indices]\n",
    "    }).sort_values('LOF_Score', ascending=False)\n",
    "    \n",
    "    print(\"\\nüîç Top Anomalous Customers:\")\n",
    "    print(outlier_details.head())\n",
    "```\n",
    "\n",
    "The key to successful LOF parameter tuning is **iterative validation** - start with data-driven estimates, validate stability and business logic, then refine based on results! üéØ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
