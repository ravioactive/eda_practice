{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71f6ca5",
   "metadata": {},
   "source": [
    "## üå≤ Isolation Forest: Advanced Outlier Detection Explained\n",
    "\n",
    "### üìã Code Breakdown\n",
    "```python\n",
    "# Isolation Forest\n",
    "import sklearn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "iso_forest = IsolationForest(contamination=0.1)\n",
    "outliers = iso_forest.fit_predict(base_df[['Age']])\n",
    "print(outliers)\n",
    "```\n",
    "\n",
    "**Line-by-line explanation:**\n",
    "1. **Import sklearn ensemble module** containing Isolation Forest\n",
    "2. **Create Isolation Forest instance** with 10% contamination expectation\n",
    "3. **Fit and predict** on Age column (returns -1 for outliers, 1 for normal)\n",
    "4. **Print binary classification** results\n",
    "\n",
    "### üìö Essential Documentation & Resources\n",
    "\n",
    "#### **Official Documentation:**\n",
    "- **[Scikit-learn Isolation Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)** - Official API reference\n",
    "- **[Scikit-learn Outlier Detection Guide](https://scikit-learn.org/stable/modules/outlier_detection.html)** - Comprehensive outlier detection overview\n",
    "- **[Original Paper: \"Isolation Forest\" by Liu et al. (2008)](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)** - Foundational research paper\n",
    "\n",
    "#### **Helpful Blogs & Tutorials:**\n",
    "- **[Towards Data Science: Isolation Forest Explained](https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e)**\n",
    "- **[Machine Learning Mastery: Isolation Forest Tutorial](https://machinelearningmastery.com/isolation-forest-for-outlier-detection/)**\n",
    "- **[Analytics Vidhya: Complete Guide to Outlier Detection](https://www.analyticsvidhya.com/blog/2021/05/feature-engineering-how-to-detect-and-remove-outliers-using-python/)**\n",
    "\n",
    "#### **Advanced Resources:**\n",
    "- **[Extended Isolation Forest Paper (2019)](https://arxiv.org/abs/1811.02141)** - Improved version addressing bias issues\n",
    "- **[Anomaly Detection Comparison Study](https://www.sciencedirect.com/science/article/pii/S0031320319302535)**\n",
    "\n",
    "### üîç How Isolation Forest Works\n",
    "\n",
    "#### **Core Algorithm Concept:**\n",
    "1. **Random Partitioning**: Creates binary trees by randomly selecting features and split values\n",
    "2. **Isolation Principle**: Outliers require fewer splits to isolate than normal points\n",
    "3. **Anomaly Score**: Based on average path length across multiple trees\n",
    "4. **Ensemble Approach**: Combines results from multiple isolation trees\n",
    "\n",
    "#### **Mathematical Foundation:**\n",
    "```python\n",
    "# Anomaly score calculation:\n",
    "s(x,n) = 2^(-E(h(x))/c(n))\n",
    "# Where:\n",
    "# E(h(x)) = average path length of point x\n",
    "# c(n) = average path length of unsuccessful search in BST with n points\n",
    "```\n",
    "\n",
    "### üìä Output Interpretation\n",
    "\n",
    "Your output will be an array like: `[1, 1, -1, 1, 1, -1, ...]`\n",
    "\n",
    "**Interpretation:**\n",
    "- **`1`**: Normal point (inlier)\n",
    "- **`-1`**: Outlier (anomaly)\n",
    "\n",
    "**Practical Usage:**\n",
    "```python\n",
    "# Get outlier indices\n",
    "outlier_indices = np.where(outliers == -1)[0]\n",
    "outlier_customers = base_df.iloc[outlier_indices]\n",
    "\n",
    "# Get anomaly scores (confidence measure)\n",
    "anomaly_scores = iso_forest.decision_function(base_df[['Age']])\n",
    "# Scores closer to -1 = more anomalous\n",
    "# Scores closer to 0 = more normal\n",
    "\n",
    "# Practical analysis\n",
    "print(f\"Found {len(outlier_customers)} outliers out of {len(base_df)} customers\")\n",
    "print(f\"Outlier percentage: {len(outlier_customers)/len(base_df)*100:.1f}%\")\n",
    "```\n",
    "\n",
    "### ‚öñÔ∏è Isolation Forest vs Other Outlier Detection Methods\n",
    "\n",
    "| **Method** | **Strengths** | **Weaknesses** | **Best Use Case** |\n",
    "|------------|---------------|----------------|-------------------|\n",
    "| **Standard Z-Score** | ‚úÖ Simple, fast<br/>‚úÖ Interpretable<br/>‚úÖ Works well for normal data | ‚ùå Sensitive to outliers<br/>‚ùå Assumes normality<br/>‚ùå Univariate only | Clean, normally distributed data |\n",
    "| **Modified Z-Score** | ‚úÖ Robust to outliers<br/>‚úÖ No normality assumption<br/>‚úÖ Interpretable | ‚ùå Univariate only<br/>‚ùå May miss complex patterns<br/>‚ùå Less efficient than parametric methods | Univariate data with potential outliers |\n",
    "| **Isolation Forest** | ‚úÖ **Multivariate capable**<br/>‚úÖ **No distribution assumptions**<br/>‚úÖ **Handles complex patterns**<br/>‚úÖ **Scalable to big data**<br/>‚úÖ **Tree-based interpretability** | ‚ùå **Hyperparameter sensitive**<br/>‚ùå **Black box (less interpretable)**<br/>‚ùå **May struggle with very high dimensions**<br/>‚ùå **Randomness in results** | **Complex, multivariate anomaly detection** |\n",
    "\n",
    "### üéØ Detailed Comparison\n",
    "\n",
    "#### **Isolation Forest Strengths:**\n",
    "1. **Multivariate Detection**: Can find outliers based on combinations of features\n",
    "2. **No Assumptions**: Works with any data distribution\n",
    "3. **Scalability**: Linear time complexity O(nlogn)\n",
    "4. **Robust**: Not affected by data normalization\n",
    "5. **Complex Patterns**: Can detect non-linear anomalies\n",
    "\n",
    "#### **Isolation Forest Weaknesses:**\n",
    "1. **Parameter Sensitivity**: `contamination` parameter needs tuning\n",
    "2. **High Dimensionality**: Performance degrades with many features (curse of dimensionality)\n",
    "3. **Interpretability**: Harder to explain why something is an outlier\n",
    "4. **Randomness**: Results can vary between runs (set random_state for reproducibility)\n",
    "5. **Normal Data Requirement**: Needs enough normal data to learn patterns\n",
    "\n",
    "### üöÄ Advanced Usage Tips\n",
    "\n",
    "```python\n",
    "# Better implementation with more control:\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.1,        # Expected outlier proportion\n",
    "    n_estimators=100,         # Number of trees (more = stable)\n",
    "    max_samples='auto',       # Samples per tree\n",
    "    max_features=1.0,         # Features per tree\n",
    "    random_state=42          # Reproducibility\n",
    ")\n",
    "\n",
    "# Get both predictions and scores\n",
    "predictions = iso_forest.fit_predict(base_df[['Age']])\n",
    "scores = iso_forest.decision_function(base_df[['Age']])\n",
    "\n",
    "# Custom threshold based on percentile\n",
    "threshold = np.percentile(scores, 10)  # Bottom 10% as outliers\n",
    "custom_outliers = scores < threshold\n",
    "```\n",
    "\n",
    "### üéØ When to Use Isolation Forest\n",
    "\n",
    "**‚úÖ Use Isolation Forest when:**\n",
    "- Working with **multivariate data** (multiple features)\n",
    "- **No assumptions** about data distribution\n",
    "- Need to detect **complex anomaly patterns**\n",
    "- Have **sufficient normal data** for training\n",
    "- **Scalability** is important\n",
    "\n",
    "**‚ùå Don't use when:**\n",
    "- Need **highly interpretable** results\n",
    "- Working with **very high-dimensional** data (>50 features)\n",
    "- Have **very little data** (<100 points)\n",
    "- **Simple univariate** outliers are sufficient\n",
    "\n",
    "### üèÜ Recommendation for Your Customer Segmentation\n",
    "\n",
    "For customer segmentation analysis, **Isolation Forest is excellent** because:\n",
    "\n",
    "1. **Customer behavior is multivariate** - age, income, spending interact\n",
    "2. **No assumptions needed** about customer distribution patterns  \n",
    "3. **Business relevance** - can identify truly unusual customer profiles\n",
    "4. **Actionable insights** - outlier customers may represent high-value or problem segments\n",
    "\n",
    "**Next steps to enhance your analysis:**\n",
    "```python\n",
    "# Multi-feature outlier detection\n",
    "iso_forest_multi = IsolationForest(contamination=0.05, random_state=42)\n",
    "multi_outliers = iso_forest_multi.fit_predict(base_df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']])\n",
    "\n",
    "# This will give you customers who are outliers based on their overall profile,\n",
    "# not just individual features!\n",
    "```\n",
    "\n",
    "The Isolation Forest complements your Z-score methods perfectly - use Z-scores for **univariate understanding** and Isolation Forest for **multivariate anomaly detection**! üéØ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2363b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1  1  1 -1 -1  1 -1  1  1 -1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1 -1  1 -1 -1 -1  1  1  1 -1  1 -1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "# Isolation Forest\n",
    "import sklearn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "iso_forest = IsolationForest(contamination=0.1)\n",
    "outliers = iso_forest.fit_predict(base_df[['Age']])\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e75696a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Annual Income (k$)</th>\n",
       "      <th>Spending Score (1-100)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Male</td>\n",
       "      <td>64</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>Female</td>\n",
       "      <td>65</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>69</td>\n",
       "      <td>44</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>Male</td>\n",
       "      <td>70</td>\n",
       "      <td>46</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>Male</td>\n",
       "      <td>63</td>\n",
       "      <td>48</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>48</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>Female</td>\n",
       "      <td>68</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>Male</td>\n",
       "      <td>70</td>\n",
       "      <td>49</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>Female</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>59</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>107</td>\n",
       "      <td>Female</td>\n",
       "      <td>66</td>\n",
       "      <td>63</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>109</td>\n",
       "      <td>Male</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>110</td>\n",
       "      <td>Male</td>\n",
       "      <td>66</td>\n",
       "      <td>63</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>111</td>\n",
       "      <td>Male</td>\n",
       "      <td>65</td>\n",
       "      <td>63</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>115</td>\n",
       "      <td>Female</td>\n",
       "      <td>18</td>\n",
       "      <td>65</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>Female</td>\n",
       "      <td>63</td>\n",
       "      <td>65</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
       "8             9    Male   64                  19                       3\n",
       "33           34    Male   18                  33                      92\n",
       "40           41  Female   65                  38                      35\n",
       "57           58    Male   69                  44                      46\n",
       "60           61    Male   70                  46                      56\n",
       "64           65    Male   63                  48                      51\n",
       "65           66    Male   18                  48                      59\n",
       "67           68  Female   68                  48                      48\n",
       "70           71    Male   70                  49                      55\n",
       "90           91  Female   68                  59                      55\n",
       "91           92    Male   18                  59                      41\n",
       "106         107  Female   66                  63                      50\n",
       "108         109    Male   68                  63                      43\n",
       "109         110    Male   66                  63                      48\n",
       "110         111    Male   65                  63                      52\n",
       "114         115  Female   18                  65                      48\n",
       "116         117  Female   63                  65                      43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get outlier indices\n",
    "outlier_indices = np.where(outliers == -1)[0]\n",
    "outlier_customers = base_df.iloc[outlier_indices]\n",
    "outlier_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2f917",
   "metadata": {},
   "source": [
    "## üéØ Isolation Forest Parameter Tuning: Data-Driven Methodology\n",
    "\n",
    "### üìä Core Parameters and Their Impact\n",
    "\n",
    "#### **1. `contamination` - Most Critical Parameter**\n",
    "\n",
    "**What it controls:** Expected proportion of outliers in your dataset\n",
    "\n",
    "**Data-driven selection methods:**\n",
    "\n",
    "```python\n",
    "# Method 1: Domain Knowledge + EDA\n",
    "def estimate_contamination_eda(df, column):\n",
    "    \"\"\"Estimate contamination based on statistical analysis\"\"\"\n",
    "    \n",
    "    # Use IQR method as baseline\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    iqr_outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    iqr_contamination = len(iqr_outliers) / len(df)\n",
    "    \n",
    "    # Use Z-score method\n",
    "    z_scores = np.abs(stats.zscore(df[column]))\n",
    "    zscore_outliers = df[z_scores > 3]\n",
    "    zscore_contamination = len(zscore_outliers) / len(df)\n",
    "    \n",
    "    # Use modified Z-score\n",
    "    median = df[column].median()\n",
    "    mad = np.median(np.abs(df[column] - median))\n",
    "    modified_z_scores = 0.6745 * (df[column] - median) / mad\n",
    "    mod_zscore_outliers = df[np.abs(modified_z_scores) > 3.5]\n",
    "    mod_contamination = len(mod_zscore_outliers) / len(df)\n",
    "    \n",
    "    # Take conservative estimate (usually the minimum)\n",
    "    estimates = [iqr_contamination, zscore_contamination, mod_contamination]\n",
    "    conservative_estimate = min(estimates)\n",
    "    \n",
    "    print(f\"IQR contamination estimate: {iqr_contamination:.3f}\")\n",
    "    print(f\"Z-score contamination estimate: {zscore_contamination:.3f}\")\n",
    "    print(f\"Modified Z-score contamination estimate: {mod_contamination:.3f}\")\n",
    "    print(f\"Conservative estimate: {conservative_estimate:.3f}\")\n",
    "    \n",
    "    return conservative_estimate\n",
    "\n",
    "# Apply to your data\n",
    "contamination_estimate = estimate_contamination_eda(base_df, 'Age')\n",
    "```\n",
    "\n",
    "**Heuristic Rules for Contamination:**\n",
    "- **Financial data**: 1-5% (fraud detection)\n",
    "- **Customer data**: 5-15% (unusual behavior)\n",
    "- **Sensor data**: 0.1-2% (equipment failures)\n",
    "- **Web traffic**: 10-20% (bot detection)\n",
    "- **Unknown domain**: Start with 5-10%\n",
    "\n",
    "#### **2. `n_estimators` - Stability vs Speed**\n",
    "\n",
    "**Data-driven selection:**\n",
    "\n",
    "```python\n",
    "def find_optimal_n_estimators(X, contamination, max_estimators=500):\n",
    "    \"\"\"Find optimal number of estimators based on stability\"\"\"\n",
    "    \n",
    "    estimator_range = [10, 25, 50, 100, 150, 200, 300, 500]\n",
    "    stability_scores = []\n",
    "    \n",
    "    for n_est in estimator_range:\n",
    "        # Run multiple times to check stability\n",
    "        scores = []\n",
    "        for seed in range(5):  # 5 different random seeds\n",
    "            iso_forest = IsolationForest(\n",
    "                contamination=contamination,\n",
    "                n_estimators=n_est,\n",
    "                random_state=seed\n",
    "            )\n",
    "            score = iso_forest.fit(X).decision_function(X)\n",
    "            scores.append(score)\n",
    "        \n",
    "        # Calculate coefficient of variation (stability measure)\n",
    "        mean_scores = np.mean(scores, axis=0)\n",
    "        std_scores = np.std(scores, axis=0)\n",
    "        cv = np.mean(std_scores / np.abs(mean_scores))\n",
    "        stability_scores.append(cv)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(estimator_range, stability_scores, 'bo-')\n",
    "    plt.xlabel('Number of Estimators')\n",
    "    plt.ylabel('Coefficient of Variation (lower = more stable)')\n",
    "    plt.title('Stability vs Number of Estimators')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Find elbow point\n",
    "    optimal_n = estimator_range[np.argmin(stability_scores)]\n",
    "    return optimal_n, stability_scores\n",
    "\n",
    "# Apply to your data\n",
    "optimal_n_estimators, _ = find_optimal_n_estimators(base_df[['Age']], contamination_estimate)\n",
    "```\n",
    "\n",
    "**Heuristic Rules:**\n",
    "- **Small datasets** (<1000 points): 50-100 estimators\n",
    "- **Medium datasets** (1000-10000): 100-200 estimators  \n",
    "- **Large datasets** (>10000): 200-500 estimators\n",
    "- **Rule of thumb**: More estimators = more stable, but diminishing returns after 200\n",
    "\n",
    "#### **3. `max_samples` - Sample Size Control**\n",
    "\n",
    "```python\n",
    "def determine_max_samples(n_samples):\n",
    "    \"\"\"Determine optimal max_samples based on dataset size\"\"\"\n",
    "    \n",
    "    if n_samples < 100:\n",
    "        return 'auto'  # Use all samples\n",
    "    elif n_samples < 1000:\n",
    "        return min(256, n_samples)  # Use up to 256\n",
    "    elif n_samples < 10000:\n",
    "        return 256  # Standard recommendation\n",
    "    else:\n",
    "        return 512  # For large datasets\n",
    "        \n",
    "max_samples_optimal = determine_max_samples(len(base_df))\n",
    "```\n",
    "\n",
    "**Rules:**\n",
    "- **'auto'**: Uses min(256, n_samples) - good default\n",
    "- **Small values** (64-128): Faster, less memory, might be less accurate\n",
    "- **Large values** (512+): More accurate, slower, more memory\n",
    "- **Sweet spot**: 256 for most applications\n",
    "\n",
    "#### **4. `max_features` - Feature Sampling**\n",
    "\n",
    "```python\n",
    "def determine_max_features(n_features):\n",
    "    \"\"\"Determine optimal max_features based on dimensionality\"\"\"\n",
    "    \n",
    "    if n_features == 1:\n",
    "        return 1.0  # Use the only feature\n",
    "    elif n_features <= 5:\n",
    "        return 1.0  # Use all features\n",
    "    elif n_features <= 20:\n",
    "        return 0.8  # Use 80% of features\n",
    "    else:\n",
    "        return 0.5  # Use 50% for high-dimensional data\n",
    "\n",
    "max_features_optimal = determine_max_features(1)  # For Age only\n",
    "```\n",
    "\n",
    "### üß™ Validation Methodologies\n",
    "\n",
    "#### **1. Cross-Validation for Unsupervised Learning**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def validate_isolation_forest(X, param_grid, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Validate Isolation Forest using multiple metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for contamination in param_grid['contamination']:\n",
    "        for n_estimators in param_grid['n_estimators']:\n",
    "            fold_scores = []\n",
    "            \n",
    "            # Use time-based or random splits\n",
    "            kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X):\n",
    "                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                \n",
    "                # Fit on train, predict on validation\n",
    "                iso_forest = IsolationForest(\n",
    "                    contamination=contamination,\n",
    "                    n_estimators=n_estimators,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                iso_forest.fit(X_train)\n",
    "                val_scores = iso_forest.decision_function(X_val)\n",
    "                val_predictions = iso_forest.predict(X_val)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                # 1. Silhouette score (higher is better)\n",
    "                silhouette = silhouette_score(X_val, val_predictions)\n",
    "                \n",
    "                # 2. Stability score (lower variance is better)\n",
    "                score_variance = np.var(val_scores)\n",
    "                \n",
    "                fold_scores.append({\n",
    "                    'silhouette': silhouette,\n",
    "                    'score_variance': score_variance,\n",
    "                    'mean_score': np.mean(val_scores),\n",
    "                    'outlier_ratio': np.mean(val_predictions == -1)\n",
    "                })\n",
    "            \n",
    "            # Aggregate fold results\n",
    "            avg_silhouette = np.mean([s['silhouette'] for s in fold_scores])\n",
    "            avg_variance = np.mean([s['score_variance'] for s in fold_scores])\n",
    "            \n",
    "            results.append({\n",
    "                'contamination': contamination,\n",
    "                'n_estimators': n_estimators,\n",
    "                'avg_silhouette': avg_silhouette,\n",
    "                'avg_variance': avg_variance,\n",
    "                'score': avg_silhouette - 0.1 * avg_variance  # Combined metric\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Parameter grid for validation\n",
    "param_grid = {\n",
    "    'contamination': [0.05, 0.1, 0.15, 0.2],\n",
    "    'n_estimators': [50, 100, 150, 200]\n",
    "}\n",
    "\n",
    "validation_results = validate_isolation_forest(base_df[['Age']], param_grid)\n",
    "best_params = validation_results.loc[validation_results['score'].idxmax()]\n",
    "print(\"Best parameters:\", best_params)\n",
    "```\n",
    "\n",
    "#### **2. Business-Driven Validation**\n",
    "\n",
    "```python\n",
    "def business_validation(outliers_df, original_df, domain_knowledge):\n",
    "    \"\"\"\n",
    "    Validate outliers using business logic\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example for customer segmentation\n",
    "    validation_metrics = {}\n",
    "    \n",
    "    # 1. Age distribution check\n",
    "    age_outliers = outliers_df['Age']\n",
    "    if domain_knowledge['min_reasonable_age'] <= age_outliers.min() <= age_outliers.max() <= domain_knowledge['max_reasonable_age']:\n",
    "        validation_metrics['age_reasonable'] = True\n",
    "    else:\n",
    "        validation_metrics['age_reasonable'] = False\n",
    "    \n",
    "    # 2. Outlier characteristics\n",
    "    validation_metrics['outlier_stats'] = {\n",
    "        'mean_age': age_outliers.mean(),\n",
    "        'median_age': age_outliers.median(),\n",
    "        'age_range': (age_outliers.min(), age_outliers.max())\n",
    "    }\n",
    "    \n",
    "    # 3. Distribution comparison\n",
    "    from scipy.stats import ks_2samp\n",
    "    ks_stat, p_value = ks_2samp(original_df['Age'], age_outliers)\n",
    "    validation_metrics['distribution_different'] = p_value < 0.05\n",
    "    \n",
    "    return validation_metrics\n",
    "\n",
    "# Define domain knowledge for customers\n",
    "domain_knowledge = {\n",
    "    'min_reasonable_age': 15,  # Minimum customer age\n",
    "    'max_reasonable_age': 80,  # Maximum reasonable age\n",
    "}\n",
    "```\n",
    "\n",
    "#### **3. Ensemble Validation Method**\n",
    "\n",
    "```python\n",
    "def ensemble_validation(X, contamination_range, n_runs=10):\n",
    "    \"\"\"\n",
    "    Use ensemble of different configurations to validate\n",
    "    \"\"\"\n",
    "    \n",
    "    all_predictions = []\n",
    "    configurations = []\n",
    "    \n",
    "    for contamination in contamination_range:\n",
    "        for run in range(n_runs):\n",
    "            iso_forest = IsolationForest(\n",
    "                contamination=contamination,\n",
    "                n_estimators=100,\n",
    "                random_state=run,\n",
    "                max_samples='auto'\n",
    "            )\n",
    "            \n",
    "            predictions = iso_forest.fit_predict(X)\n",
    "            scores = iso_forest.decision_function(X)\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            configurations.append({\n",
    "                'contamination': contamination,\n",
    "                'run': run,\n",
    "                'outlier_count': np.sum(predictions == -1),\n",
    "                'mean_score': np.mean(scores)\n",
    "            })\n",
    "    \n",
    "    # Find consensus outliers (detected by multiple configurations)\n",
    "    predictions_matrix = np.array(all_predictions)\n",
    "    consensus_strength = np.mean(predictions_matrix == -1, axis=0)\n",
    "    \n",
    "    # Points detected as outliers by >50% of models\n",
    "    consensus_outliers = consensus_strength > 0.5\n",
    "    \n",
    "    return consensus_outliers, consensus_strength, pd.DataFrame(configurations)\n",
    "\n",
    "contamination_range = [0.05, 0.1, 0.15]\n",
    "consensus_outliers, strength, config_df = ensemble_validation(base_df[['Age']], contamination_range)\n",
    "```\n",
    "\n",
    "### üéØ Complete Parameter Tuning Pipeline\n",
    "\n",
    "```python\n",
    "def complete_isolation_forest_tuning(X, domain_knowledge=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline for Isolation Forest parameter tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîç Step 1: Estimating contamination...\")\n",
    "    contamination_est = estimate_contamination_eda(X, X.columns[0])\n",
    "    \n",
    "    print(f\"\\nüå≤ Step 2: Finding optimal n_estimators...\")\n",
    "    optimal_n_est, _ = find_optimal_n_estimators(X, contamination_est)\n",
    "    \n",
    "    print(f\"\\nüìä Step 3: Determining other parameters...\")\n",
    "    max_samples_opt = determine_max_samples(len(X))\n",
    "    max_features_opt = determine_max_features(X.shape[1])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Step 4: Validation...\")\n",
    "    # Test around the estimated contamination\n",
    "    contamination_range = [\n",
    "        max(0.01, contamination_est - 0.05),\n",
    "        contamination_est,\n",
    "        contamination_est + 0.05\n",
    "    ]\n",
    "    \n",
    "    param_grid = {\n",
    "        'contamination': contamination_range,\n",
    "        'n_estimators': [optimal_n_est - 50, optimal_n_est, optimal_n_est + 50]\n",
    "    }\n",
    "    \n",
    "    validation_results = validate_isolation_forest(X, param_grid)\n",
    "    best_params = validation_results.loc[validation_results['score'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nüèÜ Final Recommended Parameters:\")\n",
    "    print(f\"contamination: {best_params['contamination']:.3f}\")\n",
    "    print(f\"n_estimators: {int(best_params['n_estimators'])}\")\n",
    "    print(f\"max_samples: {max_samples_opt}\")\n",
    "    print(f\"max_features: {max_features_opt}\")\n",
    "    \n",
    "    return {\n",
    "        'contamination': best_params['contamination'],\n",
    "        'n_estimators': int(best_params['n_estimators']),\n",
    "        'max_samples': max_samples_opt,\n",
    "        'max_features': max_features_opt,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "# Apply to your customer data\n",
    "optimal_params = complete_isolation_forest_tuning(base_df[['Age']])\n",
    "\n",
    "# Create optimally tuned Isolation Forest\n",
    "iso_forest_tuned = IsolationForest(**optimal_params)\n",
    "```\n",
    "\n",
    "### üìù Summary: Validation Checklist\n",
    "\n",
    "#### **‚úÖ Parameter Tuning is Correct When:**\n",
    "\n",
    "1. **Contamination Check:**\n",
    "   - Detected outlier proportion ‚âà expected contamination ¬±2%\n",
    "   - Outliers are interpretable in business context\n",
    "   - Not too many obvious normal points flagged as outliers\n",
    "\n",
    "2. **Stability Check:**\n",
    "   - Results consistent across multiple runs (CV < 0.1)\n",
    "   - Similar outliers detected with different random seeds\n",
    "   - Gradual changes in contamination don't cause dramatic shifts\n",
    "\n",
    "3. **Business Logic Check:**\n",
    "   - Outliers make domain sense\n",
    "   - Can explain why these points are unusual\n",
    "   - Actionable insights emerge from outliers\n",
    "\n",
    "4. **Statistical Validation:**\n",
    "   - High silhouette score (>0.3)\n",
    "   - Outliers significantly different from normal points\n",
    "   - Low variance in anomaly scores for normal points\n",
    "\n",
    "#### **üö® Red Flags (Poor Tuning):**\n",
    "\n",
    "- **Too many outliers** (>20% unless expected)\n",
    "- **No clear pattern** in detected outliers\n",
    "- **High variance** between runs\n",
    "- **Outliers cluster together** (should be scattered)\n",
    "- **Domain experts disagree** with flagged outliers\n",
    "\n",
    "### üéØ For Your Customer Segmentation:\n",
    "\n",
    "```python\n",
    "# Recommended starting point for your Age analysis\n",
    "iso_forest_customer = IsolationForest(\n",
    "    contamination=0.08,      # Based on customer data typical range\n",
    "    n_estimators=150,        # Good balance for 200 customers\n",
    "    max_samples='auto',      # Let sklearn decide\n",
    "    max_features=1.0,        # Use all features (only Age)\n",
    "    random_state=42,         # Reproducibility\n",
    "    bootstrap=False          # Don't bootstrap for small datasets\n",
    ")\n",
    "\n",
    "# For multivariate analysis (Age + Income + Spending)\n",
    "iso_forest_multivariate = IsolationForest(\n",
    "    contamination=0.05,      # More conservative for multivariate\n",
    "    n_estimators=200,        # More estimators for stability\n",
    "    max_samples=256,         # Good for 200 samples\n",
    "    max_features=0.8,        # Use 80% of features\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "The key is to **iterate and validate** - start with data-driven estimates, then refine based on business validation and stability checks! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be22dc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05875794,  0.04603012,  0.03964111,  0.07076774,  0.1243589 ,\n",
       "        0.01752559,  0.13316182,  0.07076774, -0.04900193,  0.11759143,\n",
       "        0.        ,  0.13316182,  0.03658042,  0.05457051,  0.07422109,\n",
       "        0.01752559,  0.13316182,  0.03964111,  0.0352743 ,  0.13316182,\n",
       "        0.13316182,  0.04210439,  0.05356203,  0.1243589 ,  0.06338619,\n",
       "        0.09342186,  0.05325864,  0.13316182,  0.09838464,  0.07076774,\n",
       "        0.01147403,  0.04603012,  0.04077028, -0.04620988,  0.1089244 ,\n",
       "        0.04603012,  0.05828609,  0.11759143,  0.10648652,  0.03964111,\n",
       "       -0.01599448,  0.05457051,  0.08807805,  0.1243589 ,  0.1089244 ,\n",
       "        0.05457051,  0.09009225,  0.09391343,  0.09342186,  0.1243589 ,\n",
       "        0.1089244 ,  0.07533285,  0.1243589 ,  0.05525224,  0.09009225,\n",
       "        0.09339446,  0.04078287, -0.10264807,  0.09391343,  0.04077028,\n",
       "       -0.10605735,  0.05875794,  0.        ,  0.06338619, -0.0211363 ,\n",
       "       -0.04620988,  0.05465165, -0.02911194,  0.05875794,  0.13988973,\n",
       "       -0.10605735,  0.09339446,  0.01147403,  0.01147403,  0.05525224,\n",
       "        0.03728697,  0.05325864,  0.09838464,  0.07076774,  0.1089244 ,\n",
       "        0.03676597,  0.10479496,  0.        ,  0.05356203,  0.04603012,\n",
       "        0.08807805,  0.00291979,  0.01752559,  0.10218369,  0.09009225,\n",
       "       -0.02911194, -0.04620988,  0.08807805,  0.09838464,  0.13988973,\n",
       "        0.05457051,  0.09339446,  0.09391343,  0.08807805,  0.03964111,\n",
       "        0.07076774,  0.1089244 ,  0.        ,  0.03728697,  0.1089244 ,\n",
       "        0.04603012, -0.02231893,  0.06338619, -0.02911194, -0.02231893,\n",
       "       -0.01599448,  0.05875794,  0.10479496,  0.05875794, -0.04620988,\n",
       "        0.05875794, -0.0211363 ,  0.1089244 ,  0.04078287,  0.09009225,\n",
       "        0.09391343,  0.10479496,  0.09838464,  0.06952633,  0.07076774,\n",
       "        0.1243589 ,  0.05465165,  0.09838464,  0.05525224,  0.10479496,\n",
       "        0.09339446,  0.06952633,  0.04210439,  0.1243589 ,  0.03964111,\n",
       "        0.09342186,  0.04958602,  0.13988973,  0.05875794,  0.13316182,\n",
       "        0.03676597,  0.13988973,  0.07490171,  0.13988973,  0.04210439,\n",
       "        0.07490171,  0.08807805,  0.13988973,  0.10218369,  0.10218369,\n",
       "        0.05465165,  0.06952633,  0.04958602,  0.10479496,  0.09339446,\n",
       "        0.09391343,  0.07422109,  0.11759143,  0.10218369,  0.11759143,\n",
       "        0.01094201,  0.09342186,  0.05875794,  0.1243589 ,  0.09009225,\n",
       "        0.10648652,  0.05828609,  0.07533285,  0.10648652,  0.13988973,\n",
       "        0.09838464,  0.07490171,  0.10648652,  0.10648652,  0.0352743 ,\n",
       "        0.11759143,  0.03658042,  0.09391343,  0.05525224,  0.13316182,\n",
       "        0.07422109,  0.13988973,  0.05356203,  0.09342186,  0.05592586,\n",
       "        0.11759143,  0.06338619,  0.07490171,  0.05592586,  0.10648652,\n",
       "        0.10218369,  0.13988973,  0.07533285,  0.10479496,  0.09339446,\n",
       "        0.13316182,  0.05325864,  0.13988973,  0.13988973,  0.11759143])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anomaly_scores = iso_forest.decision_function(base_df[['Age']])\n",
    "# Scores closer to -1 = more anomalous\n",
    "# Scores closer to 0 = more normal\n",
    "anomaly_scores"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
