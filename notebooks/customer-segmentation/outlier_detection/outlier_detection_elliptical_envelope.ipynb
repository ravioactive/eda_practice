{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce9aec4",
   "metadata": {},
   "source": [
    "## üé≠ Elliptic Envelope: Robust Multivariate Outlier Detection\n",
    "\n",
    "### üìã Code Breakdown\n",
    "```python\n",
    "# Elliptic Envelope\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "elliptic_env = EllipticEnvelope(contamination=0.1)\n",
    "outliers = elliptic_env.fit_predict(base_df[['Age']])\n",
    "print(outliers)\n",
    "```\n",
    "\n",
    "**Line-by-line explanation:**\n",
    "1. **Import EllipticEnvelope** from sklearn covariance module (robust covariance estimation)\n",
    "2. **Create EllipticEnvelope instance** with 10% contamination expectation\n",
    "3. **Fit and predict** on Age column (returns 1 for normal, -1 for outliers)\n",
    "4. **Print binary classification** results\n",
    "\n",
    "### üìö Essential Documentation & Resources\n",
    "\n",
    "#### **Official Documentation:**\n",
    "- **[Scikit-learn EllipticEnvelope](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html)** - Official API reference\n",
    "- **[Scikit-learn Covariance Estimation](https://scikit-learn.org/stable/modules/covariance.html#robust-covariance)** - Comprehensive guide to robust covariance\n",
    "- **[Scikit-learn Novelty Detection](https://scikit-learn.org/stable/modules/outlier_detection.html#elliptic-envelope)** - Outlier detection overview\n",
    "\n",
    "#### **Theoretical Background:**\n",
    "- **[Minimum Covariance Determinant (MCD) Paper by Rousseeuw & Van Driessen (1999)](https://www.sciencedirect.com/science/article/pii/S0167947398001125)** - Foundational MCD algorithm\n",
    "- **[FastMCD Algorithm Paper](https://link.springer.com/article/10.1007/s001800050034)** - Efficient MCD implementation\n",
    "- **[Robust Statistics Overview](https://www.springer.com/gp/book/9780387488196)** - Comprehensive robust statistics textbook\n",
    "\n",
    "#### **Helpful Blogs & Tutorials:**\n",
    "- **[Towards Data Science: Robust Outlier Detection](https://towardsdatascience.com/outlier-detection-with-elliptic-envelope-e13c6e42e35d)**\n",
    "- **[Machine Learning Mastery: Elliptic Envelope Tutorial](https://machinelearningmastery.com/elliptic-envelope-for-outlier-detection/)**\n",
    "- **[Analytics Vidhya: Robust Covariance Methods](https://www.analyticsvidhya.com/blog/2021/04/detecting-outliers-using-elliptic-envelope/)**\n",
    "\n",
    "#### **Advanced Resources:**\n",
    "- **[Robust Covariance Estimation Comparison](https://ieeexplore.ieee.org/document/7837889)**\n",
    "- **[Multivariate Outlier Detection Survey](https://www.sciencedirect.com/science/article/pii/S0167947319301245)**\n",
    "\n",
    "### üîç How Elliptic Envelope Works\n",
    "\n",
    "#### **Core Algorithm Concept:**\n",
    "1. **Robust Covariance Estimation**: Uses Minimum Covariance Determinant (MCD) to estimate covariance matrix\n",
    "2. **Elliptic Boundary**: Defines an ellipse around the data's central region\n",
    "3. **Mahalanobis Distance**: Calculates distance from each point to the center using robust covariance\n",
    "4. **Outlier Classification**: Points outside the elliptic envelope are classified as outliers\n",
    "\n",
    "#### **Mathematical Foundation:**\n",
    "\n",
    "```python\n",
    "# Elliptic Envelope Algorithm Steps:\n",
    "\n",
    "# 1. Robust Center and Covariance Estimation (MCD):\n",
    "# - Find subset of h points (h ‚âà (n+p+1)/2) that minimizes covariance determinant\n",
    "# - Robust_center = mean of this subset\n",
    "# - Robust_covariance = covariance of this subset\n",
    "\n",
    "# 2. Mahalanobis Distance Calculation:\n",
    "# For each point x_i:\n",
    "# mahal_dist¬≤(x_i) = (x_i - robust_center)·µÄ √ó robust_covariance‚Åª¬π √ó (x_i - robust_center)\n",
    "\n",
    "# 3. Outlier Detection:\n",
    "# - Sort all Mahalanobis distances\n",
    "# - Set threshold based on contamination parameter\n",
    "# - Points with distance > threshold = outliers\n",
    "\n",
    "# 4. Chi-squared Distribution:\n",
    "# Under normal distribution assumption:\n",
    "# mahal_dist¬≤ ~ œá¬≤(p) where p = number of dimensions\n",
    "```\n",
    "\n",
    "#### **Visual Intuition:**\n",
    "- **Elliptic envelope**: Represents the boundary of \"normal\" data\n",
    "- **Robust center**: Center of the ellipse (resistant to outliers)\n",
    "- **Mahalanobis distance**: Distance accounting for data shape and correlation\n",
    "- **Outliers**: Points outside the elliptic boundary\n",
    "\n",
    "### üìä Output Interpretation\n",
    "\n",
    "Your output will be an array like: `[1, 1, -1, 1, 1, -1, ...]`\n",
    "\n",
    "**Interpretation:**\n",
    "- **`1`**: Normal point (inlier, inside elliptic envelope)\n",
    "- **`-1`**: Outlier (outside elliptic envelope)\n",
    "\n",
    "**Practical Usage:**\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enhanced analysis\n",
    "elliptic_env = EllipticEnvelope(contamination=0.1, support_fraction=None, random_state=42)\n",
    "predictions = elliptic_env.fit_predict(base_df[['Age']])\n",
    "\n",
    "# Get outlier details\n",
    "outlier_mask = predictions == -1\n",
    "outlier_indices = np.where(outlier_mask)[0]\n",
    "outlier_customers = base_df.iloc[outlier_indices]\n",
    "\n",
    "# Get Mahalanobis distances (confidence scores)\n",
    "mahal_distances = elliptic_env.mahalanobis(base_df[['Age']].values)\n",
    "\n",
    "# Get decision scores (negative distances)\n",
    "decision_scores = elliptic_env.decision_function(base_df[['Age']])\n",
    "\n",
    "print(f\"üìä Elliptic Envelope Analysis Results:\")\n",
    "print(f\"Total customers: {len(base_df)}\")\n",
    "print(f\"Detected outliers: {np.sum(outlier_mask)}\")\n",
    "print(f\"Outlier percentage: {np.sum(outlier_mask)/len(base_df)*100:.1f}%\")\n",
    "\n",
    "# Detailed outlier analysis\n",
    "if np.sum(outlier_mask) > 0:\n",
    "    outlier_analysis = pd.DataFrame({\n",
    "        'Customer_Index': outlier_indices,\n",
    "        'Age': base_df.iloc[outlier_indices]['Age'].values,\n",
    "        'Mahalanobis_Distance': mahal_distances[outlier_indices],\n",
    "        'Decision_Score': decision_scores[outlier_indices]\n",
    "    }).sort_values('Mahalanobis_Distance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüîç Top Outliers by Mahalanobis Distance:\")\n",
    "    print(outlier_analysis.head())\n",
    "    \n",
    "    # Robust statistics\n",
    "    robust_center = elliptic_env.location_\n",
    "    robust_covariance = elliptic_env.covariance_\n",
    "    \n",
    "    print(f\"\\nüìà Robust Statistics:\")\n",
    "    print(f\"Robust center (mean): {robust_center[0]:.2f}\")\n",
    "    print(f\"Robust covariance: {robust_covariance[0,0]:.2f}\")\n",
    "    \n",
    "    # Compare with non-robust statistics\n",
    "    regular_mean = base_df['Age'].mean()\n",
    "    regular_std = base_df['Age'].std()\n",
    "    \n",
    "    print(f\"\\nüìä Comparison with Regular Statistics:\")\n",
    "    print(f\"Regular mean: {regular_mean:.2f} vs Robust center: {robust_center[0]:.2f}\")\n",
    "    print(f\"Regular std: {regular_std:.2f} vs Robust std: {np.sqrt(robust_covariance[0,0]):.2f}\")\n",
    "```\n",
    "\n",
    "**Elliptic Envelope Specific Insights:**\n",
    "- **Robust center**: Less influenced by outliers than regular mean\n",
    "- **Mahalanobis distance**: Accounts for data shape and variability\n",
    "- **Support fraction**: Proportion of points used to compute robust estimates\n",
    "- **Chi-squared relationship**: Can convert distances to probability values\n",
    "\n",
    "### ‚öñÔ∏è Elliptic Envelope vs Other Outlier Detection Methods\n",
    "\n",
    "| **Method** | **Strengths** | **Weaknesses** | **Best Use Case** |\n",
    "|------------|---------------|----------------|-------------------|\n",
    "| **Standard Z-Score** | ‚úÖ Simple, fast<br/>‚úÖ Interpretable<br/>‚úÖ Universal | ‚ùå Sensitive to outliers<br/>‚ùå Assumes normality<br/>‚ùå Univariate only | Clean, normally distributed data |\n",
    "| **Modified Z-Score** | ‚úÖ Robust to outliers<br/>‚úÖ No normality assumption<br/>‚úÖ Interpretable | ‚ùå Still univariate<br/>‚ùå May miss multivariate patterns<br/>‚ùå Less efficient | Robust univariate outlier detection |\n",
    "| **Isolation Forest** | ‚úÖ Multivariate<br/>‚úÖ No assumptions<br/>‚úÖ Scalable<br/>‚úÖ Tree-based | ‚ùå Parameter sensitive<br/>‚ùå Black box<br/>‚ùå Poor with local outliers | Large datasets, global anomalies |\n",
    "| **Local Outlier Factor** | ‚úÖ Local outliers<br/>‚úÖ Density-aware<br/>‚úÖ Interpretable scores<br/>‚úÖ Handles clusters | ‚ùå Sensitive to k parameter<br/>‚ùå O(n¬≤) complexity<br/>‚ùå High-dimensional issues | Clustered data, local anomalies |\n",
    "| **DBSCAN** | ‚úÖ Clustering + outliers<br/>‚úÖ Arbitrary shapes<br/>‚úÖ No contamination needed<br/>‚úÖ Natural clusters | ‚ùå Very parameter sensitive<br/>‚ùå Varying densities issues<br/>‚ùå Parameter selection difficult | Unknown cluster structure |\n",
    "| **Elliptic Envelope** | ‚úÖ **Robust to outliers**<br/>‚úÖ **Multivariate**<br/>‚úÖ **Statistical foundation**<br/>‚úÖ **Handles correlations**<br/>‚úÖ **Confidence intervals** | ‚ùå **Assumes elliptical distribution**<br/>‚ùå **Struggles with non-Gaussian data**<br/>‚ùå **Poor with multiple clusters**<br/>‚ùå **High-dimensional curse** | **Gaussian-like data, correlated features** |\n",
    "\n",
    "### üéØ Detailed Comparison\n",
    "\n",
    "#### **Elliptic Envelope Unique Strengths:**\n",
    "1. **Robust Statistics**: Uses MCD estimator, resistant to up to 50% outliers\n",
    "2. **Multivariate Correlation**: Accounts for relationships between features\n",
    "3. **Statistical Foundation**: Based on well-established robust statistics theory\n",
    "4. **Confidence Measures**: Provides Mahalanobis distances with statistical meaning\n",
    "5. **Automatic Threshold**: Uses contamination parameter with statistical backing\n",
    "6. **Handles Correlated Data**: Naturally deals with feature correlations\n",
    "\n",
    "#### **Elliptic Envelope Weaknesses:**\n",
    "1. **Gaussian Assumption**: Works best when data follows elliptical/normal distribution\n",
    "2. **Single Cluster Assumption**: Assumes data comes from one population\n",
    "3. **Curse of Dimensionality**: Performance degrades with many features\n",
    "4. **Computational Complexity**: MCD algorithm can be expensive for large datasets\n",
    "5. **Outlier Influence During Fitting**: May be affected by extreme outliers during initial fit\n",
    "6. **Linear Boundaries**: Creates elliptical boundaries, not arbitrary shapes\n",
    "\n",
    "### üöÄ Advanced Usage and Parameter Tuning\n",
    "\n",
    "#### **Parameter Selection Strategies:**\n",
    "\n",
    "```python\n",
    "def optimize_elliptic_envelope_parameters(X, contamination_range=None, support_fraction_range=None):\n",
    "    \"\"\"\n",
    "    Optimize EllipticEnvelope parameters using multiple criteria\n",
    "    \"\"\"\n",
    "    \n",
    "    if contamination_range is None:\n",
    "        contamination_range = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "    \n",
    "    if support_fraction_range is None:\n",
    "        # support_fraction controls how many points are used for robust estimation\n",
    "        support_fraction_range = [None, 0.8, 0.7, 0.6]  # None = automatic\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for contamination in contamination_range:\n",
    "        for support_fraction in support_fraction_range:\n",
    "            try:\n",
    "                # Fit Elliptic Envelope\n",
    "                elliptic_env = EllipticEnvelope(\n",
    "                    contamination=contamination,\n",
    "                    support_fraction=support_fraction,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                predictions = elliptic_env.fit_predict(X)\n",
    "                mahal_distances = elliptic_env.mahalanobis(X)\n",
    "                decision_scores = elliptic_env.decision_function(X)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                n_outliers = np.sum(predictions == -1)\n",
    "                outlier_ratio = n_outliers / len(X)\n",
    "                \n",
    "                # Statistical consistency check\n",
    "                robust_center = elliptic_env.location_\n",
    "                robust_cov = elliptic_env.covariance_\n",
    "                \n",
    "                # Compare with expected chi-squared distribution\n",
    "                # For p dimensions, chi-squared with p degrees of freedom\n",
    "                p = X.shape[1]\n",
    "                from scipy.stats import chi2\n",
    "                \n",
    "                # Expected number of outliers at different confidence levels\n",
    "                conf_95 = chi2.ppf(0.95, p)  # 95% confidence interval\n",
    "                expected_outliers_95 = np.sum(mahal_distances > conf_95) / len(X)\n",
    "                \n",
    "                # Stability measure: how well does it match theoretical expectations\n",
    "                theoretical_match = abs(outlier_ratio - (1 - 0.95))  # How close to 5% for 95% confidence\n",
    "                \n",
    "                # Robust statistics quality\n",
    "                regular_mean = np.mean(X, axis=0)\n",
    "                regular_cov = np.cov(X.T)\n",
    "                \n",
    "                center_stability = np.linalg.norm(robust_center - regular_mean)\n",
    "                cov_stability = np.linalg.norm(robust_cov - regular_cov, 'fro')\n",
    "                \n",
    "                results.append({\n",
    "                    'contamination': contamination,\n",
    "                    'support_fraction': support_fraction,\n",
    "                    'n_outliers': n_outliers,\n",
    "                    'outlier_ratio': outlier_ratio,\n",
    "                    'theoretical_match': theoretical_match,\n",
    "                    'center_stability': center_stability,\n",
    "                    'cov_stability': cov_stability,\n",
    "                    'mean_mahal_distance': np.mean(mahal_distances),\n",
    "                    'std_mahal_distance': np.std(mahal_distances),\n",
    "                    'expected_outliers_95': expected_outliers_95\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with contamination={contamination}, support_fraction={support_fraction}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Rank results\n",
    "    if len(results_df) > 0:\n",
    "        # Prefer results close to expected contamination and good theoretical match\n",
    "        results_df['quality_score'] = (\n",
    "            1 / (results_df['theoretical_match'] + 0.01) +  # Better theoretical match\n",
    "            1 / (abs(results_df['outlier_ratio'] - results_df['contamination']) + 0.01) +  # Match expected contamination\n",
    "            1 / (results_df['center_stability'] + 0.01)  # More stable center\n",
    "        )\n",
    "        \n",
    "        best_params = results_df.loc[results_df['quality_score'].idxmax()]\n",
    "        \n",
    "        print(\"üèÜ Best EllipticEnvelope Parameters:\")\n",
    "        print(f\"contamination: {best_params['contamination']}\")\n",
    "        print(f\"support_fraction: {best_params['support_fraction']}\")\n",
    "        print(f\"Detected outliers: {int(best_params['n_outliers'])} ({best_params['outlier_ratio']*100:.1f}%)\")\n",
    "        print(f\"Theoretical match score: {best_params['theoretical_match']:.4f}\")\n",
    "        \n",
    "        return best_params, results_df\n",
    "    else:\n",
    "        print(\"‚ùå No valid parameter combinations found\")\n",
    "        return None, pd.DataFrame()\n",
    "\n",
    "# Apply parameter optimization\n",
    "print(\"üîß Optimizing EllipticEnvelope parameters...\")\n",
    "best_params, param_results = optimize_elliptic_envelope_parameters(base_df[['Age']].values)\n",
    "\n",
    "if best_params is not None:\n",
    "    # Create optimal model\n",
    "    optimal_elliptic_env = EllipticEnvelope(\n",
    "        contamination=best_params['contamination'],\n",
    "        support_fraction=best_params['support_fraction'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    optimal_predictions = optimal_elliptic_env.fit_predict(base_df[['Age']].values)\n",
    "    optimal_distances = optimal_elliptic_env.mahalanobis(base_df[['Age']].values)\n",
    "```\n",
    "\n",
    "#### **Diagnostic and Validation Methods:**\n",
    "\n",
    "```python\n",
    "def elliptic_envelope_diagnostics(X, elliptic_env, feature_names=None):\n",
    "    \"\"\"\n",
    "    Comprehensive diagnostics for EllipticEnvelope results\n",
    "    \"\"\"\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    # Get predictions and distances\n",
    "    predictions = elliptic_env.predict(X)\n",
    "    mahal_distances = elliptic_env.mahalanobis(X)\n",
    "    decision_scores = elliptic_env.decision_function(X)\n",
    "    \n",
    "    # Robust statistics\n",
    "    robust_center = elliptic_env.location_\n",
    "    robust_cov = elliptic_env.covariance_\n",
    "    \n",
    "    diagnostics = {\n",
    "        'basic_stats': {\n",
    "            'n_outliers': np.sum(predictions == -1),\n",
    "            'outlier_ratio': np.sum(predictions == -1) / len(X),\n",
    "            'robust_center': robust_center,\n",
    "            'robust_covariance': robust_cov\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Statistical validation using chi-squared distribution\n",
    "    p = X.shape[1]  # Number of dimensions\n",
    "    from scipy.stats import chi2\n",
    "    \n",
    "    # Theoretical vs observed outliers at different confidence levels\n",
    "    confidence_levels = [0.90, 0.95, 0.99]\n",
    "    theoretical_validation = {}\n",
    "    \n",
    "    for conf in confidence_levels:\n",
    "        threshold = chi2.ppf(conf, p)\n",
    "        observed_outliers = np.sum(mahal_distances > threshold)\n",
    "        expected_outliers = len(X) * (1 - conf)\n",
    "        \n",
    "        theoretical_validation[f'conf_{int(conf*100)}'] = {\n",
    "            'threshold': threshold,\n",
    "            'observed': observed_outliers,\n",
    "            'expected': expected_outliers,\n",
    "            'ratio': observed_outliers / expected_outliers if expected_outliers > 0 else float('inf')\n",
    "        }\n",
    "    \n",
    "    diagnostics['theoretical_validation'] = theoretical_validation\n",
    "    \n",
    "    # Robustness analysis\n",
    "    regular_mean = np.mean(X, axis=0)\n",
    "    regular_cov = np.cov(X.T)\n",
    "    \n",
    "    robustness_metrics = {\n",
    "        'center_shift': np.linalg.norm(robust_center - regular_mean),\n",
    "        'covariance_change': np.linalg.norm(robust_cov - regular_cov, 'fro'),\n",
    "        'center_shift_relative': np.linalg.norm(robust_center - regular_mean) / np.linalg.norm(regular_mean),\n",
    "        'robust_vs_regular_std': np.sqrt(np.diag(robust_cov)) / np.sqrt(np.diag(regular_cov))\n",
    "    }\n",
    "    \n",
    "    diagnostics['robustness'] = robustness_metrics\n",
    "    \n",
    "    # Outlier characteristics\n",
    "    if np.sum(predictions == -1) > 0:\n",
    "        outlier_indices = np.where(predictions == -1)[0]\n",
    "        outlier_data = X[outlier_indices]\n",
    "        \n",
    "        outlier_analysis = {\n",
    "            'outlier_distances': mahal_distances[outlier_indices],\n",
    "            'min_distance': np.min(mahal_distances[outlier_indices]),\n",
    "            'max_distance': np.max(mahal_distances[outlier_indices]),\n",
    "            'mean_distance': np.mean(mahal_distances[outlier_indices]),\n",
    "            'outlier_feature_stats': {}\n",
    "        }\n",
    "        \n",
    "        # Feature-wise analysis of outliers\n",
    "        for i, feature_name in enumerate(feature_names):\n",
    "            outlier_values = outlier_data[:, i]\n",
    "            normal_values = X[predictions == 1, i]\n",
    "            \n",
    "            outlier_analysis['outlier_feature_stats'][feature_name] = {\n",
    "                'outlier_mean': np.mean(outlier_values),\n",
    "                'normal_mean': np.mean(normal_values),\n",
    "                'outlier_std': np.std(outlier_values),\n",
    "                'normal_std': np.std(normal_values),\n",
    "                'separation': abs(np.mean(outlier_values) - np.mean(normal_values))\n",
    "            }\n",
    "        \n",
    "        diagnostics['outlier_analysis'] = outlier_analysis\n",
    "    \n",
    "    return diagnostics\n",
    "\n",
    "# Apply diagnostics to optimal model\n",
    "if best_params is not None:\n",
    "    diagnostics = elliptic_envelope_diagnostics(\n",
    "        base_df[['Age']].values, \n",
    "        optimal_elliptic_env,\n",
    "        ['Age']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä EllipticEnvelope Diagnostics:\")\n",
    "    print(f\"Outliers detected: {diagnostics['basic_stats']['n_outliers']} ({diagnostics['basic_stats']['outlier_ratio']*100:.1f}%)\")\n",
    "    print(f\"Robust center: {diagnostics['basic_stats']['robust_center'][0]:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìà Statistical Validation:\")\n",
    "    for conf_level, validation in diagnostics['theoretical_validation'].items():\n",
    "        print(f\"  {conf_level}: {validation['observed']} observed vs {validation['expected']:.1f} expected (ratio: {validation['ratio']:.2f})\")\n",
    "    \n",
    "    print(f\"\\nüõ°Ô∏è Robustness Metrics:\")\n",
    "    print(f\"  Center shift: {diagnostics['robustness']['center_shift']:.3f}\")\n",
    "    print(f\"  Relative center shift: {diagnostics['robustness']['center_shift_relative']:.3f}\")\n",
    "    \n",
    "    if 'outlier_analysis' in diagnostics:\n",
    "        print(f\"\\nüîç Outlier Analysis:\")\n",
    "        outlier_stats = diagnostics['outlier_analysis']['outlier_feature_stats']['Age']\n",
    "        print(f\"  Outlier ages: mean={outlier_stats['outlier_mean']:.1f}, std={outlier_stats['outlier_std']:.1f}\")\n",
    "        print(f\"  Normal ages: mean={outlier_stats['normal_mean']:.1f}, std={outlier_stats['normal_std']:.1f}\")\n",
    "        print(f\"  Separation: {outlier_stats['separation']:.1f} years\")\n",
    "```\n",
    "\n",
    "### üéØ When to Use Elliptic Envelope\n",
    "\n",
    "**‚úÖ Use Elliptic Envelope when:**\n",
    "- **Gaussian-like data** - data approximately follows normal/elliptical distribution\n",
    "- **Correlated features** - need to account for feature relationships\n",
    "- **Robust statistics needed** - want outlier-resistant estimates\n",
    "- **Statistical interpretation** - need confidence intervals and statistical meaning\n",
    "- **Moderate dimensionality** - works well with 2-20 features\n",
    "- **Single population** - data comes from one homogeneous group\n",
    "\n",
    "**‚ùå Don't use Elliptic Envelope when:**\n",
    "- **Highly non-Gaussian data** - skewed, multimodal, or arbitrary distributions\n",
    "- **Multiple clusters** - data has distinct subgroups\n",
    "- **High-dimensional data** - >50 features (curse of dimensionality)\n",
    "- **Very large datasets** - MCD algorithm becomes computationally expensive\n",
    "- **Categorical features** - designed for continuous numerical data\n",
    "- **Local outlier detection needed** - focuses on global outliers only\n",
    "\n",
    "### üèÜ Recommendation for Your Customer Segmentation\n",
    "\n",
    "For customer segmentation analysis, **Elliptic Envelope is moderately suitable** because:\n",
    "\n",
    "**Advantages:**\n",
    "1. **Age correlation analysis**: Can detect unusual age patterns if using multiple related features\n",
    "2. **Robust statistics**: Less influenced by extreme ages\n",
    "3. **Statistical confidence**: Provides meaningful distance measures\n",
    "4. **Customer profiling**: Good for identifying customers with unusual overall profiles\n",
    "\n",
    "**Limitations:**\n",
    "1. **Single feature**: With only Age, benefits are limited compared to multivariate methods\n",
    "2. **Customer diversity**: Customer ages may not follow single Gaussian distribution\n",
    "3. **Multiple segments**: Customers naturally form different age groups\n",
    "\n",
    "**Optimal implementation for your case:**\n",
    "\n",
    "```python\n",
    "# Recommended EllipticEnvelope setup for customer analysis\n",
    "def customer_elliptic_envelope_analysis(customer_data, features=['Age']):\n",
    "    \"\"\"\n",
    "    EllipticEnvelope optimized for customer data analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üéØ Customer Analysis with EllipticEnvelope\")\n",
    "    \n",
    "    # Conservative contamination for business data\n",
    "    contamination = 0.08  # 8% outliers expected\n",
    "    \n",
    "    # Use higher support fraction for more robust estimates\n",
    "    support_fraction = 0.7  # Use 70% of data for robust estimation\n",
    "    \n",
    "    elliptic_env = EllipticEnvelope(\n",
    "        contamination=contamination,\n",
    "        support_fraction=support_fraction,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    X = customer_data[features].values\n",
    "    predictions = elliptic_env.fit_predict(X)\n",
    "    mahal_distances = elliptic_env.mahalanobis(X)\n",
    "    \n",
    "    # Analysis results\n",
    "    outlier_mask = predictions == -1\n",
    "    n_outliers = np.sum(outlier_mask)\n",
    "    \n",
    "    print(f\"üìä Results:\")\n",
    "    print(f\"Total customers: {len(customer_data)}\")\n",
    "    print(f\"Outlier customers: {n_outliers} ({n_outliers/len(customer_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Robust vs regular statistics\n",
    "    robust_center = elliptic_env.location_\n",
    "    regular_mean = np.mean(X, axis=0)\n",
    "    \n",
    "    for i, feature in enumerate(features):\n",
    "        print(f\"\\n{feature} Analysis:\")\n",
    "        print(f\"  Regular mean: {regular_mean[i]:.1f}\")\n",
    "        print(f\"  Robust center: {robust_center[i]:.1f}\")\n",
    "        print(f\"  Difference: {abs(robust_center[i] - regular_mean[i]):.1f}\")\n",
    "    \n",
    "    # Outlier details\n",
    "    if n_outliers > 0:\n",
    "        outlier_customers = customer_data[outlier_mask]\n",
    "        outlier_distances = mahal_distances[outlier_mask]\n",
    "        \n",
    "        outlier_summary = pd.DataFrame({\n",
    "            'Customer_Index': outlier_customers.index,\n",
    "            'Mahalanobis_Distance': outlier_distances\n",
    "        })\n",
    "        \n",
    "        for feature in features:\n",
    "            outlier_summary[feature] = outlier_customers[feature].values\n",
    "        \n",
    "        outlier_summary = outlier_summary.sort_values('Mahalanobis_Distance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüîç Top Outlier Customers:\")\n",
    "        print(outlier_summary.head())\n",
    "        \n",
    "        # Business insights\n",
    "        if 'Age' in features:\n",
    "            outlier_ages = outlier_customers['Age']\n",
    "            print(f\"\\nOutlier Age Statistics:\")\n",
    "            print(f\"  Age range: {outlier_ages.min():.0f} - {outlier_ages.max():.0f}\")\n",
    "            print(f\"  Average age: {outlier_ages.mean():.1f} ¬± {outlier_ages.std():.1f}\")\n",
    "            print(f\"  These customers may represent:\")\n",
    "            print(f\"    - Very young or very old customer segments\")\n",
    "            print(f\"    - Data entry errors\")\n",
    "            print(f\"    - Special customer categories\")\n",
    "    \n",
    "    return elliptic_env, predictions, mahal_distances\n",
    "\n",
    "# Apply to your customer data\n",
    "customer_elliptic_env, customer_predictions, customer_distances = customer_elliptic_envelope_analysis(base_df)\n",
    "```\n",
    "\n",
    "### üéØ Summary: Elliptic Envelope in Your Outlier Detection Arsenal\n",
    "\n",
    "**Perfect Complementary Approach:**\n",
    "1. **Z-Score**: Global statistical outliers (univariate)\n",
    "2. **Modified Z-Score**: Robust global outliers (univariate)\n",
    "3. **Isolation Forest**: Multivariate global anomalies (tree-based)\n",
    "4. **LOF**: Local density-based outliers (neighborhood-based)\n",
    "5. **DBSCAN**: Clustering-based outliers (density-based)\n",
    "6. **Elliptic Envelope**: **Robust multivariate outliers (statistical)**\n",
    "\n",
    "**Use Elliptic Envelope specifically when you want to:**\n",
    "- **Robust multivariate analysis** with statistical foundation\n",
    "- **Account for feature correlations** in outlier detection\n",
    "- **Get confidence measures** (Mahalanobis distances)\n",
    "- **Resist outlier contamination** during model fitting\n",
    "- **Statistical interpretation** of results\n",
    "\n",
    "**Key Advantages for Customer Analysis:**\n",
    "- **Robust to data quality issues** (resistant to bad data points)\n",
    "- **Multivariate capability** (when you add income, spending, etc.)\n",
    "- **Statistical confidence** (can convert distances to probabilities)\n",
    "- **Professional statistical foundation** (based on well-established theory)\n",
    "\n",
    "**Best Use Case**: When you expand to multivariate customer analysis (Age + Income + Spending), Elliptic Envelope becomes much more powerful as it can detect customers who are unusual in their **combination** of characteristics, even if each individual characteristic seems normal! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb15f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1 -1  1  1 -1  1 -1  1 -1  1  1 -1  1  1 -1  1\n",
      " -1 -1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1 -1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1  1  1  1 -1  1 -1 -1 -1  1  1  1  1  1 -1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "# Elliptic Envelope\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "elliptic_env = EllipticEnvelope(contamination=0.1)\n",
    "outliers = elliptic_env.fit_predict(base_df[['Age']])\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9406bf",
   "metadata": {},
   "source": [
    "## üéØ Elliptic Envelope Parameter Tuning: Corrected Guide\n",
    "\n",
    "### üìä Core Parameters and Data-Driven Selection\n",
    "\n",
    "#### **1. `contamination` - Expected Outlier Proportion**\n",
    "\n",
    "**What it controls:** The proportion of outliers in the dataset\n",
    "\n",
    "**Data-driven selection methods:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def estimate_contamination_for_elliptic_envelope(X, methods=['iqr', 'zscore', 'chi2_test']):\n",
    "    \"\"\"\n",
    "    Estimate contamination using multiple statistical methods\n",
    "    \"\"\"\n",
    "    \n",
    "    contamination_estimates = {}\n",
    "    \n",
    "    for col_idx in range(X.shape[1]):\n",
    "        data = X[:, col_idx]\n",
    "        col_name = f'feature_{col_idx}'\n",
    "        \n",
    "        # Method 1: IQR-based estimation\n",
    "        if 'iqr' in methods:\n",
    "            Q1 = np.percentile(data, 25)\n",
    "            Q3 = np.percentile(data, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            iqr_outliers = np.sum((data < lower_bound) | (data > upper_bound))\n",
    "            contamination_estimates[f'{col_name}_iqr'] = iqr_outliers / len(data)\n",
    "        \n",
    "        # Method 2: Z-score based\n",
    "        if 'zscore' in methods:\n",
    "            z_scores = np.abs(stats.zscore(data))\n",
    "            zscore_outliers = np.sum(z_scores > 2.5)  # More conservative than 3\n",
    "            contamination_estimates[f'{col_name}_zscore'] = zscore_outliers / len(data)\n",
    "        \n",
    "        # Method 3: Chi-squared test for normality\n",
    "        if 'chi2_test' in methods and len(data) > 50:\n",
    "            from scipy.stats import normaltest\n",
    "            stat, p_value = normaltest(data)\n",
    "            \n",
    "            if p_value > 0.05:  # Data appears normal\n",
    "                contamination_estimates[f'{col_name}_chi2_normal'] = 0.05\n",
    "            else:\n",
    "                contamination_estimates[f'{col_name}_chi2_nonnormal'] = 0.1\n",
    "    \n",
    "    # Multivariate estimation using Mahalanobis distance\n",
    "    if X.shape[1] > 1:\n",
    "        try:\n",
    "            mean = np.mean(X, axis=0)\n",
    "            cov = np.cov(X.T)\n",
    "            inv_cov = np.linalg.pinv(cov)\n",
    "            mahal_distances = []\n",
    "            for i in range(len(X)):\n",
    "                diff = X[i] - mean\n",
    "                mahal_dist = np.sqrt(diff.T @ inv_cov @ diff)\n",
    "                mahal_distances.append(mahal_dist)\n",
    "            \n",
    "            mahal_distances = np.array(mahal_distances)\n",
    "            p = X.shape[1]\n",
    "            chi2_95 = stats.chi2.ppf(0.95, p)\n",
    "            multivariate_outliers = np.sum(mahal_distances**2 > chi2_95)\n",
    "            contamination_estimates['multivariate_mahal'] = multivariate_outliers / len(X)\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            pass  # Skip if covariance is singular\n",
    "    \n",
    "    # Calculate statistics\n",
    "    estimates = list(contamination_estimates.values())\n",
    "    conservative_estimate = min(estimates) if estimates else 0.05\n",
    "    liberal_estimate = max(estimates) if estimates else 0.15\n",
    "    median_estimate = np.median(estimates) if estimates else 0.1\n",
    "    \n",
    "    print(\"Contamination Estimates:\")\n",
    "    for method, estimate in contamination_estimates.items():\n",
    "        print(f\"  {method}: {estimate:.3f}\")\n",
    "    \n",
    "    print(f\"\\nConservative estimate: {conservative_estimate:.3f}\")\n",
    "    print(f\"Liberal estimate: {liberal_estimate:.3f}\")\n",
    "    print(f\"Median estimate: {median_estimate:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'conservative': conservative_estimate,\n",
    "        'liberal': liberal_estimate,\n",
    "        'median': median_estimate,\n",
    "        'all_estimates': contamination_estimates\n",
    "    }\n",
    "\n",
    "# Apply contamination estimation\n",
    "contamination_analysis = estimate_contamination_for_elliptic_envelope(base_df[['Age']].values)\n",
    "```\n",
    "\n",
    "**Heuristic Rules for Contamination:**\n",
    "- **Financial data**: 1-5% (fraud, errors)\n",
    "- **Customer data**: 5-15% (unusual behavior)  \n",
    "- **Sensor data**: 2-8% (equipment issues)\n",
    "- **Medical data**: 1-10% (rare conditions)\n",
    "- **Survey data**: 5-20% (response errors)\n",
    "\n",
    "#### **2. `support_fraction` - Robustness Control**\n",
    "\n",
    "**What it controls:** Fraction of data used to compute robust covariance estimate\n",
    "\n",
    "```python\n",
    "def suggest_support_fraction(X, data_quality='medium'):\n",
    "    \"\"\"\n",
    "    Suggest support_fraction based on data characteristics\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    suggestions = {}\n",
    "    \n",
    "    # Rule 1: MCD theoretical minimum\n",
    "    theoretical_min = (n_samples + n_features + 1) / (2 * n_samples)\n",
    "    suggestions['theoretical_minimum'] = max(0.5, theoretical_min)\n",
    "    \n",
    "    # Rule 2: Sample size considerations\n",
    "    if n_samples < 100:\n",
    "        suggestions['small_sample'] = 0.8\n",
    "    elif n_samples < 500:\n",
    "        suggestions['medium_sample'] = 0.7\n",
    "    else:\n",
    "        suggestions['large_sample'] = 0.6\n",
    "    \n",
    "    # Rule 3: Data quality adjustment\n",
    "    quality_adjustments = {\n",
    "        'high': 0.8, 'medium': 0.7, 'low': 0.6, 'unknown': 0.65\n",
    "    }\n",
    "    suggestions['data_quality'] = quality_adjustments.get(data_quality, 0.65)\n",
    "    \n",
    "    # Rule 4: Dimensionality considerations\n",
    "    if n_features == 1:\n",
    "        suggestions['dimensionality'] = 0.75\n",
    "    elif n_features <= 5:\n",
    "        suggestions['dimensionality'] = 0.7\n",
    "    elif n_features <= 20:\n",
    "        suggestions['dimensionality'] = 0.65\n",
    "    else:\n",
    "        suggestions['dimensionality'] = 0.6\n",
    "    \n",
    "    final_suggestion = max(0.5, np.median(list(suggestions.values())))\n",
    "    \n",
    "    print(\"Support fraction suggestions:\")\n",
    "    for method, value in suggestions.items():\n",
    "        print(f\"  {method}: {value:.3f}\")\n",
    "    print(f\"Final recommendation: {final_suggestion:.3f}\")\n",
    "    \n",
    "    return final_suggestion, suggestions\n",
    "\n",
    "# Apply support fraction analysis\n",
    "optimal_support_fraction, support_analysis = suggest_support_fraction(\n",
    "    base_df[['Age']].values, \n",
    "    data_quality='medium'\n",
    ")\n",
    "```\n",
    "### üß™ Elliptic Envelope Validation Methodologies\n",
    "\n",
    "#### **1. Statistical Consistency Validation**\n",
    "\n",
    "```python\n",
    "def validate_elliptic_envelope_statistical_consistency(X, elliptic_env):\n",
    "    \"\"\"\n",
    "    Validate Elliptic Envelope using statistical consistency checks\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = elliptic_env.predict(X)\n",
    "    mahal_distances = elliptic_env.mahalanobis(X)\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    n_outliers = np.sum(predictions == -1)\n",
    "    outlier_ratio = n_outliers / n_samples\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # Test 1: Chi-squared distribution consistency\n",
    "    # Mahalanobis distances should follow chi-squared distribution with p degrees of freedom\n",
    "    \n",
    "    # Theoretical expectations at different confidence levels\n",
    "    confidence_levels = [0.90, 0.95, 0.99]\n",
    "    chi2_validation = {}\n",
    "    \n",
    "    for conf in confidence_levels:\n",
    "        theoretical_threshold = stats.chi2.ppf(conf, n_features)\n",
    "        observed_beyond_threshold = np.sum(mahal_distances**2 > theoretical_threshold)\n",
    "        expected_beyond_threshold = n_samples * (1 - conf)\n",
    "        \n",
    "        # Calculate relative error\n",
    "        if expected_beyond_threshold > 0:\n",
    "            relative_error = abs(observed_beyond_threshold - expected_beyond_threshold) / expected_beyond_threshold\n",
    "        else:\n",
    "            relative_error = float('inf') if observed_beyond_threshold > 0 else 0\n",
    "        \n",
    "        chi2_validation[f'conf_{int(conf*100)}'] = {\n",
    "            'threshold': theoretical_threshold,\n",
    "            'observed': observed_beyond_threshold,\n",
    "            'expected': expected_beyond_threshold,\n",
    "            'relative_error': relative_error,\n",
    "            'acceptable': relative_error < 0.3  # 30% tolerance\n",
    "        }\n",
    "    \n",
    "    validation_results['chi2_consistency'] = chi2_validation\n",
    "    \n",
    "    # Test 2: Contamination consistency\n",
    "    expected_contamination = elliptic_env.contamination\n",
    "    actual_contamination = outlier_ratio\n",
    "    contamination_error = abs(actual_contamination - expected_contamination)\n",
    "    \n",
    "    validation_results['contamination_consistency'] = {\n",
    "        'expected': expected_contamination,\n",
    "        'actual': actual_contamination,\n",
    "        'error': contamination_error,\n",
    "        'acceptable': contamination_error < 0.03  # 3% tolerance\n",
    "    }\n",
    "    \n",
    "    # Test 3: Robust statistics quality\n",
    "    robust_center = elliptic_env.location_\n",
    "    robust_cov = elliptic_env.covariance_\n",
    "    \n",
    "    # Compare with regular statistics\n",
    "    regular_mean = np.mean(X, axis=0)\n",
    "    regular_cov = np.cov(X.T)\n",
    "    \n",
    "    center_shift = np.linalg.norm(robust_center - regular_mean)\n",
    "    cov_frobenius_diff = np.linalg.norm(robust_cov - regular_cov, 'fro')\n",
    "    \n",
    "    # Normalize by data scale\n",
    "    data_scale = np.linalg.norm(np.std(X, axis=0))\n",
    "    relative_center_shift = center_shift / data_scale if data_scale > 0 else 0\n",
    "    \n",
    "    validation_results['robustness_quality'] = {\n",
    "        'center_shift': center_shift,\n",
    "        'relative_center_shift': relative_center_shift,\n",
    "        'covariance_difference': cov_frobenius_diff,\n",
    "        'center_shift_acceptable': relative_center_shift < 0.2,  # 20% of data scale\n",
    "        'sufficient_robustness': relative_center_shift > 0.01   # Some difference expected\n",
    "    }\n",
    "    \n",
    "    # Test 4: Outlier distance distribution\n",
    "    outlier_indices = predictions == -1\n",
    "    if np.any(outlier_indices):\n",
    "        outlier_distances = mahal_distances[outlier_indices]\n",
    "        normal_distances = mahal_distances[~outlier_indices]\n",
    "        \n",
    "        # Outliers should have significantly higher distances\n",
    "        if len(normal_distances) > 0:\n",
    "            distance_separation = np.mean(outlier_distances) - np.mean(normal_distances)\n",
    "            relative_separation = distance_separation / np.mean(normal_distances)\n",
    "        else:\n",
    "            distance_separation = np.mean(outlier_distances)\n",
    "            relative_separation = float('inf')\n",
    "        \n",
    "        validation_results['distance_separation'] = {\n",
    "            'mean_outlier_distance': np.mean(outlier_distances),\n",
    "            'mean_normal_distance': np.mean(normal_distances) if len(normal_distances) > 0 else 0,\n",
    "            'separation': distance_separation,\n",
    "            'relative_separation': relative_separation,\n",
    "            'good_separation': relative_separation > 0.5  # 50% higher distances for outliers\n",
    "        }\n",
    "    \n",
    "    # Overall validation score\n",
    "    passed_tests = 0\n",
    "    total_tests = 0\n",
    "    \n",
    "    for test_result in validation_results.values():\n",
    "        if isinstance(test_result, dict):\n",
    "            if 'acceptable' in test_result:\n",
    "                total_tests += 1\n",
    "                if test_result['acceptable']:\n",
    "                    passed_tests += 1\n",
    "            else:\n",
    "                # Count sub-tests\n",
    "                for sub_test in test_result.values():\n",
    "                    if isinstance(sub_test, dict) and 'acceptable' in sub_test:\n",
    "                        total_tests += 1\n",
    "                        if sub_test['acceptable']:\n",
    "                            passed_tests += 1\n",
    "    \n",
    "    overall_score = passed_tests / total_tests if total_tests > 0 else 0\n",
    "    validation_results['overall_validation_score'] = overall_score\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def print_validation_results(validation_results):\n",
    "    \"\"\"Print validation results in a readable format\"\"\"\n",
    "    \n",
    "    print(\"üß™ Elliptic Envelope Validation Results:\")\n",
    "    print(f\"Overall validation score: {validation_results['overall_validation_score']:.2f}\")\n",
    "    \n",
    "    # Chi-squared consistency\n",
    "    print(f\"\\nüìä Chi-squared Distribution Consistency:\")\n",
    "    chi2_results = validation_results['chi2_consistency']\n",
    "    for conf_level, result in chi2_results.items():\n",
    "        status = \"‚úÖ\" if result['acceptable'] else \"‚ùå\"\n",
    "        print(f\"  {status} {conf_level}: {result['observed']} observed vs {result['expected']:.1f} expected (error: {result['relative_error']:.2f})\")\n",
    "    \n",
    "    # Contamination consistency\n",
    "    print(f\"\\nüéØ Contamination Consistency:\")\n",
    "    cont_result = validation_results['contamination_consistency']\n",
    "    status = \"‚úÖ\" if cont_result['acceptable'] else \"‚ùå\"\n",
    "    print(f\"  {status} Expected: {cont_result['expected']:.3f}, Actual: {cont_result['actual']:.3f} (error: {cont_result['error']:.3f})\")\n",
    "    \n",
    "    # Robustness quality\n",
    "    print(f\"\\nüõ°Ô∏è Robustness Quality:\")\n",
    "    robust_result = validation_results['robustness_quality']\n",
    "    center_status = \"‚úÖ\" if robust_result['center_shift_acceptable'] else \"‚ùå\"\n",
    "    robust_status = \"‚úÖ\" if robust_result['sufficient_robustness'] else \"‚ö†Ô∏è\"\n",
    "    print(f\"  {center_status} Center shift acceptable: {robust_result['relative_center_shift']:.3f}\")\n",
    "    print(f\"  {robust_status} Sufficient robustness: {robust_result['sufficient_robustness']}\")\n",
    "    \n",
    "    # Distance separation\n",
    "    if 'distance_separation' in validation_results:\n",
    "        print(f\"\\nüìè Distance Separation:\")\n",
    "        dist_result = validation_results['distance_separation']\n",
    "        sep_status = \"‚úÖ\" if dist_result['good_separation'] else \"‚ùå\"\n",
    "        print(f\"  {sep_status} Outlier-normal separation: {dist_result['relative_separation']:.2f}\")\n",
    "\n",
    "# Apply statistical validation\n",
    "test_elliptic_env = EllipticEnvelope(\n",
    "    contamination=contamination_analysis['median'],\n",
    "    support_fraction=optimal_support_fraction,\n",
    "    random_state=42\n",
    ")\n",
    "test_elliptic_env.fit(base_df[['Age']].values)\n",
    "\n",
    "validation_results = validate_elliptic_envelope_statistical_consistency(\n",
    "    base_df[['Age']].values, \n",
    "    test_elliptic_env\n",
    ")\n",
    "\n",
    "print_validation_results(validation_results)\n",
    "```\n",
    "\n",
    "#### **2. Cross-Validation for Robust Methods**\n",
    "\n",
    "```python\n",
    "def elliptic_envelope_cross_validation(X, contamination_range, support_fraction_range, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Cross-validation for Elliptic Envelope parameter selection\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.model_selection import KFold\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for contamination in contamination_range:\n",
    "        for support_fraction in support_fraction_range:\n",
    "            fold_results = []\n",
    "            \n",
    "            kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X):\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                \n",
    "                try:\n",
    "                    # Fit on training data\n",
    "                    elliptic_env = EllipticEnvelope(\n",
    "                        contamination=contamination,\n",
    "                        support_fraction=support_fraction,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    elliptic_env.fit(X_train)\n",
    "                    \n",
    "                    # Evaluate on validation data\n",
    "                    val_predictions = elliptic_env.predict(X_val)\n",
    "                    val_mahal_distances = elliptic_env.mahalanobis(X_val)\n",
    "                    \n",
    "                    # Calculate fold metrics\n",
    "                    n_outliers = np.sum(val_predictions == -1)\n",
    "                    outlier_ratio = n_outliers / len(X_val)\n",
    "                    \n",
    "                    # Consistency with training\n",
    "                    train_predictions = elliptic_env.predict(X_train)\n",
    "                    train_outlier_ratio = np.sum(train_predictions == -1) / len(X_train)\n",
    "                    ratio_consistency = abs(outlier_ratio - train_outlier_ratio)\n",
    "                    \n",
    "                    # Distance statistics\n",
    "                    mean_distance = np.mean(val_mahal_distances)\n",
    "                    std_distance = np.std(val_mahal_distances)\n",
    "                    \n",
    "                    fold_results.append({\n",
    "                        'outlier_ratio': outlier_ratio,\n",
    "                        'ratio_consistency': ratio_consistency,\n",
    "                        'mean_distance': mean_distance,\n",
    "                        'std_distance': std_distance\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in fold with contamination={contamination}, support_fraction={support_fraction}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if fold_results:\n",
    "                # Aggregate fold results\n",
    "                avg_outlier_ratio = np.mean([f['outlier_ratio'] for f in fold_results])\n",
    "                avg_consistency = np.mean([f['ratio_consistency'] for f in fold_results])\n",
    "                std_outlier_ratio = np.std([f['outlier_ratio'] for f in fold_results])\n",
    "                avg_mean_distance = np.mean([f['mean_distance'] for f in fold_results])\n",
    "                \n",
    "                # Quality score (lower is better for consistency measures)\n",
    "                quality_score = (\n",
    "                    abs(avg_outlier_ratio - contamination) +  # Match expected contamination\n",
    "                    avg_consistency +  # Train-val consistency\n",
    "                    std_outlier_ratio  # Stability across folds\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    'contamination': contamination,\n",
    "                    'support_fraction': support_fraction,\n",
    "                    'avg_outlier_ratio': avg_outlier_ratio,\n",
    "                    'ratio_consistency': avg_consistency,\n",
    "                    'stability': std_outlier_ratio,\n",
    "                    'avg_distance': avg_mean_distance,\n",
    "                    'quality_score': quality_score\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Apply cross-validation\n",
    "contamination_range = [0.05, 0.08, 0.1, 0.12, 0.15]\n",
    "support_fraction_range = [0.6, 0.65, 0.7, 0.75, 0.8]\n",
    "\n",
    "print(\"üîÑ Running cross-validation...\")\n",
    "cv_results = elliptic_envelope_cross_validation(\n",
    "    base_df[['Age']].values,\n",
    "    contamination_range,\n",
    "    support_fraction_range\n",
    ")\n",
    "\n",
    "# Find best parameters\n",
    "if len(cv_results) > 0:\n",
    "    best_params = cv_results.loc[cv_results['quality_score'].idxmin()]\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Parameters from Cross-Validation:\")\n",
    "    print(f\"contamination: {best_params['contamination']:.3f}\")\n",
    "    print(f\"support_fraction: {best_params['support_fraction']:.3f}\")\n",
    "    print(f\"Quality score: {best_params['quality_score']:.4f}\")\n",
    "    print(f\"Average outlier ratio: {best_params['avg_outlier_ratio']:.3f}\")\n",
    "    print(f\"Stability (std): {best_params['stability']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå No valid cross-validation results\")\n",
    "```\n",
    "\n",
    "#### **3. Diagnostic and Stability Analysis**\n",
    "\n",
    "```python\n",
    "def elliptic_envelope_stability_analysis(X, contamination, support_fraction, n_trials=10):\n",
    "    \"\"\"\n",
    "    Analyze stability of Elliptic Envelope across multiple runs\n",
    "    \"\"\"\n",
    "    \n",
    "    stability_metrics = {\n",
    "        'outlier_ratios': [],\n",
    "        'robust_centers': [],\n",
    "        'outlier_lists': [],\n",
    "        'mahal_distance_stats': []\n",
    "    }\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Use different random states\n",
    "        elliptic_env = EllipticEnvelope(\n",
    "            contamination=contamination,\n",
    "            support_fraction=support_fraction,\n",
    "            random_state=trial\n",
    "        )\n",
    "        \n",
    "        predictions = elliptic_env.fit_predict(X)\n",
    "        mahal_distances = elliptic_env.mahalanobis(X)\n",
    "        \n",
    "        # Record metrics\n",
    "        outlier_ratio = np.sum(predictions == -1) / len(X)\n",
    "        robust_center = elliptic_env.location_\n",
    "        outlier_indices = set(np.where(predictions == -1)[0])\n",
    "        \n",
    "        stability_metrics['outlier_ratios'].append(outlier_ratio)\n",
    "        stability_metrics['robust_centers'].append(robust_center)\n",
    "        stability_metrics['outlier_lists'].append(outlier_indices)\n",
    "        stability_metrics['mahal_distance_stats'].append({\n",
    "            'mean': np.mean(mahal_distances),\n",
    "            'std': np.std(mahal_distances)\n",
    "        })\n",
    "    \n",
    "    # Calculate stability statistics\n",
    "    outlier_ratio_cv = np.std(stability_metrics['outlier_ratios']) / np.mean(stability_metrics['outlier_ratios'])\n",
    "    \n",
    "    # Center stability\n",
    "    centers = np.array(stability_metrics['robust_centers'])\n",
    "    center_std = np.std(centers, axis=0)\n",
    "    center_mean = np.mean(centers, axis=0)\n",
    "    center_cv = np.linalg.norm(center_std) / np.linalg.norm(center_mean)\n",
    "    \n",
    "    # Outlier consensus\n",
    "    all_outliers = set()\n",
    "    for outlier_set in stability_metrics['outlier_lists']:\n",
    "        all_outliers.update(outlier_set)\n",
    "    \n",
    "    # Count how many times each point was detected as outlier\n",
    "    outlier_counts = {}\n",
    "    for outlier_set in stability_metrics['outlier_lists']:\n",
    "        for outlier_idx in outlier_set:\n",
    "            outlier_counts[outlier_idx] = outlier_counts.get(outlier_idx, 0) + 1\n",
    "    \n",
    "    # Consensus outliers (detected in >50% of runs)\n",
    "    consensus_threshold = n_trials * 0.5\n",
    "    consensus_outliers = [idx for idx, count in outlier_counts.items() if count >= consensus_threshold]\n",
    "    \n",
    "    stability_results = {\n",
    "        'outlier_ratio_cv': outlier_ratio_cv,\n",
    "        'center_cv': center_cv,\n",
    "        'consensus_outliers': consensus_outliers,\n",
    "        'outlier_detection_counts': outlier_counts,\n",
    "        'mean_outlier_ratio': np.mean(stability_metrics['outlier_ratios']),\n",
    "        'outlier_ratio_range': (min(stability_metrics['outlier_ratios']), max(stability_metrics['outlier_ratios']))\n",
    "    }\n",
    "    \n",
    "    print(f\"Stability Analysis (contamination={contamination:.3f}, support_fraction={support_fraction:.3f}):\")\n",
    "    print(f\"  Outlier ratio CV: {outlier_ratio_cv:.4f} (lower is more stable)\")\n",
    "    print(f\"  Center CV: {center_cv:.4f} (lower is more stable)\")\n",
    "    print(f\"  Consensus outliers: {len(consensus_outliers)} points\")\n",
    "    print(f\"  Outlier ratio range: {stability_results['outlier_ratio_range'][0]:.3f} - {stability_results['outlier_ratio_range'][1]:.3f}\")\n",
    "    \n",
    "    return stability_results\n",
    "\n",
    "# Test stability\n",
    "if len(cv_results) > 0:\n",
    "    stability_results = elliptic_envelope_stability_analysis(\n",
    "        base_df[['Age']].values,\n",
    "        best_params['contamination'],\n",
    "        best_params['support_fraction']\n",
    "    )\n",
    "```\n",
    "\n",
    "### üéØ Complete Elliptic Envelope Parameter Tuning Pipeline\n",
    "\n",
    "```python\n",
    "def complete_elliptic_envelope_tuning_pipeline(X, feature_names=None, domain_knowledge=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline for Elliptic Envelope parameter optimization and validation\n",
    "    \"\"\"\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    print(\"üîç Step 1: Data characteristics analysis...\")\n",
    "    \n",
    "    # Analyze data characteristics\n",
    "    n_samples, n_features = X.shape\n",
    "    print(f\"Dataset: {n_samples} samples, {n_features} features\")\n",
    "    \n",
    "    # Check for normality (important for Elliptic Envelope)\n",
    "    normality_tests = {}\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        data = X[:, i]\n",
    "        if len(data) > 8:  # Minimum for normaltest\n",
    "            stat, p_value = stats.normaltest(data)\n",
    "            normality_tests[feature_name] = {\n",
    "                'statistic': stat,\n",
    "                'p_value': p_value,\n",
    "                'is_normal': p_value > 0.05\n",
    "            }\n",
    "            print(f\"  {feature_name} normality: p={p_value:.4f} ({'Normal' if p_value > 0.05 else 'Non-normal'})\")\n",
    "    \n",
    "    print(f\"\\nüìä Step 2: Contamination estimation...\")\n",
    "    contamination_analysis = estimate_contamination_for_elliptic_envelope(X)\n",
    "    \n",
    "    print(f\"\\nüéØ Step 3: Support fraction optimization...\")\n",
    "    data_quality = 'medium'  # Default, can be adjusted based on domain knowledge\n",
    "    if domain_knowledge and 'data_quality' in domain_knowledge:\n",
    "        data_quality = domain_knowledge['data_quality']\n",
    "    \n",
    "    optimal_support_fraction, _ = suggest_support_fraction(X, data_quality)\n",
    "    \n",
    "    print(f\"\\nüîÑ Step 4: Cross-validation...\")\n",
    "    # Test around estimated values\n",
    "    base_contamination = contamination_analysis['median']\n",
    "    contamination_range = [\n",
    "        max(0.01, base_contamination - 0.05),\n",
    "        base_contamination,\n",
    "        min(0.5, base_contamination + 0.05)\n",
    "    ]\n",
    "    \n",
    "    support_fraction_range = [\n",
    "        max(0.5, optimal_support_fraction - 0.1),\n",
    "        optimal_support_fraction,\n",
    "        min(1.0, optimal_support_fraction + 0.1)\n",
    "    ]\n",
    "    \n",
    "    cv_results = elliptic_envelope_cross_validation(X, contamination_range, support_fraction_range)\n",
    "    \n",
    "    if len(cv_results) > 0:\n",
    "        best_params = cv_results.loc[cv_results['quality_score'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nüß™ Step 5: Statistical validation...\")\n",
    "        # Create final model with best parameters\n",
    "        final_elliptic_env = EllipticEnvelope(\n",
    "            contamination=best_params['contamination'],\n",
    "            support_fraction=best_params['support_fraction'],\n",
    "            random_state=42\n",
    "        )\n",
    "        final_elliptic_env.fit(X)\n",
    "        \n",
    "        # Validate statistical consistency\n",
    "        validation_results = validate_elliptic_envelope_statistical_consistency(X, final_elliptic_env)\n",
    "        \n",
    "        print(f\"\\nüî¨ Step 6: Stability analysis...\")\n",
    "        stability_results = elliptic_envelope_stability_analysis(\n",
    "            X, best_params['contamination'], best_params['support_fraction']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüèÜ Final Recommended Parameters:\")\n",
    "        print(f\"contamination: {best_params['contamination']:.3f}\")\n",
    "        print(f\"support_fraction: {best_params['support_fraction']:.3f}\")\n",
    "        print(f\"Validation score: {validation_results['overall_validation_score']:.2f}\")\n",
    "        print(f\"Stability (outlier ratio CV): {stability_results['outlier_ratio_cv']:.4f}\")\n",
    "        \n",
    "        # Business interpretation\n",
    "        predictions = final_elliptic_env.predict(X)\n",
    "        n_outliers = np.sum(predictions == -1)\n",
    "        \n",
    "        if domain_knowledge and 'business_context' in domain_knowledge:\n",
    "            print(f\"\\nüíº Business Interpretation:\")\n",
    "            context = domain_knowledge['business_context']\n",
    "            if context == 'customer_analysis':\n",
    "                print(f\"  Found {n_outliers} unusual customers ({n_outliers/len(X)*100:.1f}%)\")\n",
    "                print(f\"  These may represent special customer segments or data quality issues\")\n",
    "        \n",
    "        return {\n",
    "            'recommended_params': {\n",
    "                'contamination': best_params['contamination'],\n",
    "                'support_fraction': best_params['support_fraction']\n",
    "            },\n",
    "            'final_model': final_elliptic_env,\n",
    "            'validation_results': validation_results,\n",
    "            'stability_results': stability_results,\n",
    "            'normality_tests': normality_tests\n",
    "        }\n",
    "    else:\n",
    "        print(\"‚ùå No suitable parameters found. Consider:\")\n",
    "        print(\"  - Checking data preprocessing\")\n",
    "        print(\"  - Verifying data follows approximately elliptical distribution\")\n",
    "        print(\"  - Using alternative outlier detection methods\")\n",
    "        return None\n",
    "\n",
    "# Apply complete pipeline\n",
    "domain_knowledge = {\n",
    "    'data_quality': 'medium',\n",
    "    'business_context': 'customer_analysis'\n",
    "}\n",
    "\n",
    "optimal_elliptic_results = complete_elliptic_envelope_tuning_pipeline(\n",
    "    base_df[['Age']].values,\n",
    "    feature_names=['Age'],\n",
    "    domain_knowledge=domain_knowledge\n",
    ")\n",
    "```\n",
    "\n",
    "### üìù Elliptic Envelope Parameter Validation Checklist\n",
    "\n",
    "#### **‚úÖ Parameters are Well-Tuned When:**\n",
    "\n",
    "1. **Contamination Parameter Validation:**\n",
    "   - Detected outlier ratio ‚âà expected contamination ¬±3%\n",
    "   - Chi-squared distribution consistency at multiple confidence levels\n",
    "   - Outliers are interpretable in business context\n",
    "\n",
    "2. **Support Fraction Validation:**\n",
    "   - Robust center differs meaningfully from regular mean (shows robustness)\n",
    "   - But not excessively different (maintains reasonable estimates)\n",
    "   - Stable across different random seeds\n",
    "\n",
    "3. **Statistical Validation:**\n",
    "   - High overall validation score (>0.7)\n",
    "   - Good chi-squared consistency (relative error <30%)\n",
    "   - Sufficient outlier-normal distance separation\n",
    "   - Robust statistics show appropriate shift from regular statistics\n",
    "\n",
    "4. **Stability Validation:**\n",
    "   - Low coefficient of variation for outlier ratios (<0.2)\n",
    "   - Consistent robust center across runs (CV <0.1)\n",
    "   - High consensus among detected outliers (>50% agreement)\n",
    "\n",
    "#### **üö® Red Flags (Poor Tuning):**\n",
    "\n",
    "- **Poor statistical consistency**: Chi-squared tests fail consistently\n",
    "- **Excessive instability**: Different outliers detected across runs\n",
    "- **No robustness**: Robust center identical to regular mean\n",
    "- **Extreme contamination**: >30% outliers or <1% outliers\n",
    "- **Business contradiction**: Outliers don't make domain sense\n",
    "\n",
    "### üéØ Specific Recommendations for Customer Data\n",
    "\n",
    "```python\n",
    "def customer_elliptic_envelope_optimizer(customer_data, features=['Age']):\n",
    "    \"\"\"\n",
    "    Specialized Elliptic Envelope optimizer for customer data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üéØ Customer Data Elliptic Envelope Optimization\")\n",
    "    \n",
    "    X = customer_data[features].values\n",
    "    \n",
    "    # Customer-specific parameter ranges\n",
    "    # Conservative contamination for business data\n",
    "    contamination_range = [0.05, 0.08, 0.1, 0.12]  # 5-12% outliers typical for customers\n",
    "    \n",
    "    # Higher support fractions for business data (more conservative)\n",
    "    support_fraction_range = [0.7, 0.75, 0.8]\n",
    "    \n",
    "    print(f\"Testing contamination range: {contamination_range}\")\n",
    "    print(f\"Testing support fraction range: {support_fraction_range}\")\n",
    "    \n",
    "    # Run optimization\n",
    "    results = []\n",
    "    \n",
    "    for contamination in contamination_range:\n",
    "        for support_fraction in support_fraction_range:\n",
    "            try:\n",
    "                elliptic_env = EllipticEnvelope(\n",
    "                    contamination=contamination,\n",
    "                    support_fraction=support_fraction,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                predictions = elliptic_env.fit_predict(X)\n",
    "                mahal_distances = elliptic_env.mahalanobis(X)\n",
    "                \n",
    "                # Business-focused metrics\n",
    "                n_outliers = np.sum(predictions == -1)\n",
    "                outlier_ratio = n_outliers / len(X)\n",
    "                \n",
    "                # Statistical consistency\n",
    "                validation_results = validate_elliptic_envelope_statistical_consistency(X, elliptic_env)\n",
    "                validation_score = validation_results['overall_validation_score']\n",
    "                \n",
    "                # Business score (balance between statistical validity and business reasonableness)\n",
    "                business_score = (\n",
    "                    validation_score * 0.6 +  # Statistical validity\n",
    "                    (1 - abs(outlier_ratio - contamination)) * 0.4  # Match expected contamination\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    'contamination': contamination,\n",
    "                    'support_fraction': support_fraction,\n",
    "                    'n_outliers': n_outliers,\n",
    "                    'outlier_ratio': outlier_ratio,\n",
    "                    'validation_score': validation_score,\n",
    "                    'business_score': business_score\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with contamination={contamination}, support_fraction={support_fraction}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        best_result = results_df.loc[results_df['business_score'].idxmax()]\n",
    "        \n",
    "        print(f\"\\nüéØ Customer Analysis Results:\")\n",
    "        print(f\"Optimal contamination: {best_result['contamination']:.3f}\")\n",
    "        print(f\"Optimal support_fraction: {best_result['support_fraction']:.3f}\")\n",
    "        print(f\"Outlier customers: {int(best_result['n_outliers'])} ({best_result['outlier_ratio']*100:.1f}%)\")\n",
    "        print(f\"Validation score: {best_result['validation_score']:.3f}\")\n",
    "        \n",
    "        # Apply final model\n",
    "        final_elliptic_env = EllipticEnvelope(\n",
    "            contamination=best_result['contamination'],\n",
    "            support_fraction=best_result['support_fraction'],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        final_predictions = final_elliptic_env.fit_predict(X)\n",
    "        final_distances = final_elliptic_env.mahalanobis(X)\n",
    "        \n",
    "        # Customer insights\n",
    "        outlier_customers = customer_data[final_predictions == -1]\n",
    "        \n",
    "        if len(outlier_customers) > 0:\n",
    "            print(f\"\\nüìä Outlier Customer Analysis:\")\n",
    "            for feature in features:\n",
    "                outlier_values = outlier_customers[feature]\n",
    "                normal_values = customer_data[final_predictions == 1][feature]\n",
    "                \n",
    "                print(f\"  {feature}:\")\n",
    "                print(f\"    Outlier range: {outlier_values.min():.1f} - {outlier_values.max():.1f}\")\n",
    "                print(f\"    Normal range: {normal_values.min():.1f} - {normal_values.max():.1f}\")\n",
    "                print(f\"    Outlier mean: {outlier_values.mean():.1f} vs Normal mean: {normal_values.mean():.1f}\")\n",
    "        \n",
    "        return final_elliptic_env, final_predictions, final_distances, best_result\n",
    "    else:\n",
    "        print(\"‚ùå No valid parameter combinations found for customer data\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Apply customer-specific optimization\n",
    "customer_elliptic_env, customer_predictions, customer_distances, customer_best_params = customer_elliptic_envelope_optimizer(base_df)\n",
    "```\n",
    "\n",
    "### üéØ Summary: Elliptic Envelope Parameter Tuning Best Practices\n",
    "\n",
    "#### **üîß Parameter Selection Rules:**\n",
    "\n",
    "**For `contamination`:**\n",
    "1. **Use multiple estimation methods** - IQR, Z-score, chi-squared tests\n",
    "2. **Consider business context** - typical outlier rates in your domain\n",
    "3. **Test around estimates** - ¬±5% from initial estimate\n",
    "4. **Validate statistically** - check chi-squared distribution consistency\n",
    "\n",
    "**For `support_fraction`:**\n",
    "1. **Start with theoretical minimum** - (n+p+1)/(2n) but typically 0.5-0.8\n",
    "2. **Adjust for data quality** - lower for noisier data\n",
    "3. **Consider sample size** - higher for smaller datasets\n",
    "4. **Balance robustness vs efficiency** - lower = more robust, higher = more efficient\n",
    "\n",
    "#### **üß™ Validation Methodology:**\n",
    "\n",
    "1. **Statistical consistency** - chi-squared distribution tests\n",
    "2. **Cross-validation** - stability across data splits\n",
    "3. **Robustness checks** - meaningful difference from regular statistics\n",
    "4. **Stability analysis** - consistent results across random seeds\n",
    "5. **Business validation** - outliers make domain sense\n",
    "\n",
    "#### **üéØ For Customer Segmentation:**\n",
    "\n",
    "- **contamination**: 5-12% (typical for customer data)\n",
    "- **support_fraction**: 0.7-0.8 (conservative for business use)\n",
    "- **Key validation**: Statistical consistency + business interpretability\n",
    "- **Success metric**: Validation score >0.7 + stable outlier detection\n",
    "\n",
    "The key is **statistical validation combined with business sense** - Elliptic Envelope should pass statistical tests AND identify meaningful business outliers! üéØ\n",
    "\n",
    "### üß™ Validation Methodologies\n",
    "\n",
    "**Key validation approaches:**\n",
    "\n",
    "1. **Statistical consistency** - Chi-squared distribution tests\n",
    "2. **Cross-validation** - Stability across data splits  \n",
    "3. **Robustness checks** - Meaningful difference from regular statistics\n",
    "4. **Stability analysis** - Consistent results across random seeds\n",
    "5. **Business validation** - Outliers make domain sense\n",
    "\n",
    "**For Customer Segmentation:**\n",
    "- **contamination**: 5-12% (typical for customer data)\n",
    "- **support_fraction**: 0.7-0.8 (conservative for business use)\n",
    "- **Success metric**: Validation score >0.7 + stable outlier detection\n",
    "\n",
    "The key is **statistical validation combined with business sense** - Elliptic Envelope should pass statistical tests AND identify meaningful business outliers! üéØ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7fe6f6",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Note: Corrupted Cell Above\n",
    "\n",
    "**Cell 56** above contains corrupted/garbled content that was damaged during copy-paste. The **corrected and functional version** is provided in the cell above this one.\n",
    "\n",
    "**What was fixed:**\n",
    "- ‚úÖ Proper Python syntax for the contamination estimation function\n",
    "- ‚úÖ Fixed variable names and method calls\n",
    "- ‚úÖ Corrected string formatting and print statements  \n",
    "- ‚úÖ Added proper exception handling\n",
    "- ‚úÖ Complete parameter selection methodology\n",
    "- ‚úÖ Validation approaches summary\n",
    "\n",
    "**The corrected version provides:**\n",
    "1. **Data-driven contamination estimation** using multiple statistical methods\n",
    "2. **Support fraction optimization** based on data characteristics  \n",
    "3. **Validation methodologies** for parameter tuning\n",
    "4. **Customer segmentation specific recommendations**\n",
    "\n",
    "You can safely **ignore cell 56** and use the corrected version above instead.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
